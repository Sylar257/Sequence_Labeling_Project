{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sequence_labeling_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sylar257/Sequence_Labeling_Project/blob/master/Sequence_labeling_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tmFs0zFHDQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from utils import *\n",
        "import torch.nn.functional as F\n",
        "decive = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QENba6KHo59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Highway(nn.Module):\n",
        "    \"\"\"\n",
        "    Highway network\n",
        "    \"\"\"\n",
        "    def __init__(self, size, num_layers=1, dropout=0.5):\n",
        "        \"\"\"\n",
        "        size: size of Linear layer (should match input size)\n",
        "        num_layers: number of transform and gate layers\n",
        "        dropout: dropout rate\n",
        "        \"\"\"\n",
        "        super(Highway, self).__init__()\n",
        "        self.size = size\n",
        "        self.num_layers = num_layers\n",
        "        self.transform = nn.ModuleList() # A list of transform layers\n",
        "        self.gate = nn.ModuleList()      # A list of gate layers\n",
        "        self.dropout = torch.nn.Dropout(p=dropout)\n",
        "\n",
        "        for i in range(num_layers):\n",
        "            transform = nn.Linear(size, size)\n",
        "            gate = nn.Linear(size, size)\n",
        "            self.transform.append(transform)\n",
        "            self.gate.append(gate)\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward-prop.\n",
        "        Returns a tensor with the same dimensions as input tensor\n",
        "        \"\"\"\n",
        "\n",
        "        transformed = F.relu(self.transform[0](x))  # transform with the first transform layer\n",
        "        g = F.sigmoid(self.gate[0](x))              # calculate how much of the transformed input to keep\n",
        "\n",
        "        out = self.dropout(g*transformed + (1-g)*x)               # combine input and transformed input with ratio of g\n",
        "\n",
        "        # If there are additional layers\n",
        "        for i in range(self.num_layers):\n",
        "            transformed = F.relu(self.transform[i](out))\n",
        "            g = F.sigmoid(self.gate[i](out))\n",
        "            out = self.dropout(g*transformed+(1-g)*out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zaftUlVLOad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CRF(nn.Module):\n",
        "    \"\"\"\n",
        "    Confitional Random Field\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dim, tagset_size):\n",
        "        \"\"\"\n",
        "        hidden_dim: the size of word/BLSTM's output (which is the input size for CRF)\n",
        "        tagset_size: number of tags(depending on our dataset)\n",
        "        \"\"\"\n",
        "\n",
        "        super(CRF, self).__init__()\n",
        "        self.tagset_size = tagset_size\n",
        "        self.emission = nn.Linear(hidden_dim, self.tagset_size)\n",
        "        self.transition = nn.Parameter(torch.Tensor(self.tagset_size, self.tagset_size))\n",
        "        self.transition.data.zero_() # initializa the transition matrix to be all zeros\n",
        "\n",
        "    def forward(self, feats):\n",
        "        \"\"\"\n",
        "        feats:   output of word/BLSTM, a tensor of dimensions-(batch_size, timesteps, hidden_dim)\n",
        "        returns: CRF scores, a tensor of dimensions-(batch_size, timesteps, tagset_size, tagset_size)\n",
        "        \"\"\"\n",
        "        self.batch_size = feats.size(0)\n",
        "        self.timesteps  = feats.size(1)\n",
        "\n",
        "        emission_scores  = self.emission(feats)  # (batch_size, timesteps, tagset_size)\n",
        "        # here we broadcast emission_score in order to compute the total score later with transition score\n",
        "        emission_scores  = emission_score.unsqueeze(2).expand(self.batch_size, self.timesteps, self.tagset_size,\n",
        "                                                              self.tagset_size)  # (batch_size, timesteps, tagset_size, tagset_size)\n",
        "\n",
        "        crf_scores = emission_scores + self.transition.unsqueeze(0).unsqueeze(0)  # (batch_size, timesteps, tagset_size, tagset_size)\n",
        "        return crf_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAK0OBqVT61P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ViterbiLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Viterbi Loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tag_map):\n",
        "        \"\"\"\n",
        "        :param tag_map: tag map\n",
        "        \"\"\"\n",
        "        super(ViterbiLoss, self).__init__()\n",
        "        self.tagset_size = len(tag_map)\n",
        "        self.start_tag = tag_map['<start>']\n",
        "        self.end_tag = tag_map['<end>']\n",
        "\n",
        "    def forward(self, scores, targets, lengths):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param scores: CRF scores\n",
        "        :param targets: true tags indices in unrolled CRF scores\n",
        "        :param lengths: word sequence lengths\n",
        "        :return: viterbi loss\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size = scores.size(0)\n",
        "        word_pad_len = scores.size(1)\n",
        "\n",
        "        # Gold score\n",
        "\n",
        "        targets = targets.unsqueeze(2)\n",
        "        scores_at_targets = torch.gather(scores.view(batch_size, word_pad_len, -1), 2, targets).squeeze(\n",
        "            2)  # (batch_size, word_pad_len)\n",
        "\n",
        "        # Everything is already sorted by lengths\n",
        "        scores_at_targets, _ = pack_padded_sequence(scores_at_targets, lengths, batch_first=True)\n",
        "        gold_score = scores_at_targets.sum()\n",
        "\n",
        "        # All paths' scores\n",
        "\n",
        "        # Create a tensor to hold accumulated sequence scores at each current tag\n",
        "        scores_upto_t = torch.zeros(batch_size, self.tagset_size).to(device)\n",
        "\n",
        "        for t in range(max(lengths)):\n",
        "            batch_size_t = sum([l > t for l in lengths])  # effective batch size (sans pads) at this timestep\n",
        "            if t == 0:\n",
        "                scores_upto_t[:batch_size_t] = scores[:batch_size_t, t, self.start_tag, :]  # (batch_size, tagset_size)\n",
        "            else:\n",
        "                # We add scores at current timestep to scores accumulated up to previous timestep, and log-sum-exp\n",
        "                # Remember, the cur_tag of the previous timestep is the prev_tag of this timestep\n",
        "                # So, broadcast prev. timestep's cur_tag scores along cur. timestep's cur_tag dimension\n",
        "                scores_upto_t[:batch_size_t] = log_sum_exp(\n",
        "                    scores[:batch_size_t, t, :, :] + scores_upto_t[:batch_size_t].unsqueeze(2),\n",
        "                    dim=1)  # (batch_size, tagset_size)\n",
        "\n",
        "        # We only need the final accumulated scores at the <end> tag\n",
        "        all_paths_scores = scores_upto_t[:, self.end_tag].sum()\n",
        "\n",
        "        viterbi_loss = all_paths_scores - gold_score\n",
        "        viterbi_loss = viterbi_loss / batch_size\n",
        "\n",
        "        return viterbi_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4w6o7q1jQxtd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LM_LSTM_CRF(nn.Module):\n",
        "    \"\"\"\n",
        "    The encompassing LM-LSTM-CRF\n",
        "    \"\"\"\n",
        "    def __init__(self, tagset_size, charset_size, char_emb_dim, char_rnn_dim, char_rnn_layers, vocab_size,\n",
        "                 lm_vocab_size, word_emb_dim, word_rnn_dim, word_rnn_layers, dropout, highway_layers=1):\n",
        "        \"\"\"\n",
        "        tagset_size:   number of tags\n",
        "        charset_size:  size of character vocabulary\n",
        "        char_emb_dim:  size of character embeddings\n",
        "        char_rnn_dim:  size of charactor RNNS/LSTMs\n",
        "        char_rnn_layers: number of layers in character RNN/LSTMs\n",
        "        vocab_size:    input vocabulary size\n",
        "        lm_vocab_size: vocabulary size of language models (in-corpus words subject to word frequency threshold)\n",
        "        word_emb_dim:  size of word embeddings\n",
        "        word_rnn_dim:  size of word RNN/BLSTM\n",
        "        word_rnn_layers: number of layers in word RNNs/LSTMs\n",
        "        dropout:       dropout\n",
        "        highway_layers: number of transform and gate layers\n",
        "        \"\"\"\n",
        "        \n",
        "        super(LM_LSTM_CRF, self).__init__()\n",
        "\n",
        "        self.tagset_size  = tagset_size # this is the size of the outout vocab of the tagging model\n",
        "\n",
        "        self.charset_size = charset_size\n",
        "        self.char_emb_dim = char_emb_dim\n",
        "        self.char_rnn_dim = char_rnn_dim\n",
        "        self.char_rnn_layers = char_rnn_layers\n",
        "\n",
        "        self.wordset_size  = vocab_size     # this is the size of the input vocab (embedding layer) of the tagging model\n",
        "        self.lm_vocab_size = lm_vocab_size  # this is the size of the output vocab of the language model\n",
        "        self.word_emb_dim  = word_emb_dim\n",
        "        self.word_rnn_dim  = word_rnn_dim\n",
        "        self.word_rnn_layers = word_rnn_layers\n",
        "        \n",
        "        self.highway_layers = highway_layers\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # charactor embedding layer\n",
        "        self.char_embeds = nn.Embedding(num_embeddings=self.charset_size, embedding_dim=self.char_emb_dim)  \n",
        "\n",
        "        # forward char LSTM\n",
        "        self.forw_char_lstm = nn.LSTM(input_size=self.char_emb_dim, hidden_size=self.char_rnn_dim, \n",
        "                                      num_layers=self.char_rnn_layers, bidirectional=False, dropout = dropout)\n",
        "        # backward char LSTM\n",
        "        self.back_char_lstm = nn.LSTM(input_size=self.char_emb_dim, hidden_size=self.char_rnn_dim,\n",
        "                                      num_layers=self.char_rnn_layers, bidirectional=False, dropout = dropout)\n",
        "        \n",
        "        # word embedding layer\n",
        "        self.word_embeds = nn.Embedding(num_embeddings=self.wordset_size,embedding_dim=self.word_emb_dim)\n",
        "        # Define word-level bidirection LSTM\n",
        "        # Take note on the hidden_size\n",
        "        self.word_blstm   = nn.LSTM(input_size=(self.word_emb_dim+self.char_emb_dim*2), \n",
        "                                    hidden_size=self.word_rnn_dim//2, \n",
        "                                    # This is because Bi-directional LSTM will concat forward and backward output\n",
        "                                    # therefore we specify word_rnn_dim//2 but will get output size of word_rnn_dim\n",
        "                                    num_layers=self.word_rnn_layers,\n",
        "                                    bidirectional=True,\n",
        "                                    dropout=dropout\n",
        "                                    )\n",
        "        \n",
        "        # Conditinoal Random Field layer\n",
        "        self.crf = CRF(hidden_dim=self.word_rnn_dim,tagset_size=self.tagset_size)\n",
        "\n",
        "        # 3 places that we implemented highway connections\n",
        "        self.forw_lm_hw = Highway(size=self.char_rnn_dim,\n",
        "                                  num_layers=self.highway_layers,\n",
        "                                  dropout=dropout)\n",
        "        self.back_lm_hw = Highway(size=self.char_emb_dim,\n",
        "                                  num_layers=self.highway_layers,\n",
        "                                  dropout=dropout)\n",
        "        self.subword_hw = Highway(2 * self.char_rnn_dim, \n",
        "                                  num_layers=self.highway_layers,\n",
        "                                  dropout=dropout)\n",
        "        \n",
        "        # Linear layers for language models, They are used for \"muti-task training\" for language models (predicting next word)\n",
        "        self.forw_lm_out = nn.Linear(self.char_rnn_dim, self.lm_vocab_size)\n",
        "        self.back_lm_out = nn.Linear(self.char_rnn_dim, self.lm_vocab_size)\n",
        "\n",
        "    def init_word_embedding(self, embedding):\n",
        "        \"\"\"\n",
        "        Initialize embeddings with pre-trained embeddings.\n",
        "\n",
        "        embedding: pre-trained embeddings to be loaded\n",
        "        \"\"\"\n",
        "        self.word_embeds.weights = nn.Parameter(embeddings)\n",
        "\n",
        "    def fine_tune_word_embeddings(self, fine_tune=False):\n",
        "        \"\"\"\n",
        "        Fine-tune embedding layer? (if using pre-trained embedding layer, consider no fine-tuning)\n",
        "\n",
        "        fine_tune: bool decides if fine_tune\n",
        "        \"\"\"\n",
        "        for p in self.word_embeds.parameters():\n",
        "            p.requires_grad = fine_tune\n",
        "    \n",
        "    def forward(self, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, wmaps, tmaps, wmap_lengths, cmap_lengths):\n",
        "        \"\"\"\n",
        "        cmaps_f: padded encoded forward  character sequences. (batch_size, char_pad_len)\n",
        "        cmaps_b: padded encoded backward character sequences. (batch_size, char_pad_len)\n",
        "        cmarker_f: padded forward character markers.          (batch_size, word_pad_len)\n",
        "        cmarker_b: padded backward character markers.         (batch_size, word_pad_len)\n",
        "        wmaps: padded encoded word sequences.                 (batch_size, word_pad_len)\n",
        "        tmaps: padded tag sequences.                          (batch_size, word_pad_len)\n",
        "        wmap_lengths: word sequence lengths.                  (batch_size)\n",
        "        cmap_lengths: character sequence lengths              (batch_size, word_pad_len)\n",
        "        \"\"\"\n",
        "\n",
        "        self.batch_size   = cmaps_f.size(0)\n",
        "        self.word_pad_len = cmarker_f.size(1)\n",
        "\n",
        "        # Sort by decreasing true char. sequence length for grouping up for padding later\n",
        "        cmap_lengths, char_sort_idx = cmap_lengths.sort(dim=0, descending=True)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XRFQhNj_Bfr",
        "colab_type": "code",
        "outputId": "fde016e7-aeee-484b-be33-aec3025e3454",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My\\ Drive"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n",
            "/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ixkzwHv_ye6",
        "colab_type": "code",
        "outputId": "58672a80-8194-493d-af94-c905cd4f27dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 2018\n",
            " 2019\n",
            "'Attention in deep learning.md'\n",
            "'Colab Notebooks'\n",
            " data\n",
            " Enoava\n",
            "'LSTM in PyTorch.md'\n",
            "'LSTM in PyTorch.pdf'\n",
            "'Response to comments_MECHMT_2018_1107_-V4_17_Dec.docx'\n",
            " Snaps\n",
            " Stack-presentation-Dermotologist.png\n",
            "'Starwars Project Video V2.mp4'\n",
            " YOLO.md\n",
            " YOLO.pdf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIwrkKoX_zxO",
        "colab_type": "code",
        "outputId": "d4595d3c-98c6-458e-fd48-a623fde65eef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /gdrive/My\\ Drive/data/embeddings"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive/data/embeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obm_y4aE__pm",
        "colab_type": "code",
        "outputId": "8e7b1ce3-8813-4fed-91ee-de4f20f761c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "glove.6B.100d.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATeBjYOFA5pg",
        "colab_type": "code",
        "outputId": "0dfa8142-bad8-4a13-80d4-1ccebc97cc44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive/data/embeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUmg97Kl9Z-v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "# Rewrite the __getitem__ and add __len__\n",
        "class WCDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for the LM-LSTM-CRF model. To be used by a PyTorch DataLoader to feed batches to the model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, wmaps, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, tmaps, wmap_lengths, cmap_lengths):\n",
        "        \"\"\"\n",
        "        :param wmaps: padded encoded word sequences\n",
        "        :param cmaps_f: padded encoded forward character sequences\n",
        "        :param cmaps_b: padded encoded backward character sequences\n",
        "        :param cmarkers_f: padded forward character markers\n",
        "        :param cmarkers_b: padded backward character markers\n",
        "        :param tmaps: padded encoded tag sequences (indices in unrolled CRF scores)\n",
        "        :param wmap_lengths: word sequence lengths\n",
        "        :param cmap_lengths: character sequence lengths\n",
        "        \"\"\"\n",
        "        self.wmaps = wmaps\n",
        "        self.cmaps_f = cmaps_f\n",
        "        self.cmaps_b = cmaps_b\n",
        "        self.cmarkers_f = cmarkers_f\n",
        "        self.cmarkers_b = cmarkers_b\n",
        "        self.tmaps = tmaps\n",
        "        self.wmap_lengths = wmap_lengths\n",
        "        self.cmap_lengths = cmap_lengths\n",
        "\n",
        "        self.data_size = self.wmaps.size(0)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.wmaps[i], self.cmaps_f[i], self.cmaps_b[i], self.cmarkers_f[i], self.cmarkers_b[i], self.tmaps[i], \\\n",
        "               self.wmap_lengths[i], self.cmap_lengths[i]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHc2_4Nj-iTF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ViterbiDecoder():\n",
        "    \"\"\"\n",
        "    Viterbi Decoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tag_map):\n",
        "        \"\"\"\n",
        "        :param tag_map: tag map\n",
        "        \"\"\"\n",
        "        self.tagset_size = len(tag_map)\n",
        "        self.start_tag = tag_map['<start>']\n",
        "        self.end_tag = tag_map['<end>']\n",
        "\n",
        "    def decode(self, scores, lengths):\n",
        "        \"\"\"\n",
        "        :param scores: CRF scores\n",
        "        :param lengths: word sequence lengths\n",
        "        :return: decoded sequences\n",
        "        \"\"\"\n",
        "        batch_size = scores.size(0)\n",
        "        word_pad_len = scores.size(1)\n",
        "\n",
        "        # Create a tensor to hold accumulated sequence scores at each current tag\n",
        "        scores_upto_t = torch.zeros(batch_size, self.tagset_size)\n",
        "\n",
        "        # Create a tensor to hold back-pointers\n",
        "        # i.e., indices of the previous_tag that corresponds to maximum accumulated score at current tag\n",
        "        # Let pads be the <end> tag index, since that was the last tag in the decoded sequence\n",
        "        backpointers = torch.ones((batch_size, max(lengths), self.tagset_size), dtype=torch.long) * self.end_tag\n",
        "\n",
        "        for t in range(max(lengths)):\n",
        "            batch_size_t = sum([l > t for l in lengths])  # effective batch size (sans pads) at this timestep\n",
        "            if t == 0:\n",
        "                scores_upto_t[:batch_size_t] = scores[:batch_size_t, t, self.start_tag, :]  # (batch_size, tagset_size)\n",
        "                backpointers[:batch_size_t, t, :] = torch.ones((batch_size_t, self.tagset_size),\n",
        "                                                               dtype=torch.long) * self.start_tag\n",
        "            else:\n",
        "                # We add scores at current timestep to scores accumulated up to previous timestep, and\n",
        "                # choose the previous timestep that corresponds to the max. accumulated score for each current timestep\n",
        "                scores_upto_t[:batch_size_t], backpointers[:batch_size_t, t, :] = torch.max(\n",
        "                    scores[:batch_size_t, t, :, :] + scores_upto_t[:batch_size_t].unsqueeze(2),\n",
        "                    dim=1)  # (batch_size, tagset_size)\n",
        "\n",
        "        # Decode/trace best path backwards\n",
        "        decoded = torch.zeros((batch_size, backpointers.size(1)), dtype=torch.long)\n",
        "        pointer = torch.ones((batch_size, 1),\n",
        "                             dtype=torch.long) * self.end_tag  # the pointers at the ends are all <end> tags\n",
        "\n",
        "        for t in list(reversed(range(backpointers.size(1)))):\n",
        "            decoded[:, t] = torch.gather(backpointers[:, t, :], 1, pointer).squeeze(1)\n",
        "            pointer = decoded[:, t].unsqueeze(1)  # (batch_size, 1)\n",
        "\n",
        "        # Sanity check\n",
        "        assert torch.equal(decoded[:, 0], torch.ones((batch_size), dtype=torch.long) * self.start_tag)\n",
        "\n",
        "        # Remove the <starts> at the beginning, and append with <ends> (to compare to targets, if any)\n",
        "        decoded = torch.cat([decoded[:, 1:], torch.ones((batch_size, 1), dtype=torch.long) * self.start_tag],\n",
        "                            dim=1)\n",
        "\n",
        "        return decoded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGatVIt56gXe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import sys\n",
        "from utils import *\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI_mY6W265rh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "task = 'ner'        # tagging task, choose between [ner, pos]\n",
        "train_file = '/gdrive/My Drive/data/CoNLL-2003/eng.train'\n",
        "val_file   = '/gdrive/My Drive/data/CoNLL-2003/eng.testa'\n",
        "test_file  = '/gdrive/My Drive/data/CoNLL-2003/eng.testb'\n",
        "emb_file   = '/gdrive/My Drive/data/embeddings/glove.6B.100d.txt'\n",
        "min_word_freq = 5 # threshold for word frequency to be recognized not as xxunk\n",
        "min_char_freq = 1 # same thing for char frequency\n",
        "caseless   = True # lowercase everything?\n",
        "expand_vocab = True # expand model's input vocabulary to the pre-trained embedding vocabulary?\n",
        "\n",
        "# Model parameters\n",
        "char_emb_dim = 30 # character embedding size\n",
        "with open(emb_file, 'r') as f:\n",
        "    word_emb_dim = len(f.readline().split(' ')) - 1  # word embdding size, \"-1\" is because in the txt file the first place is the word itself, followed by the actual embeddings\n",
        "word_rnn_dim = 300  # word BLSTM hidden size\n",
        "char_rnn_dim = 300  # character RNN size\n",
        "char_rnn_layers = 1 # number of layers in character RNN\n",
        "word_rnn_layers = 1 # number of layers in word BLSTM\n",
        "highway_layers  = 1 # number of layers in highway network\n",
        "dropout = 0.5       # universal dropout rate\n",
        "fine_tune_word_embeddings = False\n",
        "\n",
        "# Training parameters\n",
        "start_epoch = 0   # start at this epoch\n",
        "batch_size  = 10  # batch size\n",
        "lr = 0.015  \n",
        "lr_decay = 0.05\n",
        "momentum = 0.9\n",
        "workers  = 4\n",
        "epochs   = 200    # number of epochs without triggering early stoping\n",
        "grad_clip = 5.\n",
        "print_freq = 100  # print every ___ batches\n",
        "best_f1  = 0.\n",
        "checkpoint = None # Model checkpoint to load. None if training from scratch\n",
        "\n",
        "tag_ind = 1 if task == 'pos' else 3 # choose column in CoNLL 2003 dataset\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnLn_7JoDGUf",
        "colab_type": "code",
        "outputId": "6a5ca531-e878-4499-8cad-96bb84886e1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "global best_f1, epochs_since_improvement, checkpoint, start_epoch, word_map, char_map, tag_map\n",
        "\n",
        "# Read training and validation data\n",
        "train_words, train_tags = read_words_tags(train_file, tag_ind, caseless)\n",
        "val_words, val_tags = read_words_tags(val_file, tag_ind, caseless)\n",
        "\n",
        "if checkpoint is not None:\n",
        "    checkpoint = torch.load(checkpoint)\n",
        "    model = checkpoint['model']\n",
        "    optimizer = checkpoint['optimizer']\n",
        "    word_map  = checkpoint['word_map']\n",
        "    lm_vocab_size = checkpoint['lm_vocab_size']\n",
        "    tag_map   = checkpoint['tag_map']\n",
        "    char_map  = checkpoint['char_map']\n",
        "    start_epoch = checkpoint['epoch'] +1\n",
        "    best_f1   = checkpoint['f1']\n",
        "else:\n",
        "    # create word, char, tag maps\n",
        "    # maps are essentially dictionaries that map a token to an integer\n",
        "    word_map, char_map, tag_map = create_maps(train_words+val_words,train_tags+val_tags, min_word_freq, min_char_freq)\n",
        "\n",
        "    # load pre-trained embeddings, if expand_vocab==True, word_map expand to embedding_word_map\n",
        "    # lm_vocab_size is the word_map size before expand to \"out-of-corpus vocab\"\n",
        "    embeddings, word_map, lm_vocab_size = load_embeddings(emb_file, word_map, expand_vocab)\n",
        "\n",
        "    model = LM_LSTM_CRF(tagset_size=len(tag_map),\n",
        "                        charset_size=len(char_map),\n",
        "                        char_emb_dim=char_emb_dim,\n",
        "                        char_rnn_dim=char_rnn_dim,\n",
        "                        char_rnn_layers=char_rnn_layers,\n",
        "                        vocab_size=len(word_map),       # This is the length after expand\n",
        "                        lm_vocab_size=lm_vocab_size,    # len(word_map) before expand, not influenced by the embedding vocab\n",
        "                        word_emb_dim=word_emb_dim,\n",
        "                        word_rnn_dim=word_rnn_dim,\n",
        "                        word_rnn_layers=word_rnn_layers,\n",
        "                        dropout=dropout,\n",
        "                        highway_layers=highway_layers).to(device)\n",
        "    model.init_word_embedding(embeddings.to(device)) # initializa embedding layers with pre-trained embeddings.(Essentially we just make it nn.Parameter)\n",
        "    model.fine_tune_word_embeddings(fine_tune_word_embeddings)    # decide if these nn.Parameters has requires_grad = True (trainable)\n",
        "    optimizer = optim.SGD(params=filter(lambda p: p.requires_grad, model.parameters()), lr=lr, momentum=momentum)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedding length is 100.\n",
            "You have elected to include embeddings that are out-of-corpus.\n",
            "\n",
            "Loading embeddings...\n",
            "'word_map' is being updated accordingly.\n",
            "\n",
            "Done.\n",
            " Embedding vocabulary: 400054\n",
            " Language Model vocabulary: 4671.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tkmj3BnbrxiL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1ac90d6d-73b4-42f5-8520-aaaba987d3ce"
      },
      "source": [
        "embeddings.shape"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([400054, 100])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIMiKnjCr1U7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLEz1kpkpjVE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elf4Fik_pfex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCUtUGyqDAuu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loss funcitons\n",
        "lm_criterion  = nn.CrossEntropyLoss().to(device)\n",
        "crf_criterion = ViterbiLoss(tag_map).to(device)\n",
        "\n",
        "# Since the language model's vocab is restricted on in-corpus indices, encode training/val with only these!\n",
        "# word_map might have been expanded, and in-corpus words eliminated due to low frequency might still be added because\n",
        "# they exist in the pre-trained embeddings\n",
        "temp_word_map = {k: v for k, v in word_map.items() if v <= word_map['<unk>']}\n",
        "\n",
        "# train_input = (padded_wmaps, padded_cmaps_f, padded_cmaps_b, padded_cmarkers_f, padded_cmarkers_b, \n",
        "                #padded_tmaps, wmap_lengths, cmap_lengths)\n",
        "train_inputs = create_input_tensors(train_words, train_tags, temp_word_map, char_map,\n",
        "                                        tag_map)\n",
        "val_inputs = create_input_tensors(val_words, val_tags, temp_word_map, char_map, tag_map)\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = torch.utils.data.DataLoader(WCDataset(*train_inputs), batch_size=batch_size, shuffle=True,\n",
        "                                            num_workers=workers, pin_memory=False)\n",
        "val_loader = torch.utils.data.DataLoader(WCDataset(*val_inputs), batch_size=batch_size, shuffle=True,\n",
        "                                             num_workers=workers, pin_memory=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjtO8QdzpAys",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6198e713-bcee-467d-f919-f94930c2b49a"
      },
      "source": [
        "len(train_words),len(val_words)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14041, 3250)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLgiXMlCpJE8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wl8cNgJcccZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wmaps = list(map(lambda s: list(map(lambda w: temp_word_map.get(w, temp_word_map['<unk>']), s)) + [temp_word_map['<end>']], train_words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ci1CKRQ9c7TD",
        "colab_type": "code",
        "outputId": "7afcb4a5-dbbf-4a65-d918-a0e1f749c402",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max_num = []\n",
        "for w_list in wmaps:\n",
        "    max_num.append(len(w_list))\n",
        "max(max_num)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "114"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHxLrcdFcuDL",
        "colab_type": "code",
        "outputId": "6bea76b1-8b75-4041-bd04-6ec7ac125f89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(wmaps)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14041"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSSXNXjPXyLM",
        "colab_type": "code",
        "outputId": "baffdd7d-530f-4625-8b21-d51da8d6c71e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "padded_wmaps = train_inputs[0]\n",
        "padded_wmaps.size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([14041, 114])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9pRNS10Vfiq",
        "colab_type": "code",
        "outputId": "225e7ca0-c60d-4cd7-fdd1-eea2ecb8901e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(train_inputs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZKMKtPFVHdp",
        "colab_type": "code",
        "outputId": "3570522e-d813-49bf-f7cf-141caab392cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(temp_word_map)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4671"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrqJB8z-C666",
        "colab_type": "code",
        "outputId": "fbea00a9-6b75-477b-c98b-5046ae32e890",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(word_map)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400054"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPp3ElMeCbrD",
        "colab_type": "code",
        "outputId": "b48cb250-7909-4551-88ee-b76c664c4007",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "embeddings.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([400054, 100])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjBB6j_zBBNI",
        "colab_type": "code",
        "outputId": "23606c64-9b4a-457b-b701-17580a07b3ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(train_words), len(train_tags)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14041, 14041)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3BOnbcTA_eB",
        "colab_type": "code",
        "outputId": "d72d5e81-4c54-4d16-e143-bfbe14a24f06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_words[10][0:3], train_tags[10][0:3]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['spanish', 'farm', 'minister'], ['I-MISC', 'O', 'O'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RQszd5ZAt1z",
        "colab_type": "code",
        "outputId": "e0064cd2-3246-4bd7-828b-78c2d323dcbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "train_words[0][0:10], train_tags[0][0:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.'],\n",
              " ['NNP', 'VBZ', 'JJ', 'NN', 'TO', 'VB', 'JJ', 'NN', '.'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwdDJxDFArCo",
        "colab_type": "code",
        "outputId": "06aa7e90-0e6c-4348-864b-139cd43b7ae5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(tag_map)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mkAqnnGB3Bp",
        "colab_type": "code",
        "outputId": "b7c93b48-40aa-42dc-cf12-329a47a2e3e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "word_map['<end>']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4669"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrriZgT3BtvI",
        "colab_type": "code",
        "outputId": "d377119a-8e2c-4e6a-d81f-f620b5f5aeaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "word_map['eu']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WfvVGpSAZWx",
        "colab_type": "code",
        "outputId": "0e9d1908-66f5-46df-b627-b431e99114e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "word_map['<unk>']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4670"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14c3cJmXAohS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTJXstIU-3EL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cd My\\ Drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7afq6mvR_SsG",
        "colab_type": "code",
        "outputId": "2c243da6-ad91-4e7f-a949-01ce5cf9813e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'My Drive'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7vNMuas_UYl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wBjtxj4_PAm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u0UXBfZ_NTV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHsCg66b_Hn-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}