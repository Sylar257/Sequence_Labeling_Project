{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sequence_labeling_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sylar257/Sequence_Labeling_Project/blob/master/Sequence_labeling_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tmFs0zFHDQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from utils import *\n",
        "import torch.nn.functional as F\n",
        "decive = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QENba6KHo59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Highway(nn.Module):\n",
        "    \"\"\"\n",
        "    Highway network\n",
        "    \"\"\"\n",
        "    def __init__(self, size, num_layers=1, dropout=0.5):\n",
        "        \"\"\"\n",
        "        size: size of Linear layer (should match input size)\n",
        "        num_layers: number of transform and gate layers\n",
        "        dropout: dropout rate\n",
        "        \"\"\"\n",
        "        super(Highway, self).__init__()\n",
        "        self.size = size\n",
        "        self.num_layers = num_layers\n",
        "        self.transform = nn.ModuleList() # A list of transform layers\n",
        "        self.gate = nn.ModuleList()      # A list of gate layers\n",
        "        self.dropout = torch.nn.Dropout(p=dropout)\n",
        "\n",
        "        for i in range(num_layers):\n",
        "            transform = nn.Linear(size, size)\n",
        "            gate = nn.Linear(size, size)\n",
        "            self.transform.append(transform)\n",
        "            self.gate.append(gate)\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward-prop.\n",
        "        Returns a tensor with the same dimensions as input tensor\n",
        "        \"\"\"\n",
        "\n",
        "        transformed = F.relu(self.transform[0](x))  # transform with the first transform layer\n",
        "        g = F.sigmoid(self.gate[0](x))              # calculate how much of the transformed input to keep\n",
        "\n",
        "        out = self.dropout(g*transformed + (1-g)*x)               # combine input and transformed input with ratio of g\n",
        "\n",
        "        # If there are additional layers\n",
        "        for i in range(self.num_layers):\n",
        "            transformed = F.relu(self.transform[i](out))\n",
        "            g = F.sigmoid(self.gate[i](out))\n",
        "            out = self.dropout(g*transformed+(1-g)*out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zaftUlVLOad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CRF(nn.Module):\n",
        "    \"\"\"\n",
        "    Confitional Random Field\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dim, tagset_size):\n",
        "        \"\"\"\n",
        "        hidden_dim: the size of word/BLSTM's output (which is the input size for CRF)\n",
        "        tagset_size: number of tags(depending on our dataset)\n",
        "        \"\"\"\n",
        "\n",
        "        super(CRF, self).__init__()\n",
        "        self.tagset_size = tagset_size\n",
        "        self.emission = nn.Linear(hidden_dim, self.tagset_size)\n",
        "        self.transition = nn.Parameter(torch.Tensor(self.tagset_size, self.tagset_size))\n",
        "        self.transition.data.zero_() # initializa the transition matrix to be all zeros\n",
        "\n",
        "    def forward(self, feats):\n",
        "        \"\"\"\n",
        "        feats:   output of word/BLSTM, a tensor of dimensions-(batch_size, timesteps, hidden_dim)\n",
        "        returns: CRF scores, a tensor of dimensions-(batch_size, timesteps, tagset_size, tagset_size)\n",
        "        \"\"\"\n",
        "        self.batch_size = feats.size(0)\n",
        "        self.timesteps  = feats.size(1)\n",
        "\n",
        "        emission_scores = self.emission(feats)  # (batch_size, timesteps, tagset_size)\n",
        "        # here we broadcast emission_scores in order to compute the total score later with transition score\n",
        "        emission_scores = emission_scores.unsqueeze(2).expand(self.batch_size, self.timesteps, self.tagset_size, self.tagset_size)  # (batch_size, timesteps, tagset_size, tagset_size)\n",
        "\n",
        "        crf_scores = emission_scores + self.transition.unsqueeze(0).unsqueeze(0)  # (batch_size, timesteps, tagset_size, tagset_size)\n",
        "        return crf_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAK0OBqVT61P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ViterbiLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Viterbi Loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tag_map):\n",
        "        \"\"\"\n",
        "        :param tag_map: tag map\n",
        "        \"\"\"\n",
        "        super(ViterbiLoss, self).__init__()\n",
        "        self.tagset_size = len(tag_map)\n",
        "        self.start_tag = tag_map['<start>']\n",
        "        self.end_tag = tag_map['<end>']\n",
        "\n",
        "    def forward(self, scores, targets, lengths):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param scores: CRF scores\n",
        "        :param targets: true tags indices in unrolled CRF scores\n",
        "        :param lengths: word sequence lengths\n",
        "        :return: viterbi loss\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size = scores.size(0)\n",
        "        word_pad_len = scores.size(1)\n",
        "\n",
        "        # Gold score\n",
        "\n",
        "        targets = targets.unsqueeze(2)\n",
        "        scores_at_targets = torch.gather(scores.view(batch_size, word_pad_len, -1), 2, targets).squeeze(\n",
        "            2)  # (batch_size, word_pad_len)\n",
        "\n",
        "        # Everything is already sorted by lengths\n",
        "        scores_at_targets = pack_padded_sequence(scores_at_targets, lengths, batch_first=True)\n",
        "        gold_score = scores_at_targets.data.sum()\n",
        "\n",
        "        # All paths' scores\n",
        "\n",
        "        # Create a tensor to hold accumulated sequence scores at each current tag\n",
        "        scores_upto_t = torch.zeros(batch_size, self.tagset_size).to(device)\n",
        "\n",
        "        for t in range(max(lengths)):\n",
        "            batch_size_t = sum([l > t for l in lengths])  # effective batch size (sans pads) at this timestep\n",
        "            if t == 0:\n",
        "                scores_upto_t[:batch_size_t] = scores[:batch_size_t, t, self.start_tag, :]  # (batch_size, tagset_size)\n",
        "            else:\n",
        "                # We add scores at current timestep to scores accumulated up to previous timestep, and log-sum-exp\n",
        "                # Remember, the cur_tag of the previous timestep is the prev_tag of this timestep\n",
        "                # So, broadcast prev. timestep's cur_tag scores along cur. timestep's cur_tag dimension\n",
        "                scores_upto_t[:batch_size_t] = log_sum_exp(\n",
        "                    scores[:batch_size_t, t, :, :] + scores_upto_t[:batch_size_t].unsqueeze(2),\n",
        "                    dim=1)  # (batch_size, tagset_size)\n",
        "\n",
        "        # We only need the final accumulated scores at the <end> tag\n",
        "        all_paths_scores = scores_upto_t[:, self.end_tag].sum()\n",
        "\n",
        "        viterbi_loss = all_paths_scores - gold_score\n",
        "        viterbi_loss = viterbi_loss / batch_size\n",
        "\n",
        "        return viterbi_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4w6o7q1jQxtd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LM_LSTM_CRF(nn.Module):\n",
        "    \"\"\"\n",
        "    The encompassing LM-LSTM-CRF\n",
        "    \"\"\"\n",
        "    def __init__(self, tagset_size, charset_size, char_emb_dim, char_rnn_dim, char_rnn_layers, vocab_size,\n",
        "                 lm_vocab_size, word_emb_dim, word_rnn_dim, word_rnn_layers, dropout, highway_layers=1):\n",
        "        \"\"\"\n",
        "        tagset_size:   number of tags\n",
        "        charset_size:  size of character vocabulary\n",
        "        char_emb_dim:  size of character embeddings\n",
        "        char_rnn_dim:  size of charactor RNNS/LSTMs\n",
        "        char_rnn_layers: number of layers in character RNN/LSTMs\n",
        "        vocab_size:    input vocabulary size\n",
        "        lm_vocab_size: vocabulary size of language models (in-corpus words subject to word frequency threshold)\n",
        "        word_emb_dim:  size of word embeddings\n",
        "        word_rnn_dim:  size of word RNN/BLSTM\n",
        "        word_rnn_layers: number of layers in word RNNs/LSTMs\n",
        "        dropout:       dropout\n",
        "        highway_layers: number of transform and gate layers\n",
        "        \"\"\"\n",
        "        \n",
        "        super(LM_LSTM_CRF, self).__init__()\n",
        "\n",
        "        self.tagset_size  = tagset_size # this is the size of the outout vocab of the tagging model\n",
        "\n",
        "        self.charset_size = charset_size\n",
        "        self.char_emb_dim = char_emb_dim\n",
        "        self.char_rnn_dim = char_rnn_dim\n",
        "        self.char_rnn_layers = char_rnn_layers\n",
        "\n",
        "        self.wordset_size  = vocab_size     # this is the size of the input vocab (embedding layer) of the tagging model\n",
        "        self.lm_vocab_size = lm_vocab_size  # this is the size of the output vocab of the language model\n",
        "        self.word_emb_dim  = word_emb_dim\n",
        "        self.word_rnn_dim  = word_rnn_dim\n",
        "        self.word_rnn_layers = word_rnn_layers\n",
        "        \n",
        "        self.highway_layers = highway_layers\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # charactor embedding layer\n",
        "        self.char_embeds = nn.Embedding(self.charset_size, self.char_emb_dim) \n",
        "\n",
        "        # forward char LSTM\n",
        "        self.forw_char_lstm = nn.LSTM(input_size=self.char_emb_dim, hidden_size=self.char_rnn_dim, \n",
        "                                      num_layers=self.char_rnn_layers, bidirectional=False, dropout = dropout)\n",
        "        # backward char LSTM\n",
        "        self.back_char_lstm = nn.LSTM(input_size=self.char_emb_dim, hidden_size=self.char_rnn_dim,\n",
        "                                      num_layers=self.char_rnn_layers, bidirectional=False, dropout = dropout)\n",
        "        \n",
        "        # word embedding layer\n",
        "        self.word_embeds = nn.Embedding(num_embeddings=self.wordset_size,embedding_dim=self.word_emb_dim)\n",
        "        # Define word-level bidirection LSTM\n",
        "        # Take note on the hidden_size\n",
        "        self.word_blstm   = nn.LSTM(self.word_emb_dim + self.char_rnn_dim * 2, # input_size\n",
        "                                    self.word_rnn_dim // 2,                    # hidden_size\n",
        "                                    # This is because Bi-directional LSTM will concat forward and backward output\n",
        "                                    # therefore we specify word_rnn_dim//2 but will get output size of word_rnn_dim\n",
        "                                    num_layers=self.word_rnn_layers,\n",
        "                                    bidirectional=True,\n",
        "                                    dropout=dropout\n",
        "                                    )\n",
        "        \n",
        "        # Conditinoal Random Field layer\n",
        "        self.crf = CRF(hidden_dim=self.word_rnn_dim,tagset_size=self.tagset_size)\n",
        "\n",
        "        # 3 places that we implemented highway connections\n",
        "        self.forw_lm_hw = Highway(size=self.char_rnn_dim,\n",
        "                                  num_layers=self.highway_layers,\n",
        "                                  dropout=dropout)\n",
        "        self.back_lm_hw = Highway(size=self.char_rnn_dim,\n",
        "                                  num_layers=self.highway_layers,\n",
        "                                  dropout=dropout)\n",
        "        self.subword_hw = Highway(2 * self.char_rnn_dim, \n",
        "                                  num_layers=self.highway_layers,\n",
        "                                  dropout=dropout)\n",
        "        \n",
        "        # Linear layers for language models, They are used for \"muti-task training\" for language models (predicting next word)\n",
        "        self.forw_lm_out = nn.Linear(self.char_rnn_dim, self.lm_vocab_size)\n",
        "        self.back_lm_out = nn.Linear(self.char_rnn_dim, self.lm_vocab_size)\n",
        "\n",
        "    def init_word_embedding(self, embedding):\n",
        "        \"\"\"\n",
        "        Initialize embeddings with pre-trained embeddings.\n",
        "\n",
        "        embedding: pre-trained embeddings to be loaded\n",
        "        \"\"\"\n",
        "        self.word_embeds.weights = nn.Parameter(embeddings)\n",
        "\n",
        "    def fine_tune_word_embeddings(self, fine_tune=False):\n",
        "        \"\"\"\n",
        "        Fine-tune embedding layer? (if using pre-trained embedding layer, consider no fine-tuning)\n",
        "\n",
        "        fine_tune: bool decides if fine_tune\n",
        "        \"\"\"\n",
        "        for p in self.word_embeds.parameters():\n",
        "            p.requires_grad = fine_tune\n",
        "    \n",
        "    def forward(self, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, wmaps, tmaps, wmap_lengths, cmap_lengths):\n",
        "        \"\"\"\n",
        "        cmaps_f: padded encoded forward  character sequences. (batch_size, char_pad_len)\n",
        "        cmaps_b: padded encoded backward character sequences. (batch_size, char_pad_len)\n",
        "        cmarkers_f: padded forward character markers.          (batch_size, word_pad_len)\n",
        "        cmarkers_b: padded backward character markers.         (batch_size, word_pad_len)\n",
        "        wmaps: padded encoded word sequences.                 (batch_size, word_pad_len)\n",
        "        tmaps: padded tag sequences.                          (batch_size, word_pad_len)\n",
        "        wmap_lengths: word sequence lengths.                  (batch_size)\n",
        "        cmap_lengths: character sequence lengths              (batch_size)\n",
        "        \"\"\"\n",
        "\n",
        "        self.batch_size   = cmaps_f.size(0)\n",
        "        self.word_pad_len = wmaps.size(1)\n",
        "\n",
        "        # Sort by decreasing true char. sequence length for grouping up for padding later\n",
        "        cmap_lengths, char_sort_ind = cmap_lengths.sort(dim=0, descending=True)\n",
        "        cmaps_f = cmaps_f[char_sort_ind]\n",
        "        cmaps_b = cmaps_b[char_sort_ind]\n",
        "        cmarkers_f = cmarkers_f[char_sort_ind]\n",
        "        cmarkers_b = cmarkers_b[char_sort_ind]\n",
        "        wmaps = wmaps[char_sort_ind]\n",
        "        tmaps = tmaps[char_sort_ind]\n",
        "        wmap_lengths = wmap_lengths[char_sort_ind]\n",
        "\n",
        "        # Embedding look-up for characters, turning each character to its embedding of char_emb_dim size\n",
        "        cf = self.char_embeds(cmaps_f)  # (batch_size, char_pad_len, char_emb_dim)\n",
        "        cb = self.char_embeds(cmaps_b)  # (batch_size, char_pad_len, char_emb_dim)\n",
        "\n",
        "        # Dropout\n",
        "        cf = self.dropout(cf)  # (batch_size, char_pad_len, char_emb_dim)\n",
        "        cb = self.dropout(cb)\n",
        "\n",
        "        # Pack padded sequence\n",
        "        cf = pack_padded_sequence(cf, lengths=cmap_lengths.tolist(), batch_first=True) # packed sequence of char_emb_dim, with real sequence lengths\n",
        "        cb = pack_padded_sequence(cb, lengths=cmap_lengths.tolist(), batch_first=True)\n",
        "\n",
        "        # LSTM for forward and backword language model & feature extraction for Bi-directional LSTM\n",
        "        cf, _ = self.forw_char_lstm(cf)  # packed sequence of char_rnn_dim, with real sequence lengths\n",
        "        cb, _ = self.back_char_lstm(cb)   \n",
        "\n",
        "        # Unpack packed sequence\n",
        "        cf, _ = pad_packed_sequence(cf, batch_first=True) # (batch, max_char_len_in_batch, char_rnn_dim)\n",
        "        cb, _ = pad_packed_sequence(cb, batch_first=True) \n",
        "\n",
        "        # Sanity check\n",
        "        assert cf.size(1) == max(cmap_lengths.tolist()) == list(cmap_lengths)[0]\n",
        "\n",
        "        # Select RNN outpus only at marker points (spaces in the character sequence)\n",
        "        cmarkers_f = cmarkers_f.unsqueeze(2).expand(self.batch_size, self.word_pad_len, self.char_rnn_dim)\n",
        "        cmarkers_b = cmarkers_b.unsqueeze(2).expand(self.batch_size, self.word_pad_len, self.char_rnn_dim)\n",
        "        # torch.gather return output same dim as index, with value taken from cf. In this case we can see that dim=1 of output might be different from input(cf)\n",
        "        cf_selected = torch.gather(cf, 1, index=cmarkers_f) # (batch_size, word_pad_len, char_rnn_dim)\n",
        "        cb_selected = torch.gather(cb, 1, index=cmarkers_b)\n",
        "\n",
        "        # Only for co-training language model(to boost performance), not useful for tagging after model is trained\n",
        "        if self.training:   # this toggle is true when we set model.train(), false if we set model.eval()\n",
        "            lm_f = self.forw_lm_hw(self.dropout(cf_selected))  # (batch_size, word_pad_len, char_rnn_dim)\n",
        "            lm_b = self.back_lm_hw(self.dropout(cb_selected))\n",
        "            lm_f_scores = self.forw_lm_out(self.dropout(lm_f))  # (batch_size, word_pad_len, lm_vocab_size)\n",
        "            lm_b_scores = self.back_lm_out(self.dropout(lm_b))\n",
        "\n",
        "        # Sort by decreasing true word sequence length\n",
        "        wmap_lengths, word_sort_ind = wmap_lengths.sort(dim=0, descending=True)\n",
        "        wmaps = wmaps[word_sort_ind]\n",
        "        tmaps = tmaps[word_sort_ind]\n",
        "        cf_selected = cf_selected[word_sort_ind]  # for language model\n",
        "        cb_selected = cb_selected[word_sort_ind]\n",
        "        if self.training:\n",
        "            lm_f_scores = lm_f_scores[word_sort_ind]\n",
        "            lm_b_scores = lm_b_scores[word_sort_ind]\n",
        "\n",
        "        # Embedding look-up for words\n",
        "        w = self.word_embeds(wmaps)  # (batch_size, word_pad_len, word_emb_dim)\n",
        "        w = self.dropout(w)\n",
        "\n",
        "        # Sub-word information at each word\n",
        "        subword = self.subword_hw(self.dropout(torch.cat((cf_selected, cb_selected), dim=2)))  # (batch_size, word_pad_len, 2 * char_rnn_dim)\n",
        "        subword = self.dropout(subword)\n",
        "\n",
        "        # Concatenate word embeddings and sub-word features\n",
        "        w = torch.cat((w, subword), dim=2)  # (batch_size, word_pad_len, 2*char_rnn_dim+word_emb_dim)\n",
        "\n",
        "        # Pack padded sequence\n",
        "        w = pack_padded_sequence(w, list(wmap_lengths), batch_first=True)   # packed sequence of word_emb_dim+2*char_rnn_dim\n",
        "\n",
        "        # Bi-directional LSTM\n",
        "        w, _ = self.word_blstm(w)   # packed sequence of word_rnn_dim, with real sequence lengths\n",
        "        \n",
        "        # Unpack packed sequence\n",
        "        w, _ = pad_packed_sequence(w, batch_first=True)  # (batch_size, max_word_len_in_batch, word_rnn_dim)\n",
        "        w = self.dropout(w)\n",
        "\n",
        "        crf_scores = self.crf(w)     # (batch_size, max_word_len_in_batch, tagset_size, tagset_size)\n",
        "\n",
        "        if self.training:\n",
        "            return crf_scores, lm_f_scores, lm_b_scores, wmaps, tmaps, wmap_lengths, word_sort_ind, char_sort_ind\n",
        "        else:\n",
        "            return crf_scores, wmaps, tmaps, wmap_lengths, word_sort_ind, char_sort_ind  # sort inds to reorder, if req."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XRFQhNj_Bfr",
        "colab_type": "code",
        "outputId": "1e027da7-3474-4783-80a3-324c99d8f54e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My\\ Drive"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n",
            "/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ixkzwHv_ye6",
        "colab_type": "code",
        "outputId": "9e4c6463-b81d-48ba-c624-389a3dda675e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 2018\n",
            " 2019\n",
            "'Attention in deep learning.md'\n",
            "'Colab Notebooks'\n",
            " data\n",
            " Enoava\n",
            "'LSTM in PyTorch.md'\n",
            "'LSTM in PyTorch.pdf'\n",
            "'Response to comments_MECHMT_2018_1107_-V4_17_Dec.docx'\n",
            " Snaps\n",
            " Stack-presentation-Dermotologist.png\n",
            "'Starwars Project Video V2.mp4'\n",
            " YOLO.md\n",
            " YOLO.pdf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIwrkKoX_zxO",
        "colab_type": "code",
        "outputId": "d4595d3c-98c6-458e-fd48-a623fde65eef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /gdrive/My\\ Drive/data/embeddings"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive/data/embeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obm_y4aE__pm",
        "colab_type": "code",
        "outputId": "8e7b1ce3-8813-4fed-91ee-de4f20f761c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "glove.6B.100d.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATeBjYOFA5pg",
        "colab_type": "code",
        "outputId": "0dfa8142-bad8-4a13-80d4-1ccebc97cc44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive/data/embeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUmg97Kl9Z-v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "# Rewrite the __getitem__ and add __len__\n",
        "class WCDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for the LM-LSTM-CRF model. To be used by a PyTorch DataLoader to feed batches to the model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, wmaps, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, tmaps, wmap_lengths, cmap_lengths):\n",
        "        \"\"\"\n",
        "        :param wmaps: padded encoded word sequences\n",
        "        :param cmaps_f: padded encoded forward character sequences\n",
        "        :param cmaps_b: padded encoded backward character sequences\n",
        "        :param cmarkers_f: padded forward character markers\n",
        "        :param cmarkers_b: padded backward character markers\n",
        "        :param tmaps: padded encoded tag sequences (indices in unrolled CRF scores)\n",
        "        :param wmap_lengths: word sequence lengths\n",
        "        :param cmap_lengths: character sequence lengths\n",
        "        \"\"\"\n",
        "        self.wmaps = wmaps\n",
        "        self.cmaps_f = cmaps_f\n",
        "        self.cmaps_b = cmaps_b\n",
        "        self.cmarkers_f = cmarkers_f\n",
        "        self.cmarkers_b = cmarkers_b\n",
        "        self.tmaps = tmaps\n",
        "        self.wmap_lengths = wmap_lengths\n",
        "        self.cmap_lengths = cmap_lengths\n",
        "\n",
        "        self.data_size = self.wmaps.size(0)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.wmaps[i], self.cmaps_f[i], self.cmaps_b[i], self.cmarkers_f[i], self.cmarkers_b[i], self.tmaps[i], \\\n",
        "               self.wmap_lengths[i], self.cmap_lengths[i]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHc2_4Nj-iTF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ViterbiDecoder():\n",
        "    \"\"\"\n",
        "    Viterbi Decoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tag_map):\n",
        "        \"\"\"\n",
        "        :param tag_map: tag map\n",
        "        \"\"\"\n",
        "        self.tagset_size = len(tag_map)\n",
        "        self.start_tag = tag_map['<start>']\n",
        "        self.end_tag = tag_map['<end>']\n",
        "\n",
        "    def decode(self, scores, lengths):\n",
        "        \"\"\"\n",
        "        :param scores: CRF scores\n",
        "        :param lengths: word sequence lengths\n",
        "        :return: decoded sequences\n",
        "        \"\"\"\n",
        "        batch_size = scores.size(0)\n",
        "        word_pad_len = scores.size(1)\n",
        "\n",
        "        # Create a tensor to hold accumulated sequence scores at each current tag\n",
        "        scores_upto_t = torch.zeros(batch_size, self.tagset_size)\n",
        "\n",
        "        # Create a tensor to hold back-pointers\n",
        "        # i.e., indices of the previous_tag that corresponds to maximum accumulated score at current tag\n",
        "        # Let pads be the <end> tag index, since that was the last tag in the decoded sequence\n",
        "        backpointers = torch.ones((batch_size, max(lengths), self.tagset_size), dtype=torch.long) * self.end_tag\n",
        "\n",
        "        for t in range(max(lengths)):\n",
        "            batch_size_t = sum([l > t for l in lengths])  # effective batch size (sans pads) at this timestep\n",
        "            if t == 0:\n",
        "                scores_upto_t[:batch_size_t] = scores[:batch_size_t, t, self.start_tag, :]  # (batch_size, tagset_size)\n",
        "                backpointers[:batch_size_t, t, :] = torch.ones((batch_size_t, self.tagset_size),\n",
        "                                                               dtype=torch.long) * self.start_tag\n",
        "            else:\n",
        "                # We add scores at current timestep to scores accumulated up to previous timestep, and\n",
        "                # choose the previous timestep that corresponds to the max. accumulated score for each current timestep\n",
        "                scores_upto_t[:batch_size_t], backpointers[:batch_size_t, t, :] = torch.max(\n",
        "                    scores[:batch_size_t, t, :, :] + scores_upto_t[:batch_size_t].unsqueeze(2),\n",
        "                    dim=1)  # (batch_size, tagset_size)\n",
        "\n",
        "        # Decode/trace best path backwards\n",
        "        decoded = torch.zeros((batch_size, backpointers.size(1)), dtype=torch.long)\n",
        "        pointer = torch.ones((batch_size, 1),\n",
        "                             dtype=torch.long) * self.end_tag  # the pointers at the ends are all <end> tags\n",
        "\n",
        "        for t in list(reversed(range(backpointers.size(1)))):\n",
        "            decoded[:, t] = torch.gather(backpointers[:, t, :], 1, pointer).squeeze(1)\n",
        "            pointer = decoded[:, t].unsqueeze(1)  # (batch_size, 1)\n",
        "\n",
        "        # Sanity check\n",
        "        assert torch.equal(decoded[:, 0], torch.ones((batch_size), dtype=torch.long) * self.start_tag)\n",
        "\n",
        "        # Remove the <starts> at the beginning, and append with <ends> (to compare to targets, if any)\n",
        "        decoded = torch.cat([decoded[:, 1:], torch.ones((batch_size, 1), dtype=torch.long) * self.start_tag],\n",
        "                            dim=1)\n",
        "\n",
        "        return decoded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGatVIt56gXe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import sys\n",
        "from utils import *\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI_mY6W265rh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "task = 'ner'        # tagging task, choose between [ner, pos]\n",
        "train_file = '/gdrive/My Drive/data/CoNLL-2003/eng.train'\n",
        "val_file   = '/gdrive/My Drive/data/CoNLL-2003/eng.testa'\n",
        "test_file  = '/gdrive/My Drive/data/CoNLL-2003/eng.testb'\n",
        "emb_file   = '/gdrive/My Drive/data/embeddings/glove.6B.100d.txt'\n",
        "min_word_freq = 5 # threshold for word frequency to be recognized not as xxunk\n",
        "min_char_freq = 1 # same thing for char frequency\n",
        "caseless   = True # lowercase everything?\n",
        "expand_vocab = True # expand model's input vocabulary to the pre-trained embedding vocabulary?\n",
        "\n",
        "# Model parameters\n",
        "char_emb_dim = 30 # character embedding size\n",
        "with open(emb_file, 'r') as f:\n",
        "    word_emb_dim = len(f.readline().split(' ')) - 1  # word embdding size, \"-1\" is because in the txt file the first place is the word itself, followed by the actual embeddings\n",
        "word_rnn_dim = 300  # word BLSTM hidden size\n",
        "char_rnn_dim = 300  # character RNN size\n",
        "char_rnn_layers = 1 # number of layers in character RNN\n",
        "word_rnn_layers = 1 # number of layers in word BLSTM\n",
        "highway_layers  = 1 # number of layers in highway network\n",
        "dropout = 0.5       # universal dropout rate\n",
        "fine_tune_word_embeddings = False\n",
        "\n",
        "# Training parameters\n",
        "start_epoch = 0   # start at this epoch\n",
        "batch_size  = 10  # batch size\n",
        "lr = 0.0015  \n",
        "lr_decay = 0.05\n",
        "momentum = 0.9\n",
        "workers  = 4\n",
        "epochs   = 200    # number of epochs without triggering early stoping\n",
        "grad_clip = 5.\n",
        "print_freq = 100  # print every ___ batches\n",
        "best_f1  = 0.\n",
        "checkpoint = None # Model checkpoint to load. None if training from scratch\n",
        "\n",
        "tag_ind = 1 if task == 'pos' else 3 # choose column in CoNLL 2003 dataset\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnLn_7JoDGUf",
        "colab_type": "code",
        "outputId": "12936ed0-1435-4007-d15c-d5718c15bc9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "global best_f1, epochs_since_improvement, checkpoint, start_epoch, word_map, char_map, tag_map\n",
        "\n",
        "# Read training and validation data\n",
        "train_words, train_tags = read_words_tags(train_file, tag_ind, caseless)\n",
        "val_words, val_tags = read_words_tags(val_file, tag_ind, caseless)\n",
        "\n",
        "if checkpoint is not None:\n",
        "    checkpoint = torch.load(checkpoint)\n",
        "    model = checkpoint['model']\n",
        "    optimizer = checkpoint['optimizer']\n",
        "    word_map  = checkpoint['word_map']\n",
        "    lm_vocab_size = checkpoint['lm_vocab_size']\n",
        "    tag_map   = checkpoint['tag_map']\n",
        "    char_map  = checkpoint['char_map']\n",
        "    start_epoch = checkpoint['epoch'] +1\n",
        "    best_f1   = checkpoint['f1']\n",
        "else:\n",
        "    # create word, char, tag maps\n",
        "    # maps are essentially dictionaries that map a token to an integer\n",
        "    word_map, char_map, tag_map = create_maps(train_words+val_words,train_tags+val_tags, min_word_freq, min_char_freq)\n",
        "\n",
        "    # load pre-trained embeddings, if expand_vocab==True, word_map expand to embedding_word_map\n",
        "    # lm_vocab_size is the word_map size before expand to \"out-of-corpus vocab\"\n",
        "    embeddings, word_map, lm_vocab_size = load_embeddings(emb_file, word_map, expand_vocab)\n",
        "\n",
        "    model = LM_LSTM_CRF(tagset_size=len(tag_map),\n",
        "                        charset_size=len(char_map),\n",
        "                        char_emb_dim=char_emb_dim,\n",
        "                        char_rnn_dim=char_rnn_dim,\n",
        "                        char_rnn_layers=char_rnn_layers,\n",
        "                        vocab_size=len(word_map),       # This is the length after expand\n",
        "                        lm_vocab_size=lm_vocab_size,    # len(word_map) before expand, not influenced by the embedding vocab\n",
        "                        word_emb_dim=word_emb_dim,\n",
        "                        word_rnn_dim=word_rnn_dim,\n",
        "                        word_rnn_layers=word_rnn_layers,\n",
        "                        dropout=dropout,\n",
        "                        highway_layers=highway_layers).to(device)\n",
        "    model.init_word_embedding(embeddings.to(device)) # initializa embedding layers with pre-trained embeddings.(Essentially we just make it nn.Parameter)\n",
        "    model.fine_tune_word_embeddings(fine_tune_word_embeddings)    # decide if these nn.Parameters has requires_grad = True (trainable)\n",
        "    optimizer = optim.SGD(params=filter(lambda p: p.requires_grad, model.parameters()), lr=lr, momentum=momentum)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedding length is 100.\n",
            "You have elected to include embeddings that are out-of-corpus.\n",
            "\n",
            "Loading embeddings...\n",
            "'word_map' is being updated accordingly.\n",
            "\n",
            "Done.\n",
            " Embedding vocabulary: 400054\n",
            " Language Model vocabulary: 4671.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7eiHeWJ7zSf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining how one training step will be computed.\n",
        "# train() is used in each epoch later\n",
        "def train(train_loader, model, lm_criterion, crf_criterion, optimizer, epoch, vb_decoder):\n",
        "    \"\"\"\n",
        "    train_loader: DataLoader for training data\n",
        "    model: LM_LSTM_CRF\n",
        "    lm_criterion:  nn.CrossEntropyLoss()\n",
        "    crf_criterion: ViterbiLoss()\n",
        "    optimizer: SGD/adam/adbound whatever your choice is\n",
        "    epoch: epoch number\n",
        "    vb_decoder: viterbi decoder(to decode and find F1 score)\n",
        "    \"\"\"\n",
        "    model.train()  # training mode, so dropout\n",
        "\n",
        "    batch_time = AverageMeter()  # forward prop. + back prop. time per batch\n",
        "    data_time = AverageMeter()  # data loading time per batch\n",
        "    ce_losses = AverageMeter()  # cross entropy loss\n",
        "    vb_losses = AverageMeter()  # viterbi loss\n",
        "    f1s = AverageMeter()  # f1 score\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    # Batches\n",
        "    for i, (wmaps, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, tmaps, wmap_lengths, cmap_lengths) in enumerate(train_loader):\n",
        "        data_time.update(time.time()-start)\n",
        "\n",
        "        max_word_len = max(wmap_lengths.tolist())\n",
        "        max_char_len = max(cmap_lengths.tolist())\n",
        "\n",
        "        # Reduce batch's padded length to maximum in-batch sequence. \n",
        "        # This saves some compute on nn.Linear layers (RNNs are not affected, since they don't compute over the pads)\n",
        "        wmaps = wmaps[:, :max_word_len].to(device)\n",
        "        cmaps_f = cmaps_f[:, :max_char_len].to(device)\n",
        "        cmaps_b = cmaps_b[:, :max_char_len].to(device)\n",
        "        cmarkers_f = cmarkers_f[:, :max_word_len].to(device)\n",
        "        cmarkers_b = cmarkers_b[:, :max_word_len].to(device)\n",
        "        tmaps = tmaps[:, :max_word_len].to(device)\n",
        "        wmap_lengths = wmap_lengths.to(device)\n",
        "        cmap_lengths = cmap_lengths.to(device)\n",
        "\n",
        "        # Forward prop.\n",
        "        crf_scores, lm_f_scores, lm_b_scores, wmaps_sorted, tmaps_sorted, wmap_lengths_sorted, _, __ = model(cmaps_f,\n",
        "                                                                                                             cmaps_b,\n",
        "                                                                                                             cmarkers_f,\n",
        "                                                                                                             cmarkers_b,\n",
        "                                                                                                             wmaps,\n",
        "                                                                                                             tmaps,\n",
        "                                                                                                             wmap_lengths,\n",
        "                                                                                                             cmap_lengths)\n",
        "        \n",
        "        # LM loss\n",
        "\n",
        "        # We don't predict the next word at the pads or <end> tokens\n",
        "        # Hence, only predict [word1, word2, word3, word4] among [word1, word2, word3, word4,<pad>,<pad>,<pad>,<pad>,<pad>,<end>]\n",
        "        # So prediction lengths are word sequence lengths -1\n",
        "        lm_lengths = wmap_lengths_sorted - 1 # (batch_size) the effective length of each row\n",
        "        lm_lengths = lm_lengths.tolist()\n",
        "\n",
        "        # Remove scores at timesteps we won't predict at\n",
        "        # pack_padded_sequence is a good trick to do this (145 PyTorch tricks(my other repo)---Trick #11)\n",
        "        lm_f_scores = pack_padded_sequence(lm_f_scores, lm_lengths, batch_first=True)\n",
        "        lm_b_scores = pack_padded_sequence(lm_b_scores, lm_lengths, batch_first=True)\n",
        "\n",
        "        # For the forward sequence, targets are from the second word onwards, up to <end>\n",
        "        # (timestep -> target) ...dunston -> checks, ...checks -> in, ...in -> <end>\n",
        "        lm_f_targets = wmaps_sorted[:, 1:]\n",
        "        lm_f_targets = pack_padded_sequence(lm_f_targets, lm_lengths, batch_first=True)\n",
        "\n",
        "        # For the backward sequence, targets are <end> followed by all words except the last word\n",
        "        # ...notsnud -> <end>, ...skcehc -> dunston, ...ni -> checks\n",
        "        lm_b_targets = torch.cat([torch.LongTensor([word_map['<end>']] * wmaps_sorted.size(0)).unsqueeze(1).to(device), \n",
        "                                  wmaps_sorted], dim=1)\n",
        "        lm_b_targets = pack_padded_sequence(lm_b_targets, lm_lengths, batch_first=True)\n",
        "        \n",
        "        # Calculate loss\n",
        "        ce_loss = lm_criterion(lm_f_scores.data, lm_f_targets.data) + lm_criterion(lm_b_scores.data, lm_b_targets.data)\n",
        "        vb_loss = crf_criterion(crf_scores, tmaps_sorted, wmap_lengths_sorted)\n",
        "        loss = ce_loss + vb_loss\n",
        "\n",
        "        # Back-prop\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        if grad_clip is not None:\n",
        "            clip_gradient(optimizer, grad_clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        # Viterbi decode to find accuracy/F1\n",
        "        decoded = vb_decoder.decode(crf_scores.to(\"cpu\"), wmap_lengths_sorted.to(\"cpu\"))\n",
        "\n",
        "        # Remove timesteps we won't predict at, and also <end> tags, because to predict them would be cheating\n",
        "        decoded      = pack_padded_sequence(decoded, lm_lengths, batch_first=True)\n",
        "        tmaps_sorted = tmaps_sorted % vb_decoder.tagset_size  # actual target indices (see create_input_tensors())\n",
        "        tmaps_sorted = pack_padded_sequence(tmaps_sorted, lm_lengths, batch_first=True)\n",
        "\n",
        "        # Compute F1\n",
        "        f1 = f1_score(tmaps_sorted.data.to(\"cpu\").numpy(), decoded.data.numpy(), average='macro')\n",
        "\n",
        "        # Keep track of metrics\n",
        "        ce_losses.update(ce_loss.item(), sum(lm_lengths))\n",
        "        vb_losses.update(vb_loss.item(), crf_scores.size(0))\n",
        "        batch_time.update(time.time() - start)\n",
        "        f1s.update(f1, sum(lm_lengths))\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "        # Print training status\n",
        "        if i % print_freq == 0:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Data Load Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "                  'CE Loss {ce_loss.val:.4f} ({ce_loss.avg:.4f})\\t'\n",
        "                  'VB Loss {vb_loss.val:.4f} ({vb_loss.avg:.4f})\\t'\n",
        "                  'F1 {f1.val:.3f} ({f1.avg:.3f})'.format(epoch, i, len(train_loader),\n",
        "                                                          batch_time=batch_time,\n",
        "                                                          data_time=data_time, ce_loss=ce_losses,\n",
        "                                                          vb_loss=vb_losses, f1=f1s))\n",
        "def validate(val_loader, model, crf_criterion, vb_decoder):\n",
        "    \"\"\"\n",
        "    val_loader:    Dataloader for validation data\n",
        "    model:         Model\n",
        "    crf_criterion: Viterbi loss layer\n",
        "    return:        validation F1 score\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    batch_time = AverageMeter()\n",
        "    vb_losses  = AverageMeter()\n",
        "    f1s        = AverageMeter()\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    # validation loops\n",
        "    for i, (wmaps, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, tmaps, wmap_lengths, cmap_lengths) in enumerate(val_loader):\n",
        "        max_word_len = max(wmap_lengths.tolist())\n",
        "        max_char_len = max(cmap_lengths.tolist())\n",
        "\n",
        "        # Reduce batch's padded length to maximum in-batch sequence\n",
        "        # This saves some compute on nn.Linear layers (RNNs are unaffected, since they don't compute over the pads)\n",
        "        wmaps = wmaps[:, :max_word_len].to(device)\n",
        "        cmaps_f = cmaps_f[:, :max_char_len].to(device)\n",
        "        cmaps_b = cmaps_b[:, :max_char_len].to(device)\n",
        "        cmarkers_f = cmarkers_f[:, :max_word_len].to(device)\n",
        "        cmarkers_b = cmarkers_b[:, :max_word_len].to(device)\n",
        "        tmaps = tmaps[:, :max_word_len].to(device)\n",
        "        wmap_lengths = wmap_lengths.to(device)\n",
        "        cmap_lengths = cmap_lengths.to(device)\n",
        "\n",
        "        # Forward prop.\n",
        "        crf_scores, wmaps_sorted, tmaps_sorted, wmap_lengths_sorted, _, __ = model(cmaps_f,\n",
        "                                                                                    cmaps_b,\n",
        "                                                                                    cmarkers_f,\n",
        "                                                                                    cmarkers_b,\n",
        "                                                                                    wmaps,\n",
        "                                                                                    tmaps,\n",
        "                                                                                    wmap_lengths,\n",
        "                                                                                    cmap_lengths)\n",
        "\n",
        "        # Viterbi / CRF layer loss\n",
        "        vb_loss = crf_criterion(crf_scores, tmaps_sorted, wmap_lengths_sorted)\n",
        "\n",
        "        # Viterbi decode to find accuracy / f1\n",
        "        decoded = vb_decoder.decode(crf_scores.to(\"cpu\"), wmap_lengths_sorted.to(\"cpu\"))\n",
        "\n",
        "        # Remove timesteps we won't predict at, and also <end> tags, because to predict them would be cheating\n",
        "        decoded      = pack_padded_sequence(decoded, (wmap_lengths_sorted - 1).tolist(), batch_first=True)\n",
        "        tmaps_sorted = tmaps_sorted % vb_decoder.tagset_size  # actual target indices (see create_input_tensors())\n",
        "        tmaps_sorted = pack_padded_sequence(tmaps_sorted, (wmap_lengths_sorted - 1).tolist(), batch_first=True)\n",
        "\n",
        "        # f1\n",
        "        f1 = f1_score(tmaps_sorted.data.to(\"cpu\").numpy(), decoded.data.numpy(), average='macro')\n",
        "\n",
        "        # Keep track of metrics\n",
        "        vb_losses.update(vb_loss.item(), crf_scores.size(0))\n",
        "        f1s.update(f1, sum((wmap_lengths_sorted - 1).tolist()))\n",
        "        batch_time.update(time.time() - start)\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "        if i % print_freq == 0:\n",
        "            print('Validation: [{0}/{1}]\\t'\n",
        "                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'VB Loss {vb_loss.val:.4f} ({vb_loss.avg:.4f})\\t'\n",
        "                  'F1 Score {f1.val:.3f} ({f1.avg:.3f})\\t'.format(i, len(val_loader), batch_time=batch_time,\n",
        "                                                                  vb_loss=vb_losses, f1=f1s))\n",
        "\n",
        "    print(\n",
        "        '\\n * LOSS - {vb_loss.avg:.3f}, F1 SCORE - {f1.avg:.3f}\\n'.format(vb_loss=vb_losses,\n",
        "                                                                          f1=f1s))\n",
        "\n",
        "    return f1s.avg    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCUtUGyqDAuu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "859a4daa-01db-48e4-df89-7f9573184380"
      },
      "source": [
        "# Loss funcitons\n",
        "lm_criterion  = nn.CrossEntropyLoss().to(device)\n",
        "crf_criterion = ViterbiLoss(tag_map).to(device)\n",
        "\n",
        "# Since the language model's vocab is restricted on in-corpus indices, encode training/val with only these!\n",
        "# word_map might have been expanded, and in-corpus words eliminated due to low frequency might still be added because\n",
        "# they exist in the pre-trained embeddings.(these embeddings are added after the <unk> token)\n",
        "temp_word_map = {k: v for k, v in word_map.items() if v <= word_map['<unk>']}\n",
        "\n",
        "# train_input = (padded_wmaps, padded_cmaps_f, padded_cmaps_b, padded_cmarkers_f, padded_cmarkers_b, \n",
        "#                padded_tmaps, wmap_lengths,   cmap_lengths)\n",
        "train_inputs = create_input_tensors(train_words, train_tags, temp_word_map, char_map,\n",
        "                                        tag_map)\n",
        "val_inputs = create_input_tensors(val_words, val_tags, temp_word_map, char_map, tag_map)\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = torch.utils.data.DataLoader(WCDataset(*train_inputs), batch_size=batch_size, shuffle=True,\n",
        "                                            num_workers=workers, pin_memory=False)\n",
        "val_loader = torch.utils.data.DataLoader(WCDataset(*val_inputs), batch_size=batch_size, shuffle=True,\n",
        "                                             num_workers=workers, pin_memory=False)\n",
        "\n",
        "# Viterbi decoder (to find accuracy during validation)\n",
        "vb_decoder = ViterbiDecoder(tag_map)\n",
        "\n",
        "# Epochs\n",
        "for epoch in range(start_epoch, epochs):\n",
        "    # One epoch's training\n",
        "    train(train_loader  = train_loader,\n",
        "          model         = model,\n",
        "          lm_criterion  = lm_criterion,\n",
        "          crf_criterion = crf_criterion,\n",
        "          optimizer     = optimizer,\n",
        "          epoch         = epoch,\n",
        "          vb_decoder    = vb_decoder\n",
        "          )\n",
        "    # One epoch's validation\n",
        "    val_f1 = validate(val_loader = val_loader,\n",
        "                      model      = model,\n",
        "                      crf_criterion = crf_criterion,\n",
        "                      vb_decoder    = vb_decoder)\n",
        "    is_best = val_f1 > best_f1\n",
        "    if not is_best:\n",
        "        epochs_since_improvement += 1\n",
        "        print(\"\\nEpochs since improvement: %d\\n\" % (epochs_since_improvement,))\n",
        "    else:\n",
        "        epochs_since_improvement = 0\n",
        "    \n",
        "    # Save checkpoint\n",
        "    save_checkpoint(epoch, model, optimizer, val_f1, word_map, char_map, tag_map, lm_vocab_size, is_best)\n",
        "\n",
        "    # Decay learning rate every epoch\n",
        "    adjust_learning_rate(optimizer, lr / (1 + (epoch + 1) * lr_decay))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: [0][0/1405]\tBatch Time 0.481 (0.481)\tData Load Time 0.190 (0.190)\tCE Loss 16.9152 (16.9152)\tVB Loss 36.0599 (36.0599)\tF1 0.027 (0.027)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: [0][100/1405]\tBatch Time 0.212 (0.221)\tData Load Time 0.004 (0.006)\tCE Loss 16.8109 (16.8691)\tVB Loss 9.1074 (12.3388)\tF1 0.172 (0.181)\n",
            "Epoch: [0][200/1405]\tBatch Time 0.182 (0.215)\tData Load Time 0.005 (0.005)\tCE Loss 16.6991 (16.8095)\tVB Loss 4.6801 (10.6299)\tF1 0.307 (0.189)\n",
            "Epoch: [0][300/1405]\tBatch Time 0.162 (0.212)\tData Load Time 0.004 (0.005)\tCE Loss 16.5286 (16.7476)\tVB Loss 4.8739 (9.9557)\tF1 0.187 (0.195)\n",
            "Epoch: [0][400/1405]\tBatch Time 0.276 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 16.2398 (16.6743)\tVB Loss 4.8017 (9.4936)\tF1 0.195 (0.199)\n",
            "Epoch: [0][500/1405]\tBatch Time 0.193 (0.207)\tData Load Time 0.004 (0.005)\tCE Loss 16.0174 (16.5842)\tVB Loss 5.0068 (9.1562)\tF1 0.239 (0.203)\n",
            "Epoch: [0][600/1405]\tBatch Time 0.170 (0.206)\tData Load Time 0.004 (0.005)\tCE Loss 15.5035 (16.4263)\tVB Loss 4.1356 (8.9302)\tF1 0.252 (0.209)\n",
            "Epoch: [0][700/1405]\tBatch Time 0.175 (0.205)\tData Load Time 0.004 (0.005)\tCE Loss 13.9566 (16.1939)\tVB Loss 7.4565 (8.6965)\tF1 0.223 (0.217)\n",
            "Epoch: [0][800/1405]\tBatch Time 0.191 (0.203)\tData Load Time 0.004 (0.005)\tCE Loss 13.4071 (15.9534)\tVB Loss 8.1281 (8.5068)\tF1 0.178 (0.222)\n",
            "Epoch: [0][900/1405]\tBatch Time 0.191 (0.203)\tData Load Time 0.004 (0.005)\tCE Loss 13.6147 (15.7239)\tVB Loss 7.6243 (8.3699)\tF1 0.247 (0.228)\n",
            "Epoch: [0][1000/1405]\tBatch Time 0.208 (0.203)\tData Load Time 0.005 (0.005)\tCE Loss 13.5379 (15.5144)\tVB Loss 8.1624 (8.2075)\tF1 0.396 (0.233)\n",
            "Epoch: [0][1100/1405]\tBatch Time 0.197 (0.203)\tData Load Time 0.004 (0.005)\tCE Loss 13.2367 (15.3231)\tVB Loss 6.1067 (8.0861)\tF1 0.492 (0.239)\n",
            "Epoch: [0][1200/1405]\tBatch Time 0.145 (0.202)\tData Load Time 0.005 (0.005)\tCE Loss 13.8813 (15.1535)\tVB Loss 3.4341 (7.9704)\tF1 0.547 (0.245)\n",
            "Epoch: [0][1300/1405]\tBatch Time 0.224 (0.202)\tData Load Time 0.006 (0.005)\tCE Loss 13.3291 (14.9920)\tVB Loss 6.9089 (7.8577)\tF1 0.179 (0.250)\n",
            "Epoch: [0][1400/1405]\tBatch Time 0.181 (0.203)\tData Load Time 0.004 (0.005)\tCE Loss 13.0456 (14.8425)\tVB Loss 6.4053 (7.7781)\tF1 0.413 (0.255)\n",
            "Validation: [0/325]\tBatch Time 0.319 (0.319)\tVB Loss 6.3493 (6.3493)\tF1 Score 0.438 (0.438)\t\n",
            "Validation: [100/325]\tBatch Time 0.084 (0.102)\tVB Loss 6.4071 (6.7882)\tF1 Score 0.295 (0.306)\t\n",
            "Validation: [200/325]\tBatch Time 0.090 (0.100)\tVB Loss 5.4492 (6.5883)\tF1 Score 0.386 (0.317)\t\n",
            "Validation: [300/325]\tBatch Time 0.082 (0.100)\tVB Loss 7.3011 (6.6962)\tF1 Score 0.226 (0.323)\t\n",
            "\n",
            " * LOSS - 6.644, F1 SCORE - 0.323\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LM_LSTM_CRF. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type CRF. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Highway. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ModuleList. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.001429\n",
            "\n",
            "Epoch: [1][0/1405]\tBatch Time 0.450 (0.450)\tData Load Time 0.208 (0.208)\tCE Loss 12.7584 (12.7584)\tVB Loss 5.1892 (5.1892)\tF1 0.407 (0.407)\n",
            "Epoch: [1][100/1405]\tBatch Time 0.110 (0.240)\tData Load Time 0.004 (0.007)\tCE Loss 12.7484 (12.9248)\tVB Loss 2.2776 (6.1361)\tF1 0.618 (0.343)\n",
            "Epoch: [1][200/1405]\tBatch Time 0.244 (0.226)\tData Load Time 0.004 (0.006)\tCE Loss 12.7303 (12.8668)\tVB Loss 5.9631 (6.3041)\tF1 0.401 (0.346)\n",
            "Epoch: [1][300/1405]\tBatch Time 0.212 (0.220)\tData Load Time 0.004 (0.005)\tCE Loss 12.6512 (12.8303)\tVB Loss 7.6987 (6.2887)\tF1 0.456 (0.345)\n",
            "Epoch: [1][400/1405]\tBatch Time 0.150 (0.215)\tData Load Time 0.004 (0.005)\tCE Loss 11.2375 (12.8007)\tVB Loss 4.7607 (6.2644)\tF1 0.467 (0.346)\n",
            "Epoch: [1][500/1405]\tBatch Time 0.241 (0.213)\tData Load Time 0.004 (0.005)\tCE Loss 12.9932 (12.7867)\tVB Loss 5.0374 (6.1465)\tF1 0.519 (0.355)\n",
            "Epoch: [1][600/1405]\tBatch Time 0.219 (0.213)\tData Load Time 0.004 (0.005)\tCE Loss 12.2982 (12.7466)\tVB Loss 7.8035 (6.1353)\tF1 0.294 (0.360)\n",
            "Epoch: [1][700/1405]\tBatch Time 0.212 (0.212)\tData Load Time 0.004 (0.005)\tCE Loss 12.1098 (12.7355)\tVB Loss 7.2726 (6.1406)\tF1 0.436 (0.362)\n",
            "Epoch: [1][800/1405]\tBatch Time 0.224 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 12.5760 (12.7236)\tVB Loss 4.3474 (6.0613)\tF1 0.624 (0.368)\n",
            "Epoch: [1][900/1405]\tBatch Time 0.223 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 11.8510 (12.7012)\tVB Loss 6.8803 (6.0371)\tF1 0.460 (0.371)\n",
            "Epoch: [1][1000/1405]\tBatch Time 0.204 (0.208)\tData Load Time 0.005 (0.005)\tCE Loss 12.0548 (12.6779)\tVB Loss 3.8663 (5.9707)\tF1 0.485 (0.376)\n",
            "Epoch: [1][1100/1405]\tBatch Time 0.222 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 12.7224 (12.6686)\tVB Loss 4.6774 (5.9478)\tF1 0.353 (0.379)\n",
            "Epoch: [1][1200/1405]\tBatch Time 0.217 (0.207)\tData Load Time 0.004 (0.005)\tCE Loss 13.3319 (12.6541)\tVB Loss 7.7777 (5.9298)\tF1 0.290 (0.383)\n",
            "Epoch: [1][1300/1405]\tBatch Time 0.217 (0.207)\tData Load Time 0.004 (0.005)\tCE Loss 11.1980 (12.6395)\tVB Loss 4.3382 (5.9098)\tF1 0.596 (0.387)\n",
            "Epoch: [1][1400/1405]\tBatch Time 0.147 (0.207)\tData Load Time 0.004 (0.005)\tCE Loss 11.3683 (12.6257)\tVB Loss 5.0291 (5.8764)\tF1 0.319 (0.390)\n",
            "Validation: [0/325]\tBatch Time 0.331 (0.331)\tVB Loss 6.3893 (6.3893)\tF1 Score 0.393 (0.393)\t\n",
            "Validation: [100/325]\tBatch Time 0.108 (0.101)\tVB Loss 8.5861 (5.2779)\tF1 Score 0.251 (0.447)\t\n",
            "Validation: [200/325]\tBatch Time 0.081 (0.107)\tVB Loss 5.2748 (5.2227)\tF1 Score 0.360 (0.448)\t\n",
            "Validation: [300/325]\tBatch Time 0.095 (0.109)\tVB Loss 3.8718 (5.2065)\tF1 Score 0.452 (0.448)\t\n",
            "\n",
            " * LOSS - 5.236, F1 SCORE - 0.448\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.001364\n",
            "\n",
            "Epoch: [2][0/1405]\tBatch Time 0.421 (0.421)\tData Load Time 0.211 (0.211)\tCE Loss 12.1857 (12.1857)\tVB Loss 5.1586 (5.1586)\tF1 0.449 (0.449)\n",
            "Epoch: [2][100/1405]\tBatch Time 0.295 (0.230)\tData Load Time 0.004 (0.007)\tCE Loss 12.5194 (12.3298)\tVB Loss 5.5591 (5.2930)\tF1 0.641 (0.450)\n",
            "Epoch: [2][200/1405]\tBatch Time 0.177 (0.223)\tData Load Time 0.004 (0.006)\tCE Loss 11.4688 (12.4111)\tVB Loss 6.7002 (5.3779)\tF1 0.579 (0.441)\n",
            "Epoch: [2][300/1405]\tBatch Time 0.246 (0.216)\tData Load Time 0.004 (0.005)\tCE Loss 11.9763 (12.4099)\tVB Loss 3.8551 (5.2871)\tF1 0.383 (0.432)\n",
            "Epoch: [2][400/1405]\tBatch Time 0.196 (0.214)\tData Load Time 0.004 (0.005)\tCE Loss 11.6206 (12.4238)\tVB Loss 5.9259 (5.3592)\tF1 0.388 (0.434)\n",
            "Epoch: [2][500/1405]\tBatch Time 0.165 (0.213)\tData Load Time 0.004 (0.005)\tCE Loss 12.5047 (12.3999)\tVB Loss 3.5266 (5.3974)\tF1 0.479 (0.435)\n",
            "Epoch: [2][600/1405]\tBatch Time 0.220 (0.213)\tData Load Time 0.004 (0.005)\tCE Loss 11.8235 (12.3856)\tVB Loss 7.4103 (5.3718)\tF1 0.396 (0.437)\n",
            "Epoch: [2][700/1405]\tBatch Time 0.241 (0.213)\tData Load Time 0.004 (0.005)\tCE Loss 12.2376 (12.3805)\tVB Loss 3.3563 (5.3385)\tF1 0.705 (0.440)\n",
            "Epoch: [2][800/1405]\tBatch Time 0.244 (0.212)\tData Load Time 0.004 (0.005)\tCE Loss 12.5731 (12.3648)\tVB Loss 5.9556 (5.3057)\tF1 0.217 (0.444)\n",
            "Epoch: [2][900/1405]\tBatch Time 0.159 (0.213)\tData Load Time 0.004 (0.005)\tCE Loss 12.6101 (12.3555)\tVB Loss 5.9715 (5.2918)\tF1 0.503 (0.446)\n",
            "Epoch: [2][1000/1405]\tBatch Time 0.223 (0.213)\tData Load Time 0.004 (0.005)\tCE Loss 12.1233 (12.3530)\tVB Loss 4.7788 (5.2768)\tF1 0.518 (0.447)\n",
            "Epoch: [2][1100/1405]\tBatch Time 0.243 (0.214)\tData Load Time 0.004 (0.005)\tCE Loss 12.5250 (12.3466)\tVB Loss 7.2386 (5.2870)\tF1 0.409 (0.449)\n",
            "Epoch: [2][1200/1405]\tBatch Time 0.187 (0.213)\tData Load Time 0.004 (0.005)\tCE Loss 11.7174 (12.3348)\tVB Loss 5.4695 (5.2392)\tF1 0.318 (0.452)\n",
            "Epoch: [2][1300/1405]\tBatch Time 0.185 (0.212)\tData Load Time 0.004 (0.005)\tCE Loss 11.8052 (12.3336)\tVB Loss 2.8315 (5.2019)\tF1 0.638 (0.455)\n",
            "Epoch: [2][1400/1405]\tBatch Time 0.164 (0.211)\tData Load Time 0.004 (0.005)\tCE Loss 11.7823 (12.3213)\tVB Loss 2.5017 (5.1773)\tF1 0.814 (0.456)\n",
            "Validation: [0/325]\tBatch Time 0.327 (0.327)\tVB Loss 5.7975 (5.7975)\tF1 Score 0.583 (0.583)\t\n",
            "Validation: [100/325]\tBatch Time 0.125 (0.114)\tVB Loss 2.6693 (4.7211)\tF1 Score 0.766 (0.505)\t\n",
            "Validation: [200/325]\tBatch Time 0.106 (0.112)\tVB Loss 4.6169 (4.5918)\tF1 Score 0.634 (0.514)\t\n",
            "Validation: [300/325]\tBatch Time 0.131 (0.114)\tVB Loss 9.7849 (4.6327)\tF1 Score 0.444 (0.514)\t\n",
            "\n",
            " * LOSS - 4.636, F1 SCORE - 0.512\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.001304\n",
            "\n",
            "Epoch: [3][0/1405]\tBatch Time 0.586 (0.586)\tData Load Time 0.214 (0.214)\tCE Loss 12.2780 (12.2780)\tVB Loss 5.7723 (5.7723)\tF1 0.358 (0.358)\n",
            "Epoch: [3][100/1405]\tBatch Time 0.236 (0.235)\tData Load Time 0.004 (0.007)\tCE Loss 12.1823 (12.2759)\tVB Loss 6.4645 (5.1117)\tF1 0.239 (0.463)\n",
            "Epoch: [3][200/1405]\tBatch Time 0.187 (0.227)\tData Load Time 0.004 (0.006)\tCE Loss 11.5717 (12.2815)\tVB Loss 3.7750 (4.9999)\tF1 0.415 (0.488)\n",
            "Epoch: [3][300/1405]\tBatch Time 0.255 (0.220)\tData Load Time 0.004 (0.005)\tCE Loss 12.7750 (12.2606)\tVB Loss 6.1349 (4.9380)\tF1 0.422 (0.487)\n",
            "Epoch: [3][400/1405]\tBatch Time 0.187 (0.217)\tData Load Time 0.004 (0.005)\tCE Loss 12.4346 (12.2219)\tVB Loss 10.3793 (4.9918)\tF1 0.249 (0.490)\n",
            "Epoch: [3][500/1405]\tBatch Time 0.177 (0.214)\tData Load Time 0.004 (0.005)\tCE Loss 12.1853 (12.2015)\tVB Loss 3.9316 (4.9418)\tF1 0.342 (0.490)\n",
            "Epoch: [3][600/1405]\tBatch Time 0.146 (0.212)\tData Load Time 0.005 (0.005)\tCE Loss 12.2703 (12.1794)\tVB Loss 4.0724 (4.8835)\tF1 0.508 (0.494)\n",
            "Epoch: [3][700/1405]\tBatch Time 0.274 (0.210)\tData Load Time 0.005 (0.005)\tCE Loss 12.8573 (12.1814)\tVB Loss 5.9189 (4.8091)\tF1 0.601 (0.495)\n",
            "Epoch: [3][800/1405]\tBatch Time 0.223 (0.209)\tData Load Time 0.005 (0.005)\tCE Loss 11.9022 (12.1690)\tVB Loss 7.7328 (4.8159)\tF1 0.288 (0.495)\n",
            "Epoch: [3][900/1405]\tBatch Time 0.207 (0.208)\tData Load Time 0.005 (0.005)\tCE Loss 12.2357 (12.1765)\tVB Loss 5.1223 (4.7867)\tF1 0.499 (0.496)\n",
            "Epoch: [3][1000/1405]\tBatch Time 0.195 (0.207)\tData Load Time 0.004 (0.005)\tCE Loss 12.2967 (12.1686)\tVB Loss 6.4645 (4.7621)\tF1 0.378 (0.497)\n",
            "Epoch: [3][1100/1405]\tBatch Time 0.236 (0.207)\tData Load Time 0.004 (0.005)\tCE Loss 12.4293 (12.1718)\tVB Loss 4.9721 (4.7571)\tF1 0.284 (0.497)\n",
            "Epoch: [3][1200/1405]\tBatch Time 0.193 (0.207)\tData Load Time 0.004 (0.005)\tCE Loss 11.2191 (12.1733)\tVB Loss 5.9521 (4.7501)\tF1 0.405 (0.499)\n",
            "Epoch: [3][1300/1405]\tBatch Time 0.120 (0.206)\tData Load Time 0.004 (0.005)\tCE Loss 11.4366 (12.1745)\tVB Loss 2.1269 (4.7222)\tF1 0.382 (0.498)\n",
            "Epoch: [3][1400/1405]\tBatch Time 0.205 (0.206)\tData Load Time 0.004 (0.005)\tCE Loss 13.4745 (12.1748)\tVB Loss 4.2516 (4.7004)\tF1 0.463 (0.501)\n",
            "Validation: [0/325]\tBatch Time 0.321 (0.321)\tVB Loss 3.5434 (3.5434)\tF1 Score 0.513 (0.513)\t\n",
            "Validation: [100/325]\tBatch Time 0.099 (0.100)\tVB Loss 2.4927 (4.2033)\tF1 Score 0.462 (0.540)\t\n",
            "Validation: [200/325]\tBatch Time 0.123 (0.109)\tVB Loss 5.1747 (4.4631)\tF1 Score 0.466 (0.532)\t\n",
            "Validation: [300/325]\tBatch Time 0.046 (0.109)\tVB Loss 1.4987 (4.3480)\tF1 Score 0.736 (0.552)\t\n",
            "\n",
            " * LOSS - 4.348, F1 SCORE - 0.554\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.001250\n",
            "\n",
            "Epoch: [4][0/1405]\tBatch Time 0.430 (0.430)\tData Load Time 0.226 (0.226)\tCE Loss 12.2420 (12.2420)\tVB Loss 3.6930 (3.6930)\tF1 0.486 (0.486)\n",
            "Epoch: [4][100/1405]\tBatch Time 0.144 (0.236)\tData Load Time 0.005 (0.007)\tCE Loss 12.5002 (12.1728)\tVB Loss 2.7843 (4.7478)\tF1 0.503 (0.514)\n",
            "Epoch: [4][200/1405]\tBatch Time 0.279 (0.221)\tData Load Time 0.004 (0.006)\tCE Loss 12.2595 (12.1363)\tVB Loss 3.5974 (4.5414)\tF1 0.552 (0.518)\n",
            "Epoch: [4][300/1405]\tBatch Time 0.194 (0.216)\tData Load Time 0.004 (0.005)\tCE Loss 12.0412 (12.1357)\tVB Loss 5.9987 (4.5703)\tF1 0.309 (0.522)\n",
            "Epoch: [4][400/1405]\tBatch Time 0.229 (0.212)\tData Load Time 0.005 (0.005)\tCE Loss 12.1722 (12.1407)\tVB Loss 6.1106 (4.5430)\tF1 0.384 (0.520)\n",
            "Epoch: [4][500/1405]\tBatch Time 0.174 (0.211)\tData Load Time 0.004 (0.005)\tCE Loss 11.4249 (12.1234)\tVB Loss 3.7856 (4.5405)\tF1 0.661 (0.523)\n",
            "Epoch: [4][600/1405]\tBatch Time 0.165 (0.209)\tData Load Time 0.005 (0.005)\tCE Loss 11.6346 (12.1216)\tVB Loss 3.1376 (4.5383)\tF1 0.532 (0.523)\n",
            "Epoch: [4][700/1405]\tBatch Time 0.253 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 12.6938 (12.1233)\tVB Loss 4.6322 (4.5292)\tF1 0.644 (0.525)\n",
            "Epoch: [4][800/1405]\tBatch Time 0.185 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 12.2335 (12.1152)\tVB Loss 3.5903 (4.4879)\tF1 0.462 (0.523)\n",
            "Epoch: [4][900/1405]\tBatch Time 0.192 (0.206)\tData Load Time 0.004 (0.005)\tCE Loss 11.6657 (12.1101)\tVB Loss 4.5685 (4.4622)\tF1 0.553 (0.524)\n",
            "Epoch: [4][1000/1405]\tBatch Time 0.298 (0.206)\tData Load Time 0.006 (0.005)\tCE Loss 11.9971 (12.1050)\tVB Loss 4.7154 (4.4309)\tF1 0.751 (0.525)\n",
            "Epoch: [4][1100/1405]\tBatch Time 0.174 (0.206)\tData Load Time 0.004 (0.005)\tCE Loss 11.4336 (12.0985)\tVB Loss 5.8336 (4.4352)\tF1 0.482 (0.525)\n",
            "Epoch: [4][1200/1405]\tBatch Time 0.216 (0.206)\tData Load Time 0.004 (0.005)\tCE Loss 11.4888 (12.1013)\tVB Loss 3.8862 (4.4359)\tF1 0.707 (0.526)\n",
            "Epoch: [4][1300/1405]\tBatch Time 0.205 (0.206)\tData Load Time 0.004 (0.005)\tCE Loss 12.1559 (12.0945)\tVB Loss 4.7776 (4.4189)\tF1 0.548 (0.526)\n",
            "Epoch: [4][1400/1405]\tBatch Time 0.247 (0.206)\tData Load Time 0.004 (0.005)\tCE Loss 11.9644 (12.0844)\tVB Loss 6.0622 (4.4221)\tF1 0.514 (0.528)\n",
            "Validation: [0/325]\tBatch Time 0.317 (0.317)\tVB Loss 4.3746 (4.3746)\tF1 Score 0.521 (0.521)\t\n",
            "Validation: [100/325]\tBatch Time 0.141 (0.117)\tVB Loss 5.2173 (3.7106)\tF1 Score 0.583 (0.623)\t\n",
            "Validation: [200/325]\tBatch Time 0.113 (0.117)\tVB Loss 3.7892 (3.9358)\tF1 Score 0.759 (0.602)\t\n",
            "Validation: [300/325]\tBatch Time 0.144 (0.116)\tVB Loss 7.0686 (3.8763)\tF1 Score 0.209 (0.604)\t\n",
            "\n",
            " * LOSS - 3.868, F1 SCORE - 0.601\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.001200\n",
            "\n",
            "Epoch: [5][0/1405]\tBatch Time 0.562 (0.562)\tData Load Time 0.261 (0.261)\tCE Loss 11.6767 (11.6767)\tVB Loss 6.5911 (6.5911)\tF1 0.534 (0.534)\n",
            "Epoch: [5][100/1405]\tBatch Time 0.233 (0.233)\tData Load Time 0.004 (0.007)\tCE Loss 12.3179 (11.9898)\tVB Loss 7.0381 (4.2495)\tF1 0.596 (0.547)\n",
            "Epoch: [5][200/1405]\tBatch Time 0.140 (0.222)\tData Load Time 0.005 (0.006)\tCE Loss 12.3521 (12.0408)\tVB Loss 2.6696 (4.2433)\tF1 0.674 (0.543)\n",
            "Epoch: [5][300/1405]\tBatch Time 0.229 (0.217)\tData Load Time 0.006 (0.005)\tCE Loss 11.1883 (12.0191)\tVB Loss 4.0453 (4.2665)\tF1 0.576 (0.547)\n",
            "Epoch: [5][400/1405]\tBatch Time 0.193 (0.215)\tData Load Time 0.004 (0.005)\tCE Loss 11.8471 (12.0271)\tVB Loss 1.6083 (4.2135)\tF1 0.563 (0.546)\n",
            "Epoch: [5][500/1405]\tBatch Time 0.186 (0.212)\tData Load Time 0.004 (0.005)\tCE Loss 11.2989 (12.0234)\tVB Loss 5.3507 (4.2063)\tF1 0.592 (0.545)\n",
            "Epoch: [5][600/1405]\tBatch Time 0.251 (0.212)\tData Load Time 0.005 (0.005)\tCE Loss 11.8115 (12.0260)\tVB Loss 4.3656 (4.1968)\tF1 0.615 (0.544)\n",
            "Epoch: [5][700/1405]\tBatch Time 0.243 (0.212)\tData Load Time 0.004 (0.005)\tCE Loss 11.9381 (12.0174)\tVB Loss 9.7604 (4.2329)\tF1 0.427 (0.544)\n",
            "Epoch: [5][800/1405]\tBatch Time 0.187 (0.213)\tData Load Time 0.005 (0.005)\tCE Loss 11.3787 (12.0129)\tVB Loss 2.4028 (4.2293)\tF1 0.796 (0.545)\n",
            "Epoch: [5][900/1405]\tBatch Time 0.223 (0.212)\tData Load Time 0.004 (0.005)\tCE Loss 11.8988 (12.0093)\tVB Loss 4.7078 (4.2191)\tF1 0.596 (0.549)\n",
            "Epoch: [5][1000/1405]\tBatch Time 0.167 (0.212)\tData Load Time 0.005 (0.005)\tCE Loss 11.1994 (12.0105)\tVB Loss 4.1067 (4.1981)\tF1 0.740 (0.551)\n",
            "Epoch: [5][1100/1405]\tBatch Time 0.135 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 12.0330 (12.0086)\tVB Loss 1.5780 (4.1682)\tF1 0.635 (0.552)\n",
            "Epoch: [5][1200/1405]\tBatch Time 0.251 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 12.5643 (12.0075)\tVB Loss 5.0804 (4.1810)\tF1 0.602 (0.553)\n",
            "Epoch: [5][1300/1405]\tBatch Time 0.262 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 11.7098 (12.0140)\tVB Loss 7.4364 (4.1803)\tF1 0.519 (0.552)\n",
            "Epoch: [5][1400/1405]\tBatch Time 0.212 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 11.7370 (12.0099)\tVB Loss 4.8941 (4.1833)\tF1 0.577 (0.553)\n",
            "Validation: [0/325]\tBatch Time 0.312 (0.312)\tVB Loss 3.2704 (3.2704)\tF1 Score 0.467 (0.467)\t\n",
            "Validation: [100/325]\tBatch Time 0.133 (0.107)\tVB Loss 5.8263 (3.8391)\tF1 Score 0.419 (0.622)\t\n",
            "Validation: [200/325]\tBatch Time 0.124 (0.112)\tVB Loss 5.6523 (3.8467)\tF1 Score 0.489 (0.617)\t\n",
            "Validation: [300/325]\tBatch Time 0.105 (0.112)\tVB Loss 2.8063 (3.7109)\tF1 Score 0.712 (0.622)\t\n",
            "\n",
            " * LOSS - 3.731, F1 SCORE - 0.622\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.001154\n",
            "\n",
            "Epoch: [6][0/1405]\tBatch Time 0.551 (0.551)\tData Load Time 0.231 (0.231)\tCE Loss 12.5423 (12.5423)\tVB Loss 5.2774 (5.2774)\tF1 0.268 (0.268)\n",
            "Epoch: [6][100/1405]\tBatch Time 0.254 (0.231)\tData Load Time 0.004 (0.008)\tCE Loss 11.8853 (11.8727)\tVB Loss 5.2682 (3.9464)\tF1 0.459 (0.564)\n",
            "Epoch: [6][200/1405]\tBatch Time 0.124 (0.223)\tData Load Time 0.004 (0.006)\tCE Loss 12.0860 (11.9406)\tVB Loss 2.6784 (4.0903)\tF1 0.770 (0.558)\n",
            "Epoch: [6][300/1405]\tBatch Time 0.194 (0.217)\tData Load Time 0.004 (0.006)\tCE Loss 12.0917 (11.9643)\tVB Loss 2.8938 (4.0910)\tF1 0.646 (0.560)\n",
            "Epoch: [6][400/1405]\tBatch Time 0.203 (0.215)\tData Load Time 0.005 (0.005)\tCE Loss 11.8597 (11.9522)\tVB Loss 6.2628 (4.0565)\tF1 0.579 (0.563)\n",
            "Epoch: [6][500/1405]\tBatch Time 0.226 (0.212)\tData Load Time 0.004 (0.005)\tCE Loss 12.0770 (11.9556)\tVB Loss 5.3572 (4.0264)\tF1 0.548 (0.562)\n",
            "Epoch: [6][600/1405]\tBatch Time 0.202 (0.211)\tData Load Time 0.004 (0.005)\tCE Loss 12.2671 (11.9609)\tVB Loss 3.6369 (4.0700)\tF1 0.460 (0.560)\n",
            "Epoch: [6][700/1405]\tBatch Time 0.264 (0.211)\tData Load Time 0.004 (0.005)\tCE Loss 11.5636 (11.9505)\tVB Loss 5.7917 (4.0556)\tF1 0.382 (0.560)\n",
            "Epoch: [6][800/1405]\tBatch Time 0.199 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 11.9731 (11.9459)\tVB Loss 5.2295 (4.0457)\tF1 0.557 (0.562)\n",
            "Epoch: [6][900/1405]\tBatch Time 0.198 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 11.5735 (11.9505)\tVB Loss 6.6545 (4.0463)\tF1 0.371 (0.561)\n",
            "Epoch: [6][1000/1405]\tBatch Time 0.256 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 12.4318 (11.9568)\tVB Loss 6.2314 (4.0296)\tF1 0.515 (0.562)\n",
            "Epoch: [6][1100/1405]\tBatch Time 0.262 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 11.2874 (11.9439)\tVB Loss 5.2853 (4.0102)\tF1 0.770 (0.563)\n",
            "Epoch: [6][1200/1405]\tBatch Time 0.145 (0.209)\tData Load Time 0.005 (0.005)\tCE Loss 12.2776 (11.9446)\tVB Loss 1.9551 (4.0000)\tF1 0.509 (0.565)\n",
            "Epoch: [6][1300/1405]\tBatch Time 0.157 (0.209)\tData Load Time 0.005 (0.005)\tCE Loss 12.0026 (11.9535)\tVB Loss 3.3731 (4.0119)\tF1 0.594 (0.565)\n",
            "Epoch: [6][1400/1405]\tBatch Time 0.222 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 11.4102 (11.9540)\tVB Loss 5.3212 (4.0111)\tF1 0.584 (0.567)\n",
            "Validation: [0/325]\tBatch Time 0.337 (0.337)\tVB Loss 2.1568 (2.1568)\tF1 Score 0.729 (0.729)\t\n",
            "Validation: [100/325]\tBatch Time 0.126 (0.102)\tVB Loss 3.0049 (3.7132)\tF1 Score 0.729 (0.629)\t\n",
            "Validation: [200/325]\tBatch Time 0.148 (0.105)\tVB Loss 12.1576 (3.7313)\tF1 Score 0.581 (0.624)\t\n",
            "Validation: [300/325]\tBatch Time 0.107 (0.108)\tVB Loss 3.8384 (3.7158)\tF1 Score 0.535 (0.624)\t\n",
            "\n",
            " * LOSS - 3.667, F1 SCORE - 0.626\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.001111\n",
            "\n",
            "Epoch: [7][0/1405]\tBatch Time 0.494 (0.494)\tData Load Time 0.242 (0.242)\tCE Loss 11.9036 (11.9036)\tVB Loss 3.6566 (3.6566)\tF1 0.458 (0.458)\n",
            "Epoch: [7][100/1405]\tBatch Time 0.162 (0.237)\tData Load Time 0.004 (0.007)\tCE Loss 11.6360 (11.8804)\tVB Loss 1.8696 (3.9440)\tF1 0.597 (0.574)\n",
            "Epoch: [7][200/1405]\tBatch Time 0.211 (0.226)\tData Load Time 0.004 (0.006)\tCE Loss 12.1842 (11.9277)\tVB Loss 2.1030 (3.9729)\tF1 0.655 (0.575)\n",
            "Epoch: [7][300/1405]\tBatch Time 0.203 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 11.3998 (11.9129)\tVB Loss 5.6561 (3.9897)\tF1 0.366 (0.575)\n",
            "Epoch: [7][400/1405]\tBatch Time 0.169 (0.217)\tData Load Time 0.004 (0.005)\tCE Loss 11.5671 (11.9158)\tVB Loss 1.7970 (3.9425)\tF1 0.592 (0.577)\n",
            "Epoch: [7][500/1405]\tBatch Time 0.221 (0.214)\tData Load Time 0.005 (0.005)\tCE Loss 11.6180 (11.9293)\tVB Loss 4.2380 (3.9903)\tF1 0.615 (0.577)\n",
            "Epoch: [7][600/1405]\tBatch Time 0.186 (0.213)\tData Load Time 0.005 (0.005)\tCE Loss 11.7319 (11.9222)\tVB Loss 2.5415 (3.9648)\tF1 0.602 (0.576)\n",
            "Epoch: [7][700/1405]\tBatch Time 0.134 (0.212)\tData Load Time 0.004 (0.005)\tCE Loss 10.1581 (11.9250)\tVB Loss 2.5339 (3.9179)\tF1 0.567 (0.577)\n",
            "Epoch: [7][800/1405]\tBatch Time 0.161 (0.211)\tData Load Time 0.004 (0.005)\tCE Loss 12.7737 (11.9149)\tVB Loss 3.4048 (3.9214)\tF1 0.505 (0.577)\n",
            "Epoch: [7][900/1405]\tBatch Time 0.202 (0.211)\tData Load Time 0.004 (0.005)\tCE Loss 12.2533 (11.9136)\tVB Loss 3.1472 (3.8934)\tF1 0.535 (0.578)\n",
            "Epoch: [7][1000/1405]\tBatch Time 0.167 (0.211)\tData Load Time 0.004 (0.005)\tCE Loss 10.4192 (11.9056)\tVB Loss 3.1717 (3.8695)\tF1 0.758 (0.579)\n",
            "Epoch: [7][1100/1405]\tBatch Time 0.211 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 11.3639 (11.9107)\tVB Loss 6.5539 (3.8697)\tF1 0.432 (0.578)\n",
            "Epoch: [7][1200/1405]\tBatch Time 0.218 (0.210)\tData Load Time 0.005 (0.005)\tCE Loss 12.1850 (11.9054)\tVB Loss 5.5888 (3.8697)\tF1 0.535 (0.578)\n",
            "Epoch: [7][1300/1405]\tBatch Time 0.255 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 12.1394 (11.9107)\tVB Loss 9.1738 (3.8804)\tF1 0.399 (0.579)\n",
            "Epoch: [7][1400/1405]\tBatch Time 0.220 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 12.1888 (11.9109)\tVB Loss 5.6712 (3.8708)\tF1 0.669 (0.580)\n",
            "Validation: [0/325]\tBatch Time 0.306 (0.306)\tVB Loss 5.5195 (5.5195)\tF1 Score 0.593 (0.593)\t\n",
            "Validation: [100/325]\tBatch Time 0.116 (0.113)\tVB Loss 2.5913 (3.2476)\tF1 Score 0.631 (0.669)\t\n",
            "Validation: [200/325]\tBatch Time 0.121 (0.116)\tVB Loss 9.1368 (3.4178)\tF1 Score 0.589 (0.665)\t\n",
            "Validation: [300/325]\tBatch Time 0.141 (0.115)\tVB Loss 2.5484 (3.4042)\tF1 Score 0.681 (0.664)\t\n",
            "\n",
            " * LOSS - 3.396, F1 SCORE - 0.663\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.001071\n",
            "\n",
            "Epoch: [8][0/1405]\tBatch Time 0.531 (0.531)\tData Load Time 0.243 (0.243)\tCE Loss 12.3449 (12.3449)\tVB Loss 4.2459 (4.2459)\tF1 0.498 (0.498)\n",
            "Epoch: [8][100/1405]\tBatch Time 0.206 (0.233)\tData Load Time 0.005 (0.007)\tCE Loss 12.3747 (11.8980)\tVB Loss 3.2552 (3.9655)\tF1 0.366 (0.583)\n",
            "Epoch: [8][200/1405]\tBatch Time 0.217 (0.220)\tData Load Time 0.004 (0.006)\tCE Loss 11.6402 (11.9182)\tVB Loss 3.4515 (3.8951)\tF1 0.509 (0.587)\n",
            "Epoch: [8][300/1405]\tBatch Time 0.213 (0.213)\tData Load Time 0.004 (0.005)\tCE Loss 11.8472 (11.8822)\tVB Loss 3.0196 (3.8790)\tF1 0.567 (0.591)\n",
            "Epoch: [8][400/1405]\tBatch Time 0.155 (0.210)\tData Load Time 0.009 (0.005)\tCE Loss 11.3816 (11.9048)\tVB Loss 1.7947 (3.8220)\tF1 0.765 (0.594)\n",
            "Epoch: [8][500/1405]\tBatch Time 0.211 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 12.0878 (11.9040)\tVB Loss 3.7573 (3.8379)\tF1 0.592 (0.594)\n",
            "Epoch: [8][600/1405]\tBatch Time 0.245 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 11.5422 (11.8880)\tVB Loss 4.2895 (3.8393)\tF1 0.546 (0.589)\n",
            "Epoch: [8][700/1405]\tBatch Time 0.185 (0.207)\tData Load Time 0.004 (0.005)\tCE Loss 11.3950 (11.8959)\tVB Loss 5.3202 (3.7825)\tF1 0.670 (0.591)\n",
            "Epoch: [8][800/1405]\tBatch Time 0.190 (0.206)\tData Load Time 0.005 (0.005)\tCE Loss 11.3114 (11.8939)\tVB Loss 2.8828 (3.7591)\tF1 0.621 (0.592)\n",
            "Epoch: [8][900/1405]\tBatch Time 0.204 (0.206)\tData Load Time 0.005 (0.005)\tCE Loss 11.2698 (11.8885)\tVB Loss 3.6631 (3.7462)\tF1 0.608 (0.594)\n",
            "Epoch: [8][1000/1405]\tBatch Time 0.224 (0.206)\tData Load Time 0.004 (0.005)\tCE Loss 12.7401 (11.8851)\tVB Loss 3.0896 (3.7387)\tF1 0.319 (0.595)\n",
            "Epoch: [8][1100/1405]\tBatch Time 0.236 (0.206)\tData Load Time 0.004 (0.005)\tCE Loss 11.5621 (11.8797)\tVB Loss 6.7949 (3.7431)\tF1 0.602 (0.596)\n",
            "Epoch: [8][1200/1405]\tBatch Time 0.211 (0.206)\tData Load Time 0.005 (0.005)\tCE Loss 11.7443 (11.8742)\tVB Loss 3.7058 (3.7583)\tF1 0.617 (0.597)\n",
            "Epoch: [8][1300/1405]\tBatch Time 0.240 (0.206)\tData Load Time 0.004 (0.005)\tCE Loss 11.5780 (11.8691)\tVB Loss 3.2051 (3.7440)\tF1 0.700 (0.597)\n",
            "Epoch: [8][1400/1405]\tBatch Time 0.218 (0.206)\tData Load Time 0.005 (0.005)\tCE Loss 12.0677 (11.8721)\tVB Loss 2.9234 (3.7239)\tF1 0.543 (0.596)\n",
            "Validation: [0/325]\tBatch Time 0.336 (0.336)\tVB Loss 3.5334 (3.5334)\tF1 Score 0.764 (0.764)\t\n",
            "Validation: [100/325]\tBatch Time 0.116 (0.100)\tVB Loss 3.5124 (3.1602)\tF1 Score 0.826 (0.699)\t\n",
            "Validation: [200/325]\tBatch Time 0.092 (0.101)\tVB Loss 3.9995 (3.2708)\tF1 Score 0.480 (0.688)\t\n",
            "Validation: [300/325]\tBatch Time 0.114 (0.103)\tVB Loss 4.7291 (3.2682)\tF1 Score 0.688 (0.683)\t\n",
            "\n",
            " * LOSS - 3.264, F1 SCORE - 0.684\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.001034\n",
            "\n",
            "Epoch: [9][0/1405]\tBatch Time 0.497 (0.497)\tData Load Time 0.221 (0.221)\tCE Loss 12.1101 (12.1101)\tVB Loss 3.3716 (3.3716)\tF1 0.480 (0.480)\n",
            "Epoch: [9][100/1405]\tBatch Time 0.224 (0.231)\tData Load Time 0.004 (0.007)\tCE Loss 11.6929 (11.8012)\tVB Loss 3.5164 (3.6995)\tF1 0.589 (0.590)\n",
            "Epoch: [9][200/1405]\tBatch Time 0.154 (0.220)\tData Load Time 0.004 (0.006)\tCE Loss 11.4635 (11.8466)\tVB Loss 2.1826 (3.7417)\tF1 0.617 (0.599)\n",
            "Epoch: [9][300/1405]\tBatch Time 0.259 (0.218)\tData Load Time 0.004 (0.005)\tCE Loss 12.1941 (11.8630)\tVB Loss 6.2672 (3.7490)\tF1 0.599 (0.600)\n",
            "Epoch: [9][400/1405]\tBatch Time 0.168 (0.218)\tData Load Time 0.004 (0.005)\tCE Loss 11.7484 (11.8704)\tVB Loss 2.4357 (3.6978)\tF1 0.644 (0.605)\n",
            "Epoch: [9][500/1405]\tBatch Time 0.166 (0.215)\tData Load Time 0.004 (0.005)\tCE Loss 12.2163 (11.8609)\tVB Loss 2.2857 (3.6444)\tF1 0.819 (0.604)\n",
            "Epoch: [9][600/1405]\tBatch Time 0.262 (0.214)\tData Load Time 0.004 (0.005)\tCE Loss 11.9362 (11.8482)\tVB Loss 6.3197 (3.6300)\tF1 0.496 (0.603)\n",
            "Epoch: [9][700/1405]\tBatch Time 0.213 (0.212)\tData Load Time 0.004 (0.005)\tCE Loss 12.6938 (11.8407)\tVB Loss 4.3297 (3.6159)\tF1 0.655 (0.604)\n",
            "Epoch: [9][800/1405]\tBatch Time 0.167 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 11.9713 (11.8457)\tVB Loss 2.7550 (3.6202)\tF1 0.533 (0.604)\n",
            "Epoch: [9][900/1405]\tBatch Time 0.224 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 11.8647 (11.8413)\tVB Loss 4.5641 (3.6193)\tF1 0.535 (0.605)\n",
            "Epoch: [9][1000/1405]\tBatch Time 0.241 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 12.1092 (11.8511)\tVB Loss 4.2617 (3.6180)\tF1 0.445 (0.605)\n",
            "Epoch: [9][1100/1405]\tBatch Time 0.253 (0.210)\tData Load Time 0.005 (0.005)\tCE Loss 12.7185 (11.8480)\tVB Loss 6.5142 (3.6208)\tF1 0.423 (0.606)\n",
            "Epoch: [9][1200/1405]\tBatch Time 0.215 (0.209)\tData Load Time 0.005 (0.005)\tCE Loss 11.9939 (11.8454)\tVB Loss 1.9735 (3.6364)\tF1 0.484 (0.605)\n",
            "Epoch: [9][1300/1405]\tBatch Time 0.248 (0.209)\tData Load Time 0.005 (0.005)\tCE Loss 12.0362 (11.8438)\tVB Loss 2.8555 (3.6329)\tF1 0.605 (0.605)\n",
            "Epoch: [9][1400/1405]\tBatch Time 0.175 (0.208)\tData Load Time 0.007 (0.005)\tCE Loss 11.9951 (11.8371)\tVB Loss 2.0399 (3.6179)\tF1 0.673 (0.607)\n",
            "Validation: [0/325]\tBatch Time 0.319 (0.319)\tVB Loss 2.4202 (2.4202)\tF1 Score 0.758 (0.758)\t\n",
            "Validation: [100/325]\tBatch Time 0.087 (0.103)\tVB Loss 3.5442 (3.0708)\tF1 Score 0.713 (0.689)\t\n",
            "Validation: [200/325]\tBatch Time 0.141 (0.108)\tVB Loss 7.2928 (3.1828)\tF1 Score 0.685 (0.688)\t\n",
            "Validation: [300/325]\tBatch Time 0.114 (0.110)\tVB Loss 3.2725 (3.1689)\tF1 Score 0.675 (0.686)\t\n",
            "\n",
            " * LOSS - 3.161, F1 SCORE - 0.684\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.001000\n",
            "\n",
            "Epoch: [10][0/1405]\tBatch Time 0.545 (0.545)\tData Load Time 0.216 (0.216)\tCE Loss 12.2819 (12.2819)\tVB Loss 3.3232 (3.3232)\tF1 0.694 (0.694)\n",
            "Epoch: [10][100/1405]\tBatch Time 0.228 (0.234)\tData Load Time 0.004 (0.007)\tCE Loss 12.3572 (11.8874)\tVB Loss 4.5754 (3.6914)\tF1 0.496 (0.605)\n",
            "Epoch: [10][200/1405]\tBatch Time 0.217 (0.226)\tData Load Time 0.004 (0.006)\tCE Loss 12.1915 (11.8357)\tVB Loss 3.4952 (3.6532)\tF1 0.607 (0.613)\n",
            "Epoch: [10][300/1405]\tBatch Time 0.188 (0.220)\tData Load Time 0.006 (0.006)\tCE Loss 11.6984 (11.8267)\tVB Loss 2.6711 (3.6328)\tF1 0.755 (0.615)\n",
            "Epoch: [10][400/1405]\tBatch Time 0.200 (0.216)\tData Load Time 0.004 (0.005)\tCE Loss 12.7281 (11.8361)\tVB Loss 5.0948 (3.5832)\tF1 0.652 (0.610)\n",
            "Epoch: [10][500/1405]\tBatch Time 0.180 (0.214)\tData Load Time 0.004 (0.005)\tCE Loss 11.9465 (11.8381)\tVB Loss 3.9073 (3.5989)\tF1 0.588 (0.611)\n",
            "Epoch: [10][600/1405]\tBatch Time 0.229 (0.211)\tData Load Time 0.005 (0.005)\tCE Loss 11.4160 (11.8235)\tVB Loss 3.1763 (3.5888)\tF1 0.736 (0.614)\n",
            "Epoch: [10][700/1405]\tBatch Time 0.211 (0.210)\tData Load Time 0.005 (0.005)\tCE Loss 11.8285 (11.8247)\tVB Loss 3.6676 (3.5710)\tF1 0.552 (0.613)\n",
            "Epoch: [10][800/1405]\tBatch Time 0.230 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 12.4739 (11.8238)\tVB Loss 5.1118 (3.5689)\tF1 0.552 (0.614)\n",
            "Epoch: [10][900/1405]\tBatch Time 0.181 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 11.4646 (11.8187)\tVB Loss 4.0239 (3.5733)\tF1 0.449 (0.613)\n",
            "Epoch: [10][1000/1405]\tBatch Time 0.131 (0.207)\tData Load Time 0.004 (0.005)\tCE Loss 12.3064 (11.8151)\tVB Loss 2.7051 (3.5607)\tF1 0.533 (0.614)\n",
            "Epoch: [10][1100/1405]\tBatch Time 0.139 (0.207)\tData Load Time 0.004 (0.005)\tCE Loss 12.0578 (11.8152)\tVB Loss 1.9581 (3.5380)\tF1 0.655 (0.614)\n",
            "Epoch: [10][1200/1405]\tBatch Time 0.185 (0.207)\tData Load Time 0.004 (0.005)\tCE Loss 11.7252 (11.8169)\tVB Loss 2.0451 (3.5336)\tF1 0.711 (0.614)\n",
            "Epoch: [10][1300/1405]\tBatch Time 0.207 (0.206)\tData Load Time 0.004 (0.005)\tCE Loss 12.5956 (11.8168)\tVB Loss 3.5603 (3.5379)\tF1 0.788 (0.614)\n",
            "Epoch: [10][1400/1405]\tBatch Time 0.241 (0.206)\tData Load Time 0.004 (0.005)\tCE Loss 12.1542 (11.8125)\tVB Loss 5.4862 (3.5217)\tF1 0.587 (0.615)\n",
            "Validation: [0/325]\tBatch Time 0.338 (0.338)\tVB Loss 1.8090 (1.8090)\tF1 Score 0.721 (0.721)\t\n",
            "Validation: [100/325]\tBatch Time 0.097 (0.099)\tVB Loss 2.8195 (3.2641)\tF1 Score 0.592 (0.676)\t\n",
            "Validation: [200/325]\tBatch Time 0.074 (0.099)\tVB Loss 1.2816 (3.1966)\tF1 Score 0.908 (0.681)\t\n",
            "Validation: [300/325]\tBatch Time 0.101 (0.104)\tVB Loss 2.7092 (3.1426)\tF1 Score 0.816 (0.688)\t\n",
            "\n",
            " * LOSS - 3.165, F1 SCORE - 0.687\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000968\n",
            "\n",
            "Epoch: [11][0/1405]\tBatch Time 0.499 (0.499)\tData Load Time 0.215 (0.215)\tCE Loss 11.5090 (11.5090)\tVB Loss 2.3679 (2.3679)\tF1 0.555 (0.555)\n",
            "Epoch: [11][100/1405]\tBatch Time 0.238 (0.236)\tData Load Time 0.004 (0.008)\tCE Loss 11.2495 (11.8382)\tVB Loss 4.6989 (3.5253)\tF1 0.598 (0.624)\n",
            "Epoch: [11][200/1405]\tBatch Time 0.293 (0.221)\tData Load Time 0.005 (0.006)\tCE Loss 11.6451 (11.7741)\tVB Loss 5.6010 (3.4548)\tF1 0.437 (0.626)\n",
            "Epoch: [11][300/1405]\tBatch Time 0.172 (0.217)\tData Load Time 0.005 (0.006)\tCE Loss 11.3467 (11.7793)\tVB Loss 2.7551 (3.4355)\tF1 0.679 (0.631)\n",
            "Epoch: [11][400/1405]\tBatch Time 0.183 (0.213)\tData Load Time 0.006 (0.005)\tCE Loss 11.6481 (11.7612)\tVB Loss 2.1474 (3.3914)\tF1 0.511 (0.633)\n",
            "Epoch: [11][500/1405]\tBatch Time 0.213 (0.212)\tData Load Time 0.005 (0.005)\tCE Loss 11.9225 (11.7727)\tVB Loss 3.6858 (3.4336)\tF1 0.696 (0.630)\n",
            "Epoch: [11][600/1405]\tBatch Time 0.226 (0.211)\tData Load Time 0.004 (0.005)\tCE Loss 12.8095 (11.7727)\tVB Loss 4.1922 (3.4338)\tF1 0.638 (0.627)\n",
            "Epoch: [11][700/1405]\tBatch Time 0.192 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 10.9396 (11.7716)\tVB Loss 1.8324 (3.4351)\tF1 0.779 (0.626)\n",
            "Epoch: [11][800/1405]\tBatch Time 0.172 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 11.7974 (11.7719)\tVB Loss 3.4755 (3.4226)\tF1 0.414 (0.625)\n",
            "Epoch: [11][900/1405]\tBatch Time 0.190 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 12.0101 (11.7669)\tVB Loss 3.2807 (3.4112)\tF1 0.661 (0.625)\n",
            "Epoch: [11][1000/1405]\tBatch Time 0.212 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 11.6528 (11.7729)\tVB Loss 4.1477 (3.4390)\tF1 0.552 (0.625)\n",
            "Epoch: [11][1100/1405]\tBatch Time 0.227 (0.207)\tData Load Time 0.004 (0.005)\tCE Loss 12.0888 (11.7717)\tVB Loss 2.2594 (3.4141)\tF1 0.630 (0.627)\n",
            "Epoch: [11][1200/1405]\tBatch Time 0.226 (0.208)\tData Load Time 0.005 (0.005)\tCE Loss 10.8809 (11.7795)\tVB Loss 1.9371 (3.4251)\tF1 0.594 (0.627)\n",
            "Epoch: [11][1300/1405]\tBatch Time 0.204 (0.207)\tData Load Time 0.005 (0.005)\tCE Loss 12.2117 (11.7806)\tVB Loss 4.0000 (3.4250)\tF1 0.623 (0.625)\n",
            "Epoch: [11][1400/1405]\tBatch Time 0.148 (0.208)\tData Load Time 0.005 (0.005)\tCE Loss 12.0845 (11.7881)\tVB Loss 3.1199 (3.4311)\tF1 0.607 (0.624)\n",
            "Validation: [0/325]\tBatch Time 0.344 (0.344)\tVB Loss 5.7306 (5.7306)\tF1 Score 0.530 (0.530)\t\n",
            "Validation: [100/325]\tBatch Time 0.083 (0.103)\tVB Loss 2.5109 (3.1462)\tF1 Score 0.679 (0.695)\t\n",
            "Validation: [200/325]\tBatch Time 0.115 (0.103)\tVB Loss 4.7911 (3.0536)\tF1 Score 0.622 (0.703)\t\n",
            "Validation: [300/325]\tBatch Time 0.116 (0.102)\tVB Loss 2.1069 (2.9361)\tF1 Score 0.869 (0.703)\t\n",
            "\n",
            " * LOSS - 2.918, F1 SCORE - 0.705\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000937\n",
            "\n",
            "Epoch: [12][0/1405]\tBatch Time 0.502 (0.502)\tData Load Time 0.268 (0.268)\tCE Loss 11.7872 (11.7872)\tVB Loss 2.7959 (2.7959)\tF1 0.286 (0.286)\n",
            "Epoch: [12][100/1405]\tBatch Time 0.169 (0.238)\tData Load Time 0.005 (0.008)\tCE Loss 11.6670 (11.7642)\tVB Loss 2.7342 (3.4531)\tF1 0.579 (0.596)\n",
            "Epoch: [12][200/1405]\tBatch Time 0.181 (0.222)\tData Load Time 0.005 (0.006)\tCE Loss 12.0727 (11.7948)\tVB Loss 1.7343 (3.4131)\tF1 0.901 (0.617)\n",
            "Epoch: [12][300/1405]\tBatch Time 0.237 (0.216)\tData Load Time 0.005 (0.006)\tCE Loss 11.9438 (11.7618)\tVB Loss 4.4386 (3.3285)\tF1 0.758 (0.625)\n",
            "Epoch: [12][400/1405]\tBatch Time 0.224 (0.214)\tData Load Time 0.005 (0.005)\tCE Loss 11.3217 (11.7692)\tVB Loss 1.8163 (3.2842)\tF1 0.757 (0.626)\n",
            "Epoch: [12][500/1405]\tBatch Time 0.225 (0.212)\tData Load Time 0.004 (0.005)\tCE Loss 12.0083 (11.7593)\tVB Loss 3.2657 (3.3108)\tF1 0.718 (0.627)\n",
            "Epoch: [12][600/1405]\tBatch Time 0.189 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 11.9984 (11.7685)\tVB Loss 2.3761 (3.3332)\tF1 0.803 (0.627)\n",
            "Epoch: [12][700/1405]\tBatch Time 0.252 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 11.7282 (11.7697)\tVB Loss 3.3931 (3.3716)\tF1 0.791 (0.624)\n",
            "Epoch: [12][800/1405]\tBatch Time 0.220 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 12.2007 (11.7706)\tVB Loss 3.9203 (3.3614)\tF1 0.685 (0.625)\n",
            "Epoch: [12][900/1405]\tBatch Time 0.253 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 11.5779 (11.7616)\tVB Loss 6.4333 (3.3611)\tF1 0.393 (0.627)\n",
            "Epoch: [12][1000/1405]\tBatch Time 0.185 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 11.1693 (11.7661)\tVB Loss 1.3530 (3.3728)\tF1 0.858 (0.627)\n",
            "Epoch: [12][1100/1405]\tBatch Time 0.127 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 12.0267 (11.7621)\tVB Loss 2.6446 (3.3770)\tF1 0.649 (0.628)\n",
            "Epoch: [12][1200/1405]\tBatch Time 0.202 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 11.0304 (11.7634)\tVB Loss 4.2199 (3.3814)\tF1 0.303 (0.628)\n",
            "Epoch: [12][1300/1405]\tBatch Time 0.238 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 12.4669 (11.7704)\tVB Loss 2.7579 (3.3735)\tF1 0.734 (0.630)\n",
            "Epoch: [12][1400/1405]\tBatch Time 0.226 (0.208)\tData Load Time 0.005 (0.005)\tCE Loss 12.1422 (11.7687)\tVB Loss 4.3349 (3.3530)\tF1 0.541 (0.630)\n",
            "Validation: [0/325]\tBatch Time 0.300 (0.300)\tVB Loss 3.1762 (3.1762)\tF1 Score 0.666 (0.666)\t\n",
            "Validation: [100/325]\tBatch Time 0.099 (0.111)\tVB Loss 2.3762 (2.8671)\tF1 Score 0.751 (0.716)\t\n",
            "Validation: [200/325]\tBatch Time 0.191 (0.114)\tVB Loss 4.8551 (2.9938)\tF1 Score 0.693 (0.708)\t\n",
            "Validation: [300/325]\tBatch Time 0.109 (0.115)\tVB Loss 2.8711 (2.9354)\tF1 Score 0.489 (0.713)\t\n",
            "\n",
            " * LOSS - 2.910, F1 SCORE - 0.715\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000909\n",
            "\n",
            "Epoch: [13][0/1405]\tBatch Time 0.533 (0.533)\tData Load Time 0.257 (0.257)\tCE Loss 12.2772 (12.2772)\tVB Loss 4.6275 (4.6275)\tF1 0.635 (0.635)\n",
            "Epoch: [13][100/1405]\tBatch Time 0.223 (0.240)\tData Load Time 0.004 (0.008)\tCE Loss 11.5208 (11.7022)\tVB Loss 3.6284 (3.3378)\tF1 0.722 (0.631)\n",
            "Epoch: [13][200/1405]\tBatch Time 0.191 (0.224)\tData Load Time 0.007 (0.006)\tCE Loss 11.2139 (11.7434)\tVB Loss 2.6058 (3.3188)\tF1 0.582 (0.629)\n",
            "Epoch: [13][300/1405]\tBatch Time 0.131 (0.217)\tData Load Time 0.004 (0.006)\tCE Loss 10.0476 (11.7388)\tVB Loss 2.0540 (3.2870)\tF1 0.562 (0.636)\n",
            "Epoch: [13][400/1405]\tBatch Time 0.183 (0.217)\tData Load Time 0.004 (0.006)\tCE Loss 11.3829 (11.7562)\tVB Loss 5.6490 (3.3172)\tF1 0.644 (0.635)\n",
            "Epoch: [13][500/1405]\tBatch Time 0.282 (0.217)\tData Load Time 0.004 (0.005)\tCE Loss 11.3898 (11.7678)\tVB Loss 3.3764 (3.3141)\tF1 0.534 (0.636)\n",
            "Epoch: [13][600/1405]\tBatch Time 0.248 (0.215)\tData Load Time 0.004 (0.005)\tCE Loss 12.3955 (11.7637)\tVB Loss 4.1135 (3.3078)\tF1 0.612 (0.638)\n",
            "Epoch: [13][700/1405]\tBatch Time 0.212 (0.213)\tData Load Time 0.004 (0.005)\tCE Loss 11.2114 (11.7537)\tVB Loss 2.8292 (3.2719)\tF1 0.576 (0.638)\n",
            "Epoch: [13][800/1405]\tBatch Time 0.159 (0.212)\tData Load Time 0.006 (0.005)\tCE Loss 11.6028 (11.7492)\tVB Loss 2.6105 (3.2530)\tF1 0.579 (0.641)\n",
            "Epoch: [13][900/1405]\tBatch Time 0.144 (0.212)\tData Load Time 0.004 (0.005)\tCE Loss 11.4368 (11.7499)\tVB Loss 1.2843 (3.2700)\tF1 0.884 (0.641)\n",
            "Epoch: [13][1000/1405]\tBatch Time 0.219 (0.211)\tData Load Time 0.005 (0.005)\tCE Loss 11.7478 (11.7423)\tVB Loss 5.2665 (3.2687)\tF1 0.565 (0.640)\n",
            "Epoch: [13][1100/1405]\tBatch Time 0.213 (0.211)\tData Load Time 0.004 (0.005)\tCE Loss 11.9191 (11.7438)\tVB Loss 4.4289 (3.2650)\tF1 0.491 (0.641)\n",
            "Epoch: [13][1200/1405]\tBatch Time 0.211 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 12.1012 (11.7430)\tVB Loss 4.0094 (3.2631)\tF1 0.832 (0.642)\n",
            "Epoch: [13][1300/1405]\tBatch Time 0.221 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 11.3066 (11.7454)\tVB Loss 1.8364 (3.2694)\tF1 0.882 (0.641)\n",
            "Epoch: [13][1400/1405]\tBatch Time 0.269 (0.211)\tData Load Time 0.004 (0.005)\tCE Loss 11.9860 (11.7479)\tVB Loss 2.0448 (3.2774)\tF1 0.402 (0.641)\n",
            "Validation: [0/325]\tBatch Time 0.320 (0.320)\tVB Loss 1.5799 (1.5799)\tF1 Score 0.874 (0.874)\t\n",
            "Validation: [100/325]\tBatch Time 0.143 (0.105)\tVB Loss 2.6969 (2.6053)\tF1 Score 0.845 (0.744)\t\n",
            "Validation: [200/325]\tBatch Time 0.066 (0.104)\tVB Loss 0.9721 (2.8305)\tF1 Score 0.838 (0.731)\t\n",
            "Validation: [300/325]\tBatch Time 0.091 (0.103)\tVB Loss 2.5443 (2.8290)\tF1 Score 0.885 (0.731)\t\n",
            "\n",
            " * LOSS - 2.824, F1 SCORE - 0.728\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000882\n",
            "\n",
            "Epoch: [14][0/1405]\tBatch Time 0.465 (0.465)\tData Load Time 0.229 (0.229)\tCE Loss 11.0660 (11.0660)\tVB Loss 1.1823 (1.1823)\tF1 0.741 (0.741)\n",
            "Epoch: [14][100/1405]\tBatch Time 0.208 (0.230)\tData Load Time 0.008 (0.008)\tCE Loss 11.4920 (11.6698)\tVB Loss 3.0174 (3.2123)\tF1 0.650 (0.634)\n",
            "Epoch: [14][200/1405]\tBatch Time 0.171 (0.218)\tData Load Time 0.004 (0.006)\tCE Loss 12.0632 (11.6849)\tVB Loss 1.1680 (3.2184)\tF1 0.908 (0.644)\n",
            "Epoch: [14][300/1405]\tBatch Time 0.195 (0.215)\tData Load Time 0.004 (0.006)\tCE Loss 11.7618 (11.6891)\tVB Loss 2.7204 (3.2417)\tF1 0.803 (0.648)\n",
            "Epoch: [14][400/1405]\tBatch Time 0.116 (0.211)\tData Load Time 0.004 (0.005)\tCE Loss 12.0170 (11.7095)\tVB Loss 1.7430 (3.1782)\tF1 0.711 (0.649)\n",
            "Epoch: [14][500/1405]\tBatch Time 0.174 (0.208)\tData Load Time 0.005 (0.005)\tCE Loss 11.8279 (11.7031)\tVB Loss 1.6643 (3.1835)\tF1 0.631 (0.650)\n",
            "Epoch: [14][600/1405]\tBatch Time 0.232 (0.207)\tData Load Time 0.006 (0.005)\tCE Loss 12.5264 (11.7204)\tVB Loss 3.8329 (3.2090)\tF1 0.692 (0.646)\n",
            "Epoch: [14][700/1405]\tBatch Time 0.251 (0.206)\tData Load Time 0.004 (0.005)\tCE Loss 11.7556 (11.7132)\tVB Loss 4.2725 (3.2185)\tF1 0.554 (0.647)\n",
            "Epoch: [14][800/1405]\tBatch Time 0.202 (0.207)\tData Load Time 0.005 (0.005)\tCE Loss 12.7648 (11.7031)\tVB Loss 3.4288 (3.2051)\tF1 0.729 (0.649)\n",
            "Epoch: [14][900/1405]\tBatch Time 0.257 (0.207)\tData Load Time 0.005 (0.005)\tCE Loss 11.3499 (11.7110)\tVB Loss 3.7401 (3.2258)\tF1 0.736 (0.648)\n",
            "Epoch: [14][1000/1405]\tBatch Time 0.256 (0.207)\tData Load Time 0.004 (0.005)\tCE Loss 11.8145 (11.7160)\tVB Loss 2.3406 (3.2118)\tF1 0.725 (0.648)\n",
            "Epoch: [14][1100/1405]\tBatch Time 0.216 (0.206)\tData Load Time 0.004 (0.005)\tCE Loss 11.9049 (11.7184)\tVB Loss 3.0769 (3.1955)\tF1 0.327 (0.647)\n",
            "Epoch: [14][1200/1405]\tBatch Time 0.198 (0.206)\tData Load Time 0.005 (0.005)\tCE Loss 12.0126 (11.7237)\tVB Loss 2.3951 (3.2065)\tF1 0.744 (0.647)\n",
            "Epoch: [14][1300/1405]\tBatch Time 0.181 (0.206)\tData Load Time 0.004 (0.005)\tCE Loss 10.5755 (11.7259)\tVB Loss 2.7114 (3.2213)\tF1 0.585 (0.648)\n",
            "Epoch: [14][1400/1405]\tBatch Time 0.190 (0.206)\tData Load Time 0.005 (0.005)\tCE Loss 11.2566 (11.7297)\tVB Loss 5.4201 (3.2120)\tF1 0.540 (0.649)\n",
            "Validation: [0/325]\tBatch Time 0.328 (0.328)\tVB Loss 3.1658 (3.1658)\tF1 Score 0.684 (0.684)\t\n",
            "Validation: [100/325]\tBatch Time 0.102 (0.102)\tVB Loss 1.0683 (2.8015)\tF1 Score 0.927 (0.722)\t\n",
            "Validation: [200/325]\tBatch Time 0.067 (0.101)\tVB Loss 0.9314 (2.7570)\tF1 Score 0.929 (0.732)\t\n",
            "Validation: [300/325]\tBatch Time 0.137 (0.105)\tVB Loss 3.7228 (2.7743)\tF1 Score 0.836 (0.735)\t\n",
            "\n",
            " * LOSS - 2.770, F1 SCORE - 0.733\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000857\n",
            "\n",
            "Epoch: [15][0/1405]\tBatch Time 0.470 (0.470)\tData Load Time 0.244 (0.244)\tCE Loss 12.2389 (12.2389)\tVB Loss 3.2493 (3.2493)\tF1 0.521 (0.521)\n",
            "Epoch: [15][100/1405]\tBatch Time 0.202 (0.237)\tData Load Time 0.004 (0.007)\tCE Loss 12.0692 (11.7322)\tVB Loss 3.0154 (3.2194)\tF1 0.680 (0.637)\n",
            "Epoch: [15][200/1405]\tBatch Time 0.228 (0.225)\tData Load Time 0.004 (0.006)\tCE Loss 12.1455 (11.7780)\tVB Loss 3.8959 (3.2506)\tF1 0.745 (0.645)\n",
            "Epoch: [15][300/1405]\tBatch Time 0.227 (0.220)\tData Load Time 0.005 (0.006)\tCE Loss 12.0450 (11.7717)\tVB Loss 5.3658 (3.2113)\tF1 0.639 (0.651)\n",
            "Epoch: [15][400/1405]\tBatch Time 0.167 (0.216)\tData Load Time 0.004 (0.005)\tCE Loss 11.0987 (11.7782)\tVB Loss 2.0870 (3.2450)\tF1 0.653 (0.650)\n",
            "Epoch: [15][500/1405]\tBatch Time 0.201 (0.215)\tData Load Time 0.004 (0.005)\tCE Loss 12.1771 (11.7615)\tVB Loss 2.8674 (3.2363)\tF1 0.697 (0.651)\n",
            "Epoch: [15][600/1405]\tBatch Time 0.161 (0.214)\tData Load Time 0.004 (0.005)\tCE Loss 11.4314 (11.7603)\tVB Loss 2.8050 (3.2141)\tF1 0.580 (0.652)\n",
            "Epoch: [15][700/1405]\tBatch Time 0.190 (0.212)\tData Load Time 0.005 (0.005)\tCE Loss 11.6853 (11.7519)\tVB Loss 3.5760 (3.1987)\tF1 0.793 (0.655)\n",
            "Epoch: [15][800/1405]\tBatch Time 0.220 (0.211)\tData Load Time 0.004 (0.005)\tCE Loss 11.9238 (11.7467)\tVB Loss 3.0532 (3.2099)\tF1 0.599 (0.655)\n",
            "Epoch: [15][900/1405]\tBatch Time 0.222 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 12.4553 (11.7360)\tVB Loss 5.9734 (3.2067)\tF1 0.532 (0.655)\n",
            "Epoch: [15][1000/1405]\tBatch Time 0.215 (0.210)\tData Load Time 0.005 (0.005)\tCE Loss 11.1765 (11.7334)\tVB Loss 5.5003 (3.2020)\tF1 0.743 (0.654)\n",
            "Epoch: [15][1100/1405]\tBatch Time 0.165 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 11.0544 (11.7333)\tVB Loss 1.9113 (3.2053)\tF1 0.813 (0.652)\n",
            "Epoch: [15][1200/1405]\tBatch Time 0.216 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 11.3810 (11.7269)\tVB Loss 1.7038 (3.1965)\tF1 0.860 (0.653)\n",
            "Epoch: [15][1300/1405]\tBatch Time 0.228 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 11.8065 (11.7210)\tVB Loss 1.9135 (3.1759)\tF1 0.792 (0.653)\n",
            "Epoch: [15][1400/1405]\tBatch Time 0.211 (0.209)\tData Load Time 0.005 (0.005)\tCE Loss 12.1093 (11.7144)\tVB Loss 5.2235 (3.1641)\tF1 0.702 (0.653)\n",
            "Validation: [0/325]\tBatch Time 0.352 (0.352)\tVB Loss 1.4601 (1.4601)\tF1 Score 0.834 (0.834)\t\n",
            "Validation: [100/325]\tBatch Time 0.073 (0.110)\tVB Loss 1.3826 (2.8091)\tF1 Score 0.399 (0.723)\t\n",
            "Validation: [200/325]\tBatch Time 0.103 (0.106)\tVB Loss 3.1010 (2.7335)\tF1 Score 0.712 (0.728)\t\n",
            "Validation: [300/325]\tBatch Time 0.104 (0.103)\tVB Loss 1.3832 (2.7639)\tF1 Score 0.746 (0.728)\t\n",
            "\n",
            " * LOSS - 2.769, F1 SCORE - 0.729\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000833\n",
            "\n",
            "Epoch: [16][0/1405]\tBatch Time 0.399 (0.399)\tData Load Time 0.236 (0.236)\tCE Loss 12.1604 (12.1604)\tVB Loss 1.5719 (1.5719)\tF1 0.795 (0.795)\n",
            "Epoch: [16][100/1405]\tBatch Time 0.143 (0.228)\tData Load Time 0.004 (0.007)\tCE Loss 11.7474 (11.7591)\tVB Loss 3.0640 (3.0309)\tF1 0.738 (0.662)\n",
            "Epoch: [16][200/1405]\tBatch Time 0.199 (0.217)\tData Load Time 0.005 (0.006)\tCE Loss 12.0755 (11.7104)\tVB Loss 5.0344 (3.0508)\tF1 0.526 (0.657)\n",
            "Epoch: [16][300/1405]\tBatch Time 0.226 (0.214)\tData Load Time 0.004 (0.006)\tCE Loss 11.9330 (11.7128)\tVB Loss 4.3496 (3.1240)\tF1 0.709 (0.651)\n",
            "Epoch: [16][400/1405]\tBatch Time 0.196 (0.214)\tData Load Time 0.004 (0.005)\tCE Loss 11.8451 (11.7111)\tVB Loss 4.1979 (3.1326)\tF1 0.580 (0.648)\n",
            "Epoch: [16][500/1405]\tBatch Time 0.214 (0.211)\tData Load Time 0.005 (0.005)\tCE Loss 11.6315 (11.6996)\tVB Loss 3.1844 (3.0989)\tF1 0.620 (0.652)\n",
            "Epoch: [16][600/1405]\tBatch Time 0.193 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 11.9224 (11.7046)\tVB Loss 1.9934 (3.1068)\tF1 0.784 (0.654)\n",
            "Epoch: [16][700/1405]\tBatch Time 0.206 (0.209)\tData Load Time 0.006 (0.005)\tCE Loss 11.9590 (11.7002)\tVB Loss 4.8809 (3.1164)\tF1 0.657 (0.654)\n",
            "Epoch: [16][800/1405]\tBatch Time 0.232 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 11.9507 (11.7114)\tVB Loss 1.3686 (3.1440)\tF1 0.938 (0.654)\n",
            "Epoch: [16][900/1405]\tBatch Time 0.175 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 11.0322 (11.7069)\tVB Loss 1.7764 (3.1445)\tF1 0.898 (0.653)\n",
            "Epoch: [16][1000/1405]\tBatch Time 0.218 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 11.3928 (11.7066)\tVB Loss 0.7285 (3.1265)\tF1 0.941 (0.655)\n",
            "Epoch: [16][1100/1405]\tBatch Time 0.230 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 12.2522 (11.6977)\tVB Loss 2.6541 (3.1204)\tF1 0.796 (0.656)\n",
            "Epoch: [16][1200/1405]\tBatch Time 0.219 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 11.8869 (11.6909)\tVB Loss 3.4154 (3.1147)\tF1 0.641 (0.657)\n",
            "Epoch: [16][1300/1405]\tBatch Time 0.196 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 11.7218 (11.6967)\tVB Loss 1.6503 (3.1127)\tF1 0.623 (0.657)\n",
            "Epoch: [16][1400/1405]\tBatch Time 0.320 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 11.3935 (11.6939)\tVB Loss 4.5072 (3.1097)\tF1 0.652 (0.658)\n",
            "Validation: [0/325]\tBatch Time 0.302 (0.302)\tVB Loss 2.1220 (2.1220)\tF1 Score 0.872 (0.872)\t\n",
            "Validation: [100/325]\tBatch Time 0.076 (0.098)\tVB Loss 2.4716 (2.5682)\tF1 Score 0.832 (0.739)\t\n",
            "Validation: [200/325]\tBatch Time 0.086 (0.100)\tVB Loss 2.4167 (2.6840)\tF1 Score 0.712 (0.733)\t\n",
            "Validation: [300/325]\tBatch Time 0.081 (0.103)\tVB Loss 2.3097 (2.6302)\tF1 Score 0.781 (0.743)\t\n",
            "\n",
            " * LOSS - 2.623, F1 SCORE - 0.746\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000811\n",
            "\n",
            "Epoch: [17][0/1405]\tBatch Time 0.597 (0.597)\tData Load Time 0.285 (0.285)\tCE Loss 11.2981 (11.2981)\tVB Loss 2.4550 (2.4550)\tF1 0.583 (0.583)\n",
            "Epoch: [17][100/1405]\tBatch Time 0.216 (0.227)\tData Load Time 0.005 (0.008)\tCE Loss 11.4387 (11.6900)\tVB Loss 2.7907 (2.8105)\tF1 0.653 (0.692)\n",
            "Epoch: [17][200/1405]\tBatch Time 0.192 (0.215)\tData Load Time 0.005 (0.006)\tCE Loss 11.7026 (11.7301)\tVB Loss 3.0725 (2.9387)\tF1 0.669 (0.681)\n",
            "Epoch: [17][300/1405]\tBatch Time 0.268 (0.218)\tData Load Time 0.004 (0.006)\tCE Loss 12.1547 (11.7022)\tVB Loss 4.0793 (2.9769)\tF1 0.522 (0.672)\n",
            "Epoch: [17][400/1405]\tBatch Time 0.192 (0.216)\tData Load Time 0.005 (0.006)\tCE Loss 10.6758 (11.6815)\tVB Loss 3.7791 (3.0230)\tF1 0.711 (0.668)\n",
            "Epoch: [17][500/1405]\tBatch Time 0.244 (0.214)\tData Load Time 0.004 (0.005)\tCE Loss 11.1141 (11.6727)\tVB Loss 5.0811 (3.0366)\tF1 0.618 (0.666)\n",
            "Epoch: [17][600/1405]\tBatch Time 0.242 (0.213)\tData Load Time 0.004 (0.005)\tCE Loss 11.4168 (11.6797)\tVB Loss 4.2806 (3.0547)\tF1 0.577 (0.664)\n",
            "Epoch: [17][700/1405]\tBatch Time 0.174 (0.212)\tData Load Time 0.005 (0.005)\tCE Loss 11.7818 (11.6789)\tVB Loss 5.2958 (3.0447)\tF1 0.524 (0.668)\n",
            "Epoch: [17][800/1405]\tBatch Time 0.195 (0.211)\tData Load Time 0.004 (0.005)\tCE Loss 11.6371 (11.6739)\tVB Loss 2.1019 (3.0319)\tF1 0.437 (0.668)\n",
            "Epoch: [17][900/1405]\tBatch Time 0.289 (0.211)\tData Load Time 0.004 (0.005)\tCE Loss 11.7625 (11.6827)\tVB Loss 4.8143 (3.0573)\tF1 0.628 (0.668)\n",
            "Epoch: [17][1000/1405]\tBatch Time 0.205 (0.210)\tData Load Time 0.005 (0.005)\tCE Loss 12.4299 (11.6768)\tVB Loss 1.8237 (3.0291)\tF1 0.319 (0.670)\n",
            "Epoch: [17][1100/1405]\tBatch Time 0.206 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 12.7291 (11.6777)\tVB Loss 4.6821 (3.0409)\tF1 0.699 (0.669)\n",
            "Epoch: [17][1200/1405]\tBatch Time 0.207 (0.210)\tData Load Time 0.005 (0.005)\tCE Loss 12.5130 (11.6830)\tVB Loss 0.9470 (3.0529)\tF1 0.932 (0.670)\n",
            "Epoch: [17][1300/1405]\tBatch Time 0.154 (0.209)\tData Load Time 0.005 (0.005)\tCE Loss 11.3191 (11.6836)\tVB Loss 2.2862 (3.0408)\tF1 0.564 (0.669)\n",
            "Epoch: [17][1400/1405]\tBatch Time 0.213 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 11.7712 (11.6838)\tVB Loss 2.4103 (3.0372)\tF1 0.810 (0.669)\n",
            "Validation: [0/325]\tBatch Time 0.364 (0.364)\tVB Loss 1.1745 (1.1745)\tF1 Score 0.779 (0.779)\t\n",
            "Validation: [100/325]\tBatch Time 0.111 (0.106)\tVB Loss 8.3059 (2.6087)\tF1 Score 0.624 (0.720)\t\n",
            "Validation: [200/325]\tBatch Time 0.124 (0.112)\tVB Loss 3.9921 (2.6859)\tF1 Score 0.794 (0.729)\t\n",
            "Validation: [300/325]\tBatch Time 0.111 (0.114)\tVB Loss 3.2028 (2.6396)\tF1 Score 0.877 (0.738)\t\n",
            "\n",
            " * LOSS - 2.649, F1 SCORE - 0.737\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000789\n",
            "\n",
            "Epoch: [18][0/1405]\tBatch Time 0.478 (0.478)\tData Load Time 0.229 (0.229)\tCE Loss 11.5234 (11.5234)\tVB Loss 1.3919 (1.3919)\tF1 0.920 (0.920)\n",
            "Epoch: [18][100/1405]\tBatch Time 0.233 (0.229)\tData Load Time 0.004 (0.007)\tCE Loss 11.6628 (11.6673)\tVB Loss 1.6436 (2.8740)\tF1 0.828 (0.670)\n",
            "Epoch: [18][200/1405]\tBatch Time 0.213 (0.218)\tData Load Time 0.004 (0.006)\tCE Loss 11.7344 (11.7275)\tVB Loss 2.8578 (2.9492)\tF1 0.636 (0.657)\n",
            "Epoch: [18][300/1405]\tBatch Time 0.189 (0.216)\tData Load Time 0.004 (0.006)\tCE Loss 12.2800 (11.7059)\tVB Loss 1.0996 (2.9442)\tF1 0.965 (0.669)\n",
            "Epoch: [18][400/1405]\tBatch Time 0.256 (0.214)\tData Load Time 0.004 (0.005)\tCE Loss 11.8190 (11.6827)\tVB Loss 3.6507 (2.9401)\tF1 0.675 (0.668)\n",
            "Epoch: [18][500/1405]\tBatch Time 0.205 (0.211)\tData Load Time 0.004 (0.005)\tCE Loss 12.4232 (11.6823)\tVB Loss 4.3940 (2.9395)\tF1 0.733 (0.671)\n",
            "Epoch: [18][600/1405]\tBatch Time 0.261 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 11.5962 (11.6854)\tVB Loss 4.6706 (2.9615)\tF1 0.696 (0.674)\n",
            "Epoch: [18][700/1405]\tBatch Time 0.244 (0.210)\tData Load Time 0.005 (0.005)\tCE Loss 12.5829 (11.6772)\tVB Loss 4.2057 (2.9915)\tF1 0.650 (0.672)\n",
            "Epoch: [18][800/1405]\tBatch Time 0.236 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 12.0062 (11.6731)\tVB Loss 2.4223 (2.9923)\tF1 0.594 (0.672)\n",
            "Epoch: [18][900/1405]\tBatch Time 0.173 (0.209)\tData Load Time 0.005 (0.005)\tCE Loss 11.8664 (11.6757)\tVB Loss 3.3381 (2.9998)\tF1 0.618 (0.671)\n",
            "Epoch: [18][1000/1405]\tBatch Time 0.229 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 12.2075 (11.6720)\tVB Loss 3.1465 (2.9997)\tF1 0.860 (0.670)\n",
            "Epoch: [18][1100/1405]\tBatch Time 0.174 (0.209)\tData Load Time 0.005 (0.005)\tCE Loss 10.6915 (11.6692)\tVB Loss 1.7687 (3.0067)\tF1 0.794 (0.670)\n",
            "Epoch: [18][1200/1405]\tBatch Time 0.243 (0.208)\tData Load Time 0.005 (0.005)\tCE Loss 11.4269 (11.6675)\tVB Loss 4.1075 (3.0059)\tF1 0.757 (0.670)\n",
            "Epoch: [18][1300/1405]\tBatch Time 0.179 (0.208)\tData Load Time 0.006 (0.005)\tCE Loss 12.3324 (11.6688)\tVB Loss 4.9951 (3.0036)\tF1 0.653 (0.670)\n",
            "Epoch: [18][1400/1405]\tBatch Time 0.220 (0.207)\tData Load Time 0.004 (0.005)\tCE Loss 11.0807 (11.6699)\tVB Loss 2.1037 (2.9983)\tF1 0.642 (0.669)\n",
            "Validation: [0/325]\tBatch Time 0.314 (0.314)\tVB Loss 2.9169 (2.9169)\tF1 Score 0.660 (0.660)\t\n",
            "Validation: [100/325]\tBatch Time 0.105 (0.113)\tVB Loss 3.2478 (2.5938)\tF1 Score 0.655 (0.753)\t\n",
            "Validation: [200/325]\tBatch Time 0.088 (0.115)\tVB Loss 2.1021 (2.5345)\tF1 Score 0.479 (0.744)\t\n",
            "Validation: [300/325]\tBatch Time 0.127 (0.115)\tVB Loss 2.6154 (2.5406)\tF1 Score 0.705 (0.742)\t\n",
            "\n",
            " * LOSS - 2.536, F1 SCORE - 0.745\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000769\n",
            "\n",
            "Epoch: [19][0/1405]\tBatch Time 0.503 (0.503)\tData Load Time 0.247 (0.247)\tCE Loss 10.5189 (10.5189)\tVB Loss 2.0231 (2.0231)\tF1 0.556 (0.556)\n",
            "Epoch: [19][100/1405]\tBatch Time 0.187 (0.236)\tData Load Time 0.004 (0.007)\tCE Loss 11.4865 (11.6151)\tVB Loss 1.0285 (2.8546)\tF1 0.878 (0.676)\n",
            "Epoch: [19][200/1405]\tBatch Time 0.251 (0.223)\tData Load Time 0.004 (0.006)\tCE Loss 11.9563 (11.6315)\tVB Loss 2.8732 (2.9274)\tF1 0.498 (0.669)\n",
            "Epoch: [19][300/1405]\tBatch Time 0.232 (0.216)\tData Load Time 0.004 (0.005)\tCE Loss 11.9126 (11.6478)\tVB Loss 3.2105 (2.9181)\tF1 0.623 (0.668)\n",
            "Epoch: [19][400/1405]\tBatch Time 0.086 (0.213)\tData Load Time 0.004 (0.005)\tCE Loss 10.3363 (11.6393)\tVB Loss 1.8131 (2.9377)\tF1 0.853 (0.667)\n",
            "Epoch: [19][500/1405]\tBatch Time 0.202 (0.212)\tData Load Time 0.004 (0.005)\tCE Loss 11.6847 (11.6383)\tVB Loss 2.9723 (2.9826)\tF1 0.725 (0.672)\n",
            "Epoch: [19][600/1405]\tBatch Time 0.136 (0.211)\tData Load Time 0.004 (0.005)\tCE Loss 11.2048 (11.6441)\tVB Loss 3.2924 (2.9559)\tF1 0.561 (0.671)\n",
            "Epoch: [19][700/1405]\tBatch Time 0.111 (0.209)\tData Load Time 0.005 (0.005)\tCE Loss 11.2602 (11.6551)\tVB Loss 1.4526 (2.9557)\tF1 0.550 (0.669)\n",
            "Epoch: [19][800/1405]\tBatch Time 0.179 (0.208)\tData Load Time 0.005 (0.005)\tCE Loss 11.3577 (11.6500)\tVB Loss 2.4216 (2.9756)\tF1 0.512 (0.669)\n",
            "Epoch: [19][900/1405]\tBatch Time 0.221 (0.207)\tData Load Time 0.004 (0.005)\tCE Loss 11.5379 (11.6506)\tVB Loss 2.2905 (2.9602)\tF1 0.809 (0.670)\n",
            "Epoch: [19][1000/1405]\tBatch Time 0.179 (0.207)\tData Load Time 0.005 (0.005)\tCE Loss 12.2766 (11.6492)\tVB Loss 4.4804 (2.9615)\tF1 0.572 (0.670)\n",
            "Epoch: [19][1100/1405]\tBatch Time 0.172 (0.207)\tData Load Time 0.005 (0.005)\tCE Loss 11.1148 (11.6474)\tVB Loss 2.6283 (2.9531)\tF1 0.796 (0.670)\n",
            "Epoch: [19][1200/1405]\tBatch Time 0.251 (0.207)\tData Load Time 0.004 (0.005)\tCE Loss 11.9583 (11.6495)\tVB Loss 3.6279 (2.9453)\tF1 0.673 (0.672)\n",
            "Epoch: [19][1300/1405]\tBatch Time 0.191 (0.207)\tData Load Time 0.005 (0.005)\tCE Loss 12.5511 (11.6499)\tVB Loss 1.8758 (2.9510)\tF1 0.727 (0.672)\n",
            "Epoch: [19][1400/1405]\tBatch Time 0.250 (0.207)\tData Load Time 0.004 (0.005)\tCE Loss 11.2378 (11.6524)\tVB Loss 2.1709 (2.9568)\tF1 0.654 (0.672)\n",
            "Validation: [0/325]\tBatch Time 0.328 (0.328)\tVB Loss 1.7438 (1.7438)\tF1 Score 0.948 (0.948)\t\n",
            "Validation: [100/325]\tBatch Time 0.118 (0.111)\tVB Loss 2.6722 (2.3814)\tF1 Score 0.803 (0.761)\t\n",
            "Validation: [200/325]\tBatch Time 0.114 (0.115)\tVB Loss 1.0140 (2.5271)\tF1 Score 0.892 (0.759)\t\n",
            "Validation: [300/325]\tBatch Time 0.106 (0.116)\tVB Loss 2.5918 (2.5691)\tF1 Score 0.714 (0.750)\t\n",
            "\n",
            " * LOSS - 2.539, F1 SCORE - 0.751\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000750\n",
            "\n",
            "Epoch: [20][0/1405]\tBatch Time 0.464 (0.464)\tData Load Time 0.225 (0.225)\tCE Loss 11.2508 (11.2508)\tVB Loss 2.1708 (2.1708)\tF1 0.460 (0.460)\n",
            "Epoch: [20][100/1405]\tBatch Time 0.273 (0.241)\tData Load Time 0.004 (0.007)\tCE Loss 11.2282 (11.7002)\tVB Loss 2.3934 (3.0725)\tF1 0.672 (0.680)\n",
            "Epoch: [20][200/1405]\tBatch Time 0.171 (0.223)\tData Load Time 0.004 (0.006)\tCE Loss 11.4339 (11.6661)\tVB Loss 3.3989 (2.9803)\tF1 0.704 (0.673)\n",
            "Epoch: [20][300/1405]\tBatch Time 0.219 (0.219)\tData Load Time 0.005 (0.005)\tCE Loss 11.2107 (11.6760)\tVB Loss 1.9034 (3.0037)\tF1 0.849 (0.678)\n",
            "Epoch: [20][400/1405]\tBatch Time 0.241 (0.216)\tData Load Time 0.006 (0.005)\tCE Loss 11.1242 (11.6702)\tVB Loss 2.5454 (2.9716)\tF1 0.653 (0.679)\n",
            "Epoch: [20][500/1405]\tBatch Time 0.231 (0.214)\tData Load Time 0.005 (0.005)\tCE Loss 12.0751 (11.6632)\tVB Loss 4.3168 (2.9768)\tF1 0.717 (0.677)\n",
            "Epoch: [20][600/1405]\tBatch Time 0.184 (0.212)\tData Load Time 0.004 (0.005)\tCE Loss 11.6144 (11.6529)\tVB Loss 2.9235 (2.9656)\tF1 0.477 (0.676)\n",
            "Epoch: [20][700/1405]\tBatch Time 0.163 (0.210)\tData Load Time 0.005 (0.005)\tCE Loss 11.4347 (11.6444)\tVB Loss 2.3255 (2.9225)\tF1 0.747 (0.678)\n",
            "Epoch: [20][800/1405]\tBatch Time 0.241 (0.210)\tData Load Time 0.006 (0.005)\tCE Loss 12.5100 (11.6487)\tVB Loss 1.9009 (2.9019)\tF1 0.692 (0.679)\n",
            "Epoch: [20][900/1405]\tBatch Time 0.179 (0.210)\tData Load Time 0.005 (0.005)\tCE Loss 11.1983 (11.6457)\tVB Loss 3.1817 (2.9029)\tF1 0.730 (0.679)\n",
            "Epoch: [20][1000/1405]\tBatch Time 0.235 (0.210)\tData Load Time 0.005 (0.005)\tCE Loss 12.1533 (11.6449)\tVB Loss 4.6066 (2.9021)\tF1 0.689 (0.680)\n",
            "Epoch: [20][1100/1405]\tBatch Time 0.215 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 11.2304 (11.6422)\tVB Loss 2.4291 (2.9112)\tF1 0.579 (0.680)\n",
            "Epoch: [20][1200/1405]\tBatch Time 0.189 (0.209)\tData Load Time 0.005 (0.005)\tCE Loss 11.2327 (11.6432)\tVB Loss 3.0290 (2.9187)\tF1 0.618 (0.679)\n",
            "Epoch: [20][1300/1405]\tBatch Time 0.167 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 11.0454 (11.6408)\tVB Loss 1.6405 (2.9199)\tF1 0.657 (0.678)\n",
            "Epoch: [20][1400/1405]\tBatch Time 0.151 (0.208)\tData Load Time 0.005 (0.005)\tCE Loss 10.6239 (11.6431)\tVB Loss 0.8853 (2.9201)\tF1 0.791 (0.677)\n",
            "Validation: [0/325]\tBatch Time 0.345 (0.345)\tVB Loss 1.7025 (1.7025)\tF1 Score 0.787 (0.787)\t\n",
            "Validation: [100/325]\tBatch Time 0.118 (0.112)\tVB Loss 4.7680 (2.4686)\tF1 Score 0.530 (0.749)\t\n",
            "Validation: [200/325]\tBatch Time 0.132 (0.113)\tVB Loss 3.4431 (2.5152)\tF1 Score 0.740 (0.750)\t\n",
            "Validation: [300/325]\tBatch Time 0.115 (0.114)\tVB Loss 2.3523 (2.5175)\tF1 Score 0.728 (0.755)\t\n",
            "\n",
            " * LOSS - 2.543, F1 SCORE - 0.754\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000732\n",
            "\n",
            "Epoch: [21][0/1405]\tBatch Time 0.528 (0.528)\tData Load Time 0.243 (0.243)\tCE Loss 10.4326 (10.4326)\tVB Loss 3.6690 (3.6690)\tF1 0.604 (0.604)\n",
            "Epoch: [21][100/1405]\tBatch Time 0.278 (0.233)\tData Load Time 0.005 (0.008)\tCE Loss 10.9394 (11.7424)\tVB Loss 6.2492 (3.1433)\tF1 0.625 (0.681)\n",
            "Epoch: [21][200/1405]\tBatch Time 0.185 (0.218)\tData Load Time 0.005 (0.006)\tCE Loss 11.8264 (11.6690)\tVB Loss 4.8939 (2.9734)\tF1 0.576 (0.676)\n",
            "Epoch: [21][300/1405]\tBatch Time 0.255 (0.214)\tData Load Time 0.004 (0.006)\tCE Loss 11.5588 (11.6425)\tVB Loss 2.8243 (2.9467)\tF1 0.727 (0.666)\n",
            "Epoch: [21][400/1405]\tBatch Time 0.215 (0.212)\tData Load Time 0.005 (0.005)\tCE Loss 11.5735 (11.6357)\tVB Loss 3.5470 (2.9114)\tF1 0.594 (0.668)\n",
            "Epoch: [21][500/1405]\tBatch Time 0.160 (0.211)\tData Load Time 0.004 (0.005)\tCE Loss 10.8039 (11.6450)\tVB Loss 2.1365 (2.9130)\tF1 0.640 (0.672)\n",
            "Epoch: [21][600/1405]\tBatch Time 0.145 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 11.0526 (11.6469)\tVB Loss 1.8337 (2.8966)\tF1 0.659 (0.673)\n",
            "Epoch: [21][700/1405]\tBatch Time 0.201 (0.209)\tData Load Time 0.005 (0.005)\tCE Loss 10.6589 (11.6474)\tVB Loss 4.2822 (2.8904)\tF1 0.702 (0.673)\n",
            "Epoch: [21][800/1405]\tBatch Time 0.224 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 11.6686 (11.6325)\tVB Loss 3.7050 (2.8945)\tF1 0.504 (0.674)\n",
            "Epoch: [21][900/1405]\tBatch Time 0.202 (0.207)\tData Load Time 0.004 (0.005)\tCE Loss 11.8241 (11.6250)\tVB Loss 3.1710 (2.9002)\tF1 0.824 (0.675)\n",
            "Epoch: [21][1000/1405]\tBatch Time 0.201 (0.207)\tData Load Time 0.004 (0.005)\tCE Loss 11.7435 (11.6263)\tVB Loss 3.0453 (2.8862)\tF1 0.650 (0.677)\n",
            "Epoch: [21][1100/1405]\tBatch Time 0.232 (0.207)\tData Load Time 0.005 (0.005)\tCE Loss 11.6967 (11.6250)\tVB Loss 1.8443 (2.8750)\tF1 0.750 (0.679)\n",
            "Epoch: [21][1200/1405]\tBatch Time 0.187 (0.207)\tData Load Time 0.004 (0.005)\tCE Loss 11.1012 (11.6267)\tVB Loss 2.8633 (2.8858)\tF1 0.742 (0.679)\n",
            "Epoch: [21][1300/1405]\tBatch Time 0.221 (0.207)\tData Load Time 0.006 (0.005)\tCE Loss 11.1769 (11.6270)\tVB Loss 0.7165 (2.8792)\tF1 0.959 (0.679)\n",
            "Epoch: [21][1400/1405]\tBatch Time 0.243 (0.207)\tData Load Time 0.005 (0.005)\tCE Loss 11.1934 (11.6285)\tVB Loss 3.6620 (2.8767)\tF1 0.544 (0.679)\n",
            "Validation: [0/325]\tBatch Time 0.290 (0.290)\tVB Loss 2.1335 (2.1335)\tF1 Score 0.656 (0.656)\t\n",
            "Validation: [100/325]\tBatch Time 0.106 (0.103)\tVB Loss 2.9857 (2.5956)\tF1 Score 0.593 (0.754)\t\n",
            "Validation: [200/325]\tBatch Time 0.209 (0.104)\tVB Loss 2.3234 (2.4914)\tF1 Score 0.687 (0.757)\t\n",
            "Validation: [300/325]\tBatch Time 0.092 (0.102)\tVB Loss 2.3602 (2.4510)\tF1 Score 0.734 (0.757)\t\n",
            "\n",
            " * LOSS - 2.457, F1 SCORE - 0.758\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000714\n",
            "\n",
            "Epoch: [22][0/1405]\tBatch Time 0.527 (0.527)\tData Load Time 0.240 (0.240)\tCE Loss 11.2257 (11.2257)\tVB Loss 3.4428 (3.4428)\tF1 0.674 (0.674)\n",
            "Epoch: [22][100/1405]\tBatch Time 0.139 (0.232)\tData Load Time 0.004 (0.007)\tCE Loss 11.0656 (11.6098)\tVB Loss 3.3243 (3.0937)\tF1 0.535 (0.664)\n",
            "Epoch: [22][200/1405]\tBatch Time 0.177 (0.219)\tData Load Time 0.004 (0.006)\tCE Loss 11.1945 (11.6740)\tVB Loss 4.4204 (2.9833)\tF1 0.494 (0.685)\n",
            "Epoch: [22][300/1405]\tBatch Time 0.103 (0.217)\tData Load Time 0.005 (0.005)\tCE Loss 11.5409 (11.6652)\tVB Loss 1.7492 (2.9280)\tF1 0.694 (0.688)\n",
            "Epoch: [22][400/1405]\tBatch Time 0.159 (0.213)\tData Load Time 0.005 (0.005)\tCE Loss 11.4225 (11.6360)\tVB Loss 3.8152 (2.8406)\tF1 0.631 (0.687)\n",
            "Epoch: [22][500/1405]\tBatch Time 0.168 (0.209)\tData Load Time 0.005 (0.005)\tCE Loss 11.0380 (11.6324)\tVB Loss 2.4658 (2.7947)\tF1 0.684 (0.686)\n",
            "Epoch: [22][600/1405]\tBatch Time 0.184 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 11.1375 (11.6241)\tVB Loss 2.4771 (2.7902)\tF1 0.565 (0.684)\n",
            "Epoch: [22][700/1405]\tBatch Time 0.219 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 11.6809 (11.6111)\tVB Loss 3.7586 (2.8189)\tF1 0.574 (0.682)\n",
            "Epoch: [22][800/1405]\tBatch Time 0.275 (0.207)\tData Load Time 0.004 (0.005)\tCE Loss 11.5833 (11.6070)\tVB Loss 2.8321 (2.7956)\tF1 0.872 (0.686)\n",
            "Epoch: [22][900/1405]\tBatch Time 0.174 (0.207)\tData Load Time 0.004 (0.005)\tCE Loss 11.9359 (11.6124)\tVB Loss 1.5746 (2.8255)\tF1 0.772 (0.684)\n",
            "Epoch: [22][1000/1405]\tBatch Time 0.197 (0.207)\tData Load Time 0.004 (0.005)\tCE Loss 12.2746 (11.6110)\tVB Loss 3.2011 (2.8265)\tF1 0.639 (0.685)\n",
            "Epoch: [22][1100/1405]\tBatch Time 0.199 (0.207)\tData Load Time 0.004 (0.005)\tCE Loss 11.5032 (11.6157)\tVB Loss 3.0265 (2.8500)\tF1 0.596 (0.683)\n",
            "Epoch: [22][1200/1405]\tBatch Time 0.236 (0.207)\tData Load Time 0.004 (0.005)\tCE Loss 11.3896 (11.6177)\tVB Loss 3.5540 (2.8403)\tF1 0.704 (0.684)\n",
            "Epoch: [22][1300/1405]\tBatch Time 0.273 (0.206)\tData Load Time 0.004 (0.005)\tCE Loss 11.1057 (11.6193)\tVB Loss 2.3872 (2.8315)\tF1 0.725 (0.685)\n",
            "Epoch: [22][1400/1405]\tBatch Time 0.198 (0.206)\tData Load Time 0.004 (0.005)\tCE Loss 11.0296 (11.6192)\tVB Loss 2.0530 (2.8393)\tF1 0.706 (0.684)\n",
            "Validation: [0/325]\tBatch Time 0.322 (0.322)\tVB Loss 1.9464 (1.9464)\tF1 Score 0.768 (0.768)\t\n",
            "Validation: [100/325]\tBatch Time 0.044 (0.100)\tVB Loss 1.6497 (2.5032)\tF1 Score 0.810 (0.758)\t\n",
            "Validation: [200/325]\tBatch Time 0.074 (0.101)\tVB Loss 5.0088 (2.4883)\tF1 Score 0.651 (0.764)\t\n",
            "Validation: [300/325]\tBatch Time 0.195 (0.100)\tVB Loss 2.7318 (2.4824)\tF1 Score 0.817 (0.765)\t\n",
            "\n",
            " * LOSS - 2.491, F1 SCORE - 0.763\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000698\n",
            "\n",
            "Epoch: [23][0/1405]\tBatch Time 0.514 (0.514)\tData Load Time 0.226 (0.226)\tCE Loss 12.2799 (12.2799)\tVB Loss 1.7200 (1.7200)\tF1 0.910 (0.910)\n",
            "Epoch: [23][100/1405]\tBatch Time 0.242 (0.232)\tData Load Time 0.005 (0.008)\tCE Loss 12.8463 (11.5979)\tVB Loss 2.8899 (2.7666)\tF1 0.580 (0.690)\n",
            "Epoch: [23][200/1405]\tBatch Time 0.220 (0.218)\tData Load Time 0.006 (0.006)\tCE Loss 11.5860 (11.5872)\tVB Loss 3.1245 (2.7154)\tF1 0.686 (0.694)\n",
            "Epoch: [23][300/1405]\tBatch Time 0.232 (0.214)\tData Load Time 0.004 (0.006)\tCE Loss 11.8113 (11.5996)\tVB Loss 2.8743 (2.6539)\tF1 0.667 (0.693)\n",
            "Epoch: [23][400/1405]\tBatch Time 0.191 (0.211)\tData Load Time 0.004 (0.005)\tCE Loss 11.7045 (11.6204)\tVB Loss 2.7369 (2.6914)\tF1 0.519 (0.694)\n",
            "Epoch: [23][500/1405]\tBatch Time 0.214 (0.210)\tData Load Time 0.006 (0.005)\tCE Loss 12.2804 (11.6126)\tVB Loss 3.6422 (2.6983)\tF1 0.606 (0.692)\n",
            "Epoch: [23][600/1405]\tBatch Time 0.288 (0.209)\tData Load Time 0.005 (0.005)\tCE Loss 11.7104 (11.6130)\tVB Loss 3.5385 (2.7356)\tF1 0.794 (0.692)\n",
            "Epoch: [23][700/1405]\tBatch Time 0.212 (0.208)\tData Load Time 0.005 (0.005)\tCE Loss 11.5227 (11.6179)\tVB Loss 3.3455 (2.7845)\tF1 0.656 (0.691)\n",
            "Epoch: [23][800/1405]\tBatch Time 0.148 (0.207)\tData Load Time 0.004 (0.005)\tCE Loss 10.6285 (11.6046)\tVB Loss 1.0823 (2.7746)\tF1 0.590 (0.693)\n",
            "Epoch: [23][900/1405]\tBatch Time 0.176 (0.207)\tData Load Time 0.005 (0.005)\tCE Loss 11.6930 (11.6109)\tVB Loss 3.3025 (2.8132)\tF1 0.581 (0.690)\n",
            "Epoch: [23][1000/1405]\tBatch Time 0.142 (0.206)\tData Load Time 0.004 (0.005)\tCE Loss 11.4609 (11.6097)\tVB Loss 1.2458 (2.8147)\tF1 0.796 (0.691)\n",
            "Epoch: [23][1100/1405]\tBatch Time 0.222 (0.206)\tData Load Time 0.004 (0.005)\tCE Loss 10.8964 (11.6061)\tVB Loss 2.7329 (2.8240)\tF1 0.761 (0.690)\n",
            "Epoch: [23][1200/1405]\tBatch Time 0.172 (0.206)\tData Load Time 0.004 (0.005)\tCE Loss 12.1747 (11.6007)\tVB Loss 3.9417 (2.8323)\tF1 0.459 (0.686)\n",
            "Epoch: [23][1300/1405]\tBatch Time 0.215 (0.206)\tData Load Time 0.004 (0.005)\tCE Loss 11.9888 (11.6046)\tVB Loss 2.1429 (2.8208)\tF1 0.807 (0.686)\n",
            "Epoch: [23][1400/1405]\tBatch Time 0.127 (0.206)\tData Load Time 0.004 (0.005)\tCE Loss 11.4956 (11.6064)\tVB Loss 0.9144 (2.8119)\tF1 0.928 (0.687)\n",
            "Validation: [0/325]\tBatch Time 0.334 (0.334)\tVB Loss 5.4593 (5.4593)\tF1 Score 0.606 (0.606)\t\n",
            "Validation: [100/325]\tBatch Time 0.149 (0.103)\tVB Loss 1.8189 (2.4326)\tF1 Score 0.718 (0.751)\t\n",
            "Validation: [200/325]\tBatch Time 0.080 (0.104)\tVB Loss 1.8567 (2.3665)\tF1 Score 0.875 (0.764)\t\n",
            "Validation: [300/325]\tBatch Time 0.104 (0.108)\tVB Loss 0.6858 (2.4172)\tF1 Score 0.948 (0.765)\t\n",
            "\n",
            " * LOSS - 2.417, F1 SCORE - 0.766\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000682\n",
            "\n",
            "Epoch: [24][0/1405]\tBatch Time 0.599 (0.599)\tData Load Time 0.246 (0.246)\tCE Loss 11.8250 (11.8250)\tVB Loss 2.6887 (2.6887)\tF1 0.728 (0.728)\n",
            "Epoch: [24][100/1405]\tBatch Time 0.210 (0.232)\tData Load Time 0.004 (0.007)\tCE Loss 11.3772 (11.5045)\tVB Loss 2.4886 (2.7984)\tF1 0.736 (0.700)\n",
            "Epoch: [24][200/1405]\tBatch Time 0.240 (0.218)\tData Load Time 0.006 (0.006)\tCE Loss 12.0091 (11.5689)\tVB Loss 2.7568 (2.6638)\tF1 0.783 (0.701)\n",
            "Epoch: [24][300/1405]\tBatch Time 0.179 (0.215)\tData Load Time 0.004 (0.005)\tCE Loss 11.4767 (11.5882)\tVB Loss 4.0780 (2.7055)\tF1 0.493 (0.696)\n",
            "Epoch: [24][400/1405]\tBatch Time 0.168 (0.211)\tData Load Time 0.004 (0.005)\tCE Loss 10.7578 (11.6074)\tVB Loss 1.6767 (2.7391)\tF1 0.578 (0.697)\n",
            "Epoch: [24][500/1405]\tBatch Time 0.213 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 11.6637 (11.5983)\tVB Loss 3.1453 (2.6956)\tF1 0.711 (0.700)\n",
            "Epoch: [24][600/1405]\tBatch Time 0.258 (0.209)\tData Load Time 0.005 (0.005)\tCE Loss 10.9552 (11.5954)\tVB Loss 6.2215 (2.7195)\tF1 0.541 (0.698)\n",
            "Epoch: [24][700/1405]\tBatch Time 0.155 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 12.2591 (11.6002)\tVB Loss 1.7425 (2.7315)\tF1 0.807 (0.696)\n",
            "Epoch: [24][800/1405]\tBatch Time 0.219 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 11.3591 (11.5979)\tVB Loss 3.5003 (2.7447)\tF1 0.618 (0.695)\n",
            "Epoch: [24][900/1405]\tBatch Time 0.198 (0.207)\tData Load Time 0.006 (0.005)\tCE Loss 12.4033 (11.5968)\tVB Loss 3.8123 (2.7296)\tF1 0.677 (0.694)\n",
            "Epoch: [24][1000/1405]\tBatch Time 0.238 (0.207)\tData Load Time 0.004 (0.005)\tCE Loss 11.4839 (11.5912)\tVB Loss 4.0618 (2.7222)\tF1 0.577 (0.694)\n",
            "Epoch: [24][1100/1405]\tBatch Time 0.157 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 11.6825 (11.5956)\tVB Loss 2.7026 (2.7441)\tF1 0.613 (0.693)\n",
            "Epoch: [24][1200/1405]\tBatch Time 0.184 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 12.0048 (11.5956)\tVB Loss 2.1680 (2.7721)\tF1 0.705 (0.690)\n",
            "Epoch: [24][1300/1405]\tBatch Time 0.288 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 12.0815 (11.5961)\tVB Loss 4.6984 (2.7784)\tF1 0.537 (0.688)\n",
            "Epoch: [24][1400/1405]\tBatch Time 0.233 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 12.4071 (11.5953)\tVB Loss 3.7275 (2.7721)\tF1 0.614 (0.688)\n",
            "Validation: [0/325]\tBatch Time 0.355 (0.355)\tVB Loss 1.9906 (1.9906)\tF1 Score 0.883 (0.883)\t\n",
            "Validation: [100/325]\tBatch Time 0.111 (0.108)\tVB Loss 2.3675 (2.3569)\tF1 Score 0.745 (0.764)\t\n",
            "Validation: [200/325]\tBatch Time 0.119 (0.112)\tVB Loss 2.6040 (2.4242)\tF1 Score 0.718 (0.765)\t\n",
            "Validation: [300/325]\tBatch Time 0.085 (0.114)\tVB Loss 2.4906 (2.4365)\tF1 Score 0.864 (0.767)\t\n",
            "\n",
            " * LOSS - 2.451, F1 SCORE - 0.763\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000667\n",
            "\n",
            "Epoch: [25][0/1405]\tBatch Time 0.567 (0.567)\tData Load Time 0.258 (0.258)\tCE Loss 12.1769 (12.1769)\tVB Loss 3.4209 (3.4209)\tF1 0.484 (0.484)\n",
            "Epoch: [25][100/1405]\tBatch Time 0.231 (0.237)\tData Load Time 0.004 (0.007)\tCE Loss 12.1092 (11.6650)\tVB Loss 2.7460 (2.7529)\tF1 0.862 (0.702)\n",
            "Epoch: [25][200/1405]\tBatch Time 0.232 (0.223)\tData Load Time 0.004 (0.006)\tCE Loss 11.6873 (11.6284)\tVB Loss 2.6444 (2.7281)\tF1 0.676 (0.694)\n",
            "Epoch: [25][300/1405]\tBatch Time 0.256 (0.219)\tData Load Time 0.004 (0.005)\tCE Loss 12.1661 (11.6345)\tVB Loss 3.1838 (2.6700)\tF1 0.776 (0.689)\n",
            "Epoch: [25][400/1405]\tBatch Time 0.250 (0.215)\tData Load Time 0.004 (0.005)\tCE Loss 12.0671 (11.6100)\tVB Loss 4.1477 (2.6910)\tF1 0.735 (0.690)\n",
            "Epoch: [25][500/1405]\tBatch Time 0.230 (0.213)\tData Load Time 0.005 (0.005)\tCE Loss 11.3443 (11.5983)\tVB Loss 1.5737 (2.6744)\tF1 0.719 (0.689)\n",
            "Epoch: [25][600/1405]\tBatch Time 0.210 (0.212)\tData Load Time 0.004 (0.005)\tCE Loss 11.6739 (11.5884)\tVB Loss 4.0965 (2.7126)\tF1 0.768 (0.686)\n",
            "Epoch: [25][700/1405]\tBatch Time 0.193 (0.211)\tData Load Time 0.005 (0.005)\tCE Loss 11.9304 (11.5872)\tVB Loss 2.5310 (2.7128)\tF1 0.738 (0.687)\n",
            "Epoch: [25][800/1405]\tBatch Time 0.195 (0.211)\tData Load Time 0.004 (0.005)\tCE Loss 11.4332 (11.5888)\tVB Loss 2.2841 (2.7330)\tF1 0.672 (0.687)\n",
            "Epoch: [25][900/1405]\tBatch Time 0.230 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 11.9487 (11.5769)\tVB Loss 3.5940 (2.7246)\tF1 0.726 (0.688)\n",
            "Epoch: [25][1000/1405]\tBatch Time 0.168 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 11.8614 (11.5812)\tVB Loss 2.1469 (2.7399)\tF1 0.799 (0.688)\n",
            "Epoch: [25][1100/1405]\tBatch Time 0.227 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 12.0424 (11.5767)\tVB Loss 2.4854 (2.7344)\tF1 0.747 (0.688)\n",
            "Epoch: [25][1200/1405]\tBatch Time 0.265 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 12.2265 (11.5785)\tVB Loss 3.8974 (2.7397)\tF1 0.612 (0.687)\n",
            "Epoch: [25][1300/1405]\tBatch Time 0.157 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 11.6609 (11.5859)\tVB Loss 0.8021 (2.7479)\tF1 0.971 (0.688)\n",
            "Epoch: [25][1400/1405]\tBatch Time 0.204 (0.207)\tData Load Time 0.004 (0.005)\tCE Loss 11.9374 (11.5864)\tVB Loss 3.0661 (2.7458)\tF1 0.749 (0.690)\n",
            "Validation: [0/325]\tBatch Time 0.289 (0.289)\tVB Loss 0.8651 (0.8651)\tF1 Score 0.941 (0.941)\t\n",
            "Validation: [100/325]\tBatch Time 0.095 (0.106)\tVB Loss 1.8623 (2.4679)\tF1 Score 0.850 (0.776)\t\n",
            "Validation: [200/325]\tBatch Time 0.121 (0.107)\tVB Loss 2.5879 (2.3900)\tF1 Score 0.776 (0.777)\t\n",
            "Validation: [300/325]\tBatch Time 0.154 (0.109)\tVB Loss 4.1753 (2.3756)\tF1 Score 0.694 (0.768)\t\n",
            "\n",
            " * LOSS - 2.352, F1 SCORE - 0.770\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000652\n",
            "\n",
            "Epoch: [26][0/1405]\tBatch Time 0.502 (0.502)\tData Load Time 0.236 (0.236)\tCE Loss 11.2868 (11.2868)\tVB Loss 2.8839 (2.8839)\tF1 0.474 (0.474)\n",
            "Epoch: [26][100/1405]\tBatch Time 0.210 (0.237)\tData Load Time 0.004 (0.007)\tCE Loss 11.9482 (11.5697)\tVB Loss 2.9553 (2.6256)\tF1 0.478 (0.712)\n",
            "Epoch: [26][200/1405]\tBatch Time 0.172 (0.225)\tData Load Time 0.004 (0.006)\tCE Loss 10.9666 (11.5582)\tVB Loss 1.7801 (2.7080)\tF1 0.915 (0.697)\n",
            "Epoch: [26][300/1405]\tBatch Time 0.163 (0.218)\tData Load Time 0.004 (0.005)\tCE Loss 11.1513 (11.5745)\tVB Loss 3.4300 (2.7578)\tF1 0.782 (0.693)\n",
            "Epoch: [26][400/1405]\tBatch Time 0.185 (0.215)\tData Load Time 0.006 (0.005)\tCE Loss 11.5472 (11.5787)\tVB Loss 1.7607 (2.7623)\tF1 0.677 (0.693)\n",
            "Epoch: [26][500/1405]\tBatch Time 0.223 (0.213)\tData Load Time 0.004 (0.005)\tCE Loss 11.5497 (11.5889)\tVB Loss 1.4775 (2.7260)\tF1 0.810 (0.695)\n",
            "Epoch: [26][600/1405]\tBatch Time 0.198 (0.211)\tData Load Time 0.005 (0.005)\tCE Loss 11.8123 (11.5912)\tVB Loss 3.1242 (2.7091)\tF1 0.517 (0.696)\n",
            "Epoch: [26][700/1405]\tBatch Time 0.233 (0.211)\tData Load Time 0.004 (0.005)\tCE Loss 11.9812 (11.5901)\tVB Loss 2.7968 (2.7307)\tF1 0.703 (0.694)\n",
            "Epoch: [26][800/1405]\tBatch Time 0.141 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 11.1807 (11.5843)\tVB Loss 1.2180 (2.7353)\tF1 0.760 (0.693)\n",
            "Epoch: [26][900/1405]\tBatch Time 0.163 (0.209)\tData Load Time 0.005 (0.005)\tCE Loss 10.9989 (11.5875)\tVB Loss 4.0869 (2.7355)\tF1 0.578 (0.692)\n",
            "Epoch: [26][1000/1405]\tBatch Time 0.177 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 11.3142 (11.5857)\tVB Loss 3.6903 (2.7260)\tF1 0.638 (0.694)\n",
            "Epoch: [26][1100/1405]\tBatch Time 0.266 (0.209)\tData Load Time 0.008 (0.005)\tCE Loss 12.1576 (11.5813)\tVB Loss 2.4056 (2.7236)\tF1 0.757 (0.695)\n",
            "Epoch: [26][1200/1405]\tBatch Time 0.191 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 10.7806 (11.5769)\tVB Loss 1.5922 (2.7026)\tF1 0.810 (0.696)\n",
            "Epoch: [26][1300/1405]\tBatch Time 0.275 (0.208)\tData Load Time 0.005 (0.005)\tCE Loss 11.5008 (11.5713)\tVB Loss 2.7164 (2.7103)\tF1 0.670 (0.696)\n",
            "Epoch: [26][1400/1405]\tBatch Time 0.173 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 11.2644 (11.5752)\tVB Loss 2.2709 (2.7151)\tF1 0.688 (0.695)\n",
            "Validation: [0/325]\tBatch Time 0.342 (0.342)\tVB Loss 2.2173 (2.2173)\tF1 Score 0.691 (0.691)\t\n",
            "Validation: [100/325]\tBatch Time 0.070 (0.102)\tVB Loss 0.9629 (2.4150)\tF1 Score 0.936 (0.765)\t\n",
            "Validation: [200/325]\tBatch Time 0.065 (0.103)\tVB Loss 0.6807 (2.4786)\tF1 Score 0.964 (0.766)\t\n",
            "Validation: [300/325]\tBatch Time 0.138 (0.107)\tVB Loss 2.3327 (2.4439)\tF1 Score 0.827 (0.767)\t\n",
            "\n",
            " * LOSS - 2.416, F1 SCORE - 0.767\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000638\n",
            "\n",
            "Epoch: [27][0/1405]\tBatch Time 0.499 (0.499)\tData Load Time 0.239 (0.239)\tCE Loss 11.3318 (11.3318)\tVB Loss 2.8564 (2.8564)\tF1 0.659 (0.659)\n",
            "Epoch: [27][100/1405]\tBatch Time 0.236 (0.232)\tData Load Time 0.004 (0.007)\tCE Loss 11.3117 (11.5855)\tVB Loss 1.7981 (2.5133)\tF1 0.699 (0.708)\n",
            "Epoch: [27][200/1405]\tBatch Time 0.187 (0.220)\tData Load Time 0.004 (0.006)\tCE Loss 11.5843 (11.5503)\tVB Loss 4.5198 (2.6279)\tF1 0.582 (0.711)\n",
            "Epoch: [27][300/1405]\tBatch Time 0.224 (0.217)\tData Load Time 0.004 (0.005)\tCE Loss 12.0900 (11.5689)\tVB Loss 3.8845 (2.6113)\tF1 0.605 (0.705)\n",
            "Epoch: [27][400/1405]\tBatch Time 0.284 (0.213)\tData Load Time 0.004 (0.005)\tCE Loss 11.7963 (11.5595)\tVB Loss 3.0902 (2.6586)\tF1 0.710 (0.698)\n",
            "Epoch: [27][500/1405]\tBatch Time 0.183 (0.211)\tData Load Time 0.005 (0.005)\tCE Loss 11.4736 (11.5553)\tVB Loss 1.7545 (2.6801)\tF1 0.812 (0.697)\n",
            "Epoch: [27][600/1405]\tBatch Time 0.146 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 11.7509 (11.5483)\tVB Loss 3.9887 (2.6934)\tF1 0.558 (0.697)\n",
            "Epoch: [27][700/1405]\tBatch Time 0.088 (0.207)\tData Load Time 0.004 (0.005)\tCE Loss 10.1753 (11.5536)\tVB Loss 0.9182 (2.6795)\tF1 0.895 (0.698)\n",
            "Epoch: [27][800/1405]\tBatch Time 0.238 (0.206)\tData Load Time 0.005 (0.005)\tCE Loss 11.2581 (11.5543)\tVB Loss 2.0211 (2.6648)\tF1 0.818 (0.696)\n",
            "Epoch: [27][900/1405]\tBatch Time 0.271 (0.206)\tData Load Time 0.005 (0.005)\tCE Loss 11.5820 (11.5612)\tVB Loss 2.2780 (2.6522)\tF1 0.644 (0.696)\n",
            "Epoch: [27][1000/1405]\tBatch Time 0.196 (0.206)\tData Load Time 0.005 (0.005)\tCE Loss 12.7617 (11.5610)\tVB Loss 1.8311 (2.6737)\tF1 0.636 (0.696)\n",
            "Epoch: [27][1100/1405]\tBatch Time 0.110 (0.207)\tData Load Time 0.004 (0.005)\tCE Loss 11.0675 (11.5665)\tVB Loss 1.7872 (2.6654)\tF1 0.875 (0.696)\n",
            "Epoch: [27][1200/1405]\tBatch Time 0.165 (0.207)\tData Load Time 0.005 (0.005)\tCE Loss 11.3139 (11.5584)\tVB Loss 0.8713 (2.6599)\tF1 0.904 (0.696)\n",
            "Epoch: [27][1300/1405]\tBatch Time 0.298 (0.207)\tData Load Time 0.004 (0.005)\tCE Loss 11.6514 (11.5602)\tVB Loss 4.3737 (2.6753)\tF1 0.553 (0.696)\n",
            "Epoch: [27][1400/1405]\tBatch Time 0.206 (0.207)\tData Load Time 0.004 (0.005)\tCE Loss 11.6033 (11.5630)\tVB Loss 2.1608 (2.6749)\tF1 0.757 (0.698)\n",
            "Validation: [0/325]\tBatch Time 0.327 (0.327)\tVB Loss 3.0430 (3.0430)\tF1 Score 0.831 (0.831)\t\n",
            "Validation: [100/325]\tBatch Time 0.078 (0.105)\tVB Loss 1.3559 (2.3901)\tF1 Score 0.741 (0.781)\t\n",
            "Validation: [200/325]\tBatch Time 0.092 (0.101)\tVB Loss 2.5570 (2.4519)\tF1 Score 0.613 (0.774)\t\n",
            "Validation: [300/325]\tBatch Time 0.125 (0.101)\tVB Loss 5.1903 (2.4043)\tF1 Score 0.581 (0.768)\t\n",
            "\n",
            " * LOSS - 2.397, F1 SCORE - 0.770\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000625\n",
            "\n",
            "Epoch: [28][0/1405]\tBatch Time 0.505 (0.505)\tData Load Time 0.249 (0.249)\tCE Loss 11.6235 (11.6235)\tVB Loss 4.5964 (4.5964)\tF1 0.492 (0.492)\n",
            "Epoch: [28][100/1405]\tBatch Time 0.218 (0.236)\tData Load Time 0.005 (0.007)\tCE Loss 11.4519 (11.5583)\tVB Loss 3.0641 (2.8186)\tF1 0.607 (0.693)\n",
            "Epoch: [28][200/1405]\tBatch Time 0.179 (0.226)\tData Load Time 0.005 (0.006)\tCE Loss 12.0066 (11.5194)\tVB Loss 3.9131 (2.8734)\tF1 0.669 (0.686)\n",
            "Epoch: [28][300/1405]\tBatch Time 0.251 (0.220)\tData Load Time 0.004 (0.006)\tCE Loss 11.1328 (11.5165)\tVB Loss 3.9750 (2.8273)\tF1 0.834 (0.693)\n",
            "Epoch: [28][400/1405]\tBatch Time 0.103 (0.216)\tData Load Time 0.004 (0.005)\tCE Loss 9.4079 (11.5247)\tVB Loss 1.2332 (2.7923)\tF1 0.730 (0.699)\n",
            "Epoch: [28][500/1405]\tBatch Time 0.204 (0.214)\tData Load Time 0.004 (0.005)\tCE Loss 11.7224 (11.5189)\tVB Loss 3.1485 (2.7719)\tF1 0.694 (0.697)\n",
            "Epoch: [28][600/1405]\tBatch Time 0.223 (0.211)\tData Load Time 0.006 (0.005)\tCE Loss 11.5302 (11.5393)\tVB Loss 2.7956 (2.7617)\tF1 0.585 (0.698)\n",
            "Epoch: [28][700/1405]\tBatch Time 0.225 (0.209)\tData Load Time 0.005 (0.005)\tCE Loss 12.2962 (11.5469)\tVB Loss 2.3906 (2.7371)\tF1 0.618 (0.700)\n",
            "Epoch: [28][800/1405]\tBatch Time 0.200 (0.207)\tData Load Time 0.004 (0.005)\tCE Loss 10.8108 (11.5480)\tVB Loss 1.0602 (2.7017)\tF1 0.903 (0.699)\n",
            "Epoch: [28][900/1405]\tBatch Time 0.207 (0.207)\tData Load Time 0.004 (0.005)\tCE Loss 11.7607 (11.5490)\tVB Loss 4.3311 (2.7032)\tF1 0.621 (0.700)\n",
            "Epoch: [28][1000/1405]\tBatch Time 0.197 (0.207)\tData Load Time 0.005 (0.005)\tCE Loss 12.3579 (11.5513)\tVB Loss 3.9917 (2.6838)\tF1 0.847 (0.701)\n",
            "Epoch: [28][1100/1405]\tBatch Time 0.217 (0.206)\tData Load Time 0.005 (0.005)\tCE Loss 11.1551 (11.5531)\tVB Loss 3.5363 (2.6688)\tF1 0.644 (0.703)\n",
            "Epoch: [28][1200/1405]\tBatch Time 0.231 (0.206)\tData Load Time 0.004 (0.005)\tCE Loss 11.9799 (11.5549)\tVB Loss 2.6793 (2.6895)\tF1 0.667 (0.701)\n",
            "Epoch: [28][1300/1405]\tBatch Time 0.239 (0.206)\tData Load Time 0.005 (0.005)\tCE Loss 10.9528 (11.5508)\tVB Loss 5.3621 (2.6895)\tF1 0.624 (0.701)\n",
            "Epoch: [28][1400/1405]\tBatch Time 0.224 (0.206)\tData Load Time 0.004 (0.005)\tCE Loss 11.7361 (11.5545)\tVB Loss 4.5882 (2.6774)\tF1 0.733 (0.702)\n",
            "Validation: [0/325]\tBatch Time 0.330 (0.330)\tVB Loss 1.0906 (1.0906)\tF1 Score 0.859 (0.859)\t\n",
            "Validation: [100/325]\tBatch Time 0.092 (0.102)\tVB Loss 2.8870 (2.3444)\tF1 Score 0.696 (0.764)\t\n",
            "Validation: [200/325]\tBatch Time 0.066 (0.100)\tVB Loss 0.8571 (2.2586)\tF1 Score 0.889 (0.785)\t\n",
            "Validation: [300/325]\tBatch Time 0.092 (0.099)\tVB Loss 2.7625 (2.2188)\tF1 Score 0.861 (0.785)\t\n",
            "\n",
            " * LOSS - 2.302, F1 SCORE - 0.779\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000612\n",
            "\n",
            "Epoch: [29][0/1405]\tBatch Time 0.552 (0.552)\tData Load Time 0.263 (0.263)\tCE Loss 11.7790 (11.7790)\tVB Loss 4.3603 (4.3603)\tF1 0.696 (0.696)\n",
            "Epoch: [29][100/1405]\tBatch Time 0.209 (0.226)\tData Load Time 0.005 (0.007)\tCE Loss 12.3823 (11.5358)\tVB Loss 1.8802 (2.6063)\tF1 0.733 (0.700)\n",
            "Epoch: [29][200/1405]\tBatch Time 0.174 (0.212)\tData Load Time 0.005 (0.006)\tCE Loss 11.8052 (11.5153)\tVB Loss 0.9941 (2.5816)\tF1 0.918 (0.705)\n",
            "Epoch: [29][300/1405]\tBatch Time 0.184 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 11.7266 (11.5260)\tVB Loss 2.6260 (2.5701)\tF1 0.549 (0.705)\n",
            "Epoch: [29][400/1405]\tBatch Time 0.151 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 11.3031 (11.5410)\tVB Loss 1.7597 (2.5509)\tF1 0.648 (0.704)\n",
            "Epoch: [29][500/1405]\tBatch Time 0.254 (0.208)\tData Load Time 0.005 (0.005)\tCE Loss 11.9269 (11.5455)\tVB Loss 3.0440 (2.5918)\tF1 0.568 (0.701)\n",
            "Epoch: [29][600/1405]\tBatch Time 0.213 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 11.6757 (11.5476)\tVB Loss 3.5397 (2.6131)\tF1 0.643 (0.700)\n",
            "Epoch: [29][700/1405]\tBatch Time 0.216 (0.207)\tData Load Time 0.004 (0.005)\tCE Loss 11.9561 (11.5586)\tVB Loss 5.7397 (2.6097)\tF1 0.599 (0.703)\n",
            "Epoch: [29][800/1405]\tBatch Time 0.236 (0.206)\tData Load Time 0.004 (0.005)\tCE Loss 11.6879 (11.5540)\tVB Loss 3.6439 (2.6095)\tF1 0.605 (0.704)\n",
            "Epoch: [29][900/1405]\tBatch Time 0.182 (0.205)\tData Load Time 0.004 (0.005)\tCE Loss 11.2346 (11.5588)\tVB Loss 2.3230 (2.6184)\tF1 0.838 (0.703)\n",
            "Epoch: [29][1000/1405]\tBatch Time 0.189 (0.205)\tData Load Time 0.004 (0.005)\tCE Loss 11.0174 (11.5612)\tVB Loss 1.5149 (2.6229)\tF1 0.742 (0.704)\n",
            "Epoch: [29][1100/1405]\tBatch Time 0.262 (0.205)\tData Load Time 0.004 (0.005)\tCE Loss 11.2078 (11.5614)\tVB Loss 4.4406 (2.6395)\tF1 0.582 (0.704)\n",
            "Epoch: [29][1200/1405]\tBatch Time 0.162 (0.205)\tData Load Time 0.004 (0.005)\tCE Loss 10.6943 (11.5580)\tVB Loss 2.1577 (2.6461)\tF1 0.697 (0.703)\n",
            "Epoch: [29][1300/1405]\tBatch Time 0.224 (0.205)\tData Load Time 0.004 (0.005)\tCE Loss 10.5389 (11.5522)\tVB Loss 2.8807 (2.6552)\tF1 0.514 (0.703)\n",
            "Epoch: [29][1400/1405]\tBatch Time 0.132 (0.204)\tData Load Time 0.005 (0.005)\tCE Loss 10.5292 (11.5448)\tVB Loss 2.0850 (2.6458)\tF1 0.334 (0.704)\n",
            "Validation: [0/325]\tBatch Time 0.305 (0.305)\tVB Loss 0.8703 (0.8703)\tF1 Score 0.863 (0.863)\t\n",
            "Validation: [100/325]\tBatch Time 0.083 (0.099)\tVB Loss 1.2724 (2.2980)\tF1 Score 0.743 (0.778)\t\n",
            "Validation: [200/325]\tBatch Time 0.130 (0.099)\tVB Loss 1.5620 (2.2935)\tF1 Score 0.668 (0.782)\t\n",
            "Validation: [300/325]\tBatch Time 0.135 (0.106)\tVB Loss 3.8202 (2.2779)\tF1 Score 0.580 (0.773)\t\n",
            "\n",
            " * LOSS - 2.267, F1 SCORE - 0.772\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000600\n",
            "\n",
            "Epoch: [30][0/1405]\tBatch Time 0.517 (0.517)\tData Load Time 0.224 (0.224)\tCE Loss 11.8942 (11.8942)\tVB Loss 1.8536 (1.8536)\tF1 0.759 (0.759)\n",
            "Epoch: [30][100/1405]\tBatch Time 0.229 (0.232)\tData Load Time 0.009 (0.008)\tCE Loss 11.3263 (11.5145)\tVB Loss 1.8807 (2.7227)\tF1 0.742 (0.696)\n",
            "Epoch: [30][200/1405]\tBatch Time 0.243 (0.223)\tData Load Time 0.004 (0.006)\tCE Loss 11.2826 (11.5289)\tVB Loss 2.1229 (2.6688)\tF1 0.555 (0.691)\n",
            "Epoch: [30][300/1405]\tBatch Time 0.210 (0.218)\tData Load Time 0.004 (0.006)\tCE Loss 12.1408 (11.5414)\tVB Loss 1.9359 (2.6343)\tF1 0.809 (0.693)\n",
            "Epoch: [30][400/1405]\tBatch Time 0.205 (0.214)\tData Load Time 0.004 (0.005)\tCE Loss 11.8386 (11.5315)\tVB Loss 2.5412 (2.6515)\tF1 0.674 (0.693)\n",
            "Epoch: [30][500/1405]\tBatch Time 0.202 (0.213)\tData Load Time 0.005 (0.005)\tCE Loss 12.3631 (11.5443)\tVB Loss 2.5794 (2.6474)\tF1 0.627 (0.694)\n",
            "Epoch: [30][600/1405]\tBatch Time 0.272 (0.212)\tData Load Time 0.004 (0.005)\tCE Loss 11.5202 (11.5437)\tVB Loss 4.8636 (2.6209)\tF1 0.534 (0.698)\n",
            "Epoch: [30][700/1405]\tBatch Time 0.171 (0.211)\tData Load Time 0.004 (0.005)\tCE Loss 11.8259 (11.5349)\tVB Loss 2.2457 (2.6116)\tF1 0.683 (0.700)\n",
            "Epoch: [30][800/1405]\tBatch Time 0.144 (0.209)\tData Load Time 0.005 (0.005)\tCE Loss 11.8219 (11.5407)\tVB Loss 1.6382 (2.6155)\tF1 0.738 (0.703)\n",
            "Epoch: [30][900/1405]\tBatch Time 0.236 (0.208)\tData Load Time 0.005 (0.005)\tCE Loss 11.2743 (11.5422)\tVB Loss 3.8837 (2.6087)\tF1 0.862 (0.704)\n",
            "Epoch: [30][1000/1405]\tBatch Time 0.155 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 11.3204 (11.5387)\tVB Loss 1.0471 (2.6104)\tF1 0.721 (0.704)\n",
            "Epoch: [30][1100/1405]\tBatch Time 0.250 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 11.4206 (11.5355)\tVB Loss 3.9348 (2.6162)\tF1 0.425 (0.703)\n",
            "Epoch: [30][1200/1405]\tBatch Time 0.224 (0.207)\tData Load Time 0.005 (0.005)\tCE Loss 11.9246 (11.5354)\tVB Loss 3.6056 (2.6152)\tF1 0.375 (0.705)\n",
            "Epoch: [30][1300/1405]\tBatch Time 0.250 (0.207)\tData Load Time 0.006 (0.005)\tCE Loss 11.7818 (11.5334)\tVB Loss 3.1150 (2.6126)\tF1 0.718 (0.705)\n",
            "Epoch: [30][1400/1405]\tBatch Time 0.219 (0.206)\tData Load Time 0.005 (0.005)\tCE Loss 12.0012 (11.5387)\tVB Loss 1.7619 (2.6181)\tF1 0.446 (0.704)\n",
            "Validation: [0/325]\tBatch Time 0.343 (0.343)\tVB Loss 3.6453 (3.6453)\tF1 Score 0.528 (0.528)\t\n",
            "Validation: [100/325]\tBatch Time 0.166 (0.111)\tVB Loss 8.9051 (2.3238)\tF1 Score 0.781 (0.785)\t\n",
            "Validation: [200/325]\tBatch Time 0.091 (0.113)\tVB Loss 1.5376 (2.2225)\tF1 Score 0.895 (0.793)\t\n",
            "Validation: [300/325]\tBatch Time 0.098 (0.114)\tVB Loss 1.6549 (2.2311)\tF1 Score 0.820 (0.783)\t\n",
            "\n",
            " * LOSS - 2.251, F1 SCORE - 0.782\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000588\n",
            "\n",
            "Epoch: [31][0/1405]\tBatch Time 0.502 (0.502)\tData Load Time 0.225 (0.225)\tCE Loss 11.1345 (11.1345)\tVB Loss 1.8463 (1.8463)\tF1 0.598 (0.598)\n",
            "Epoch: [31][100/1405]\tBatch Time 0.243 (0.234)\tData Load Time 0.004 (0.007)\tCE Loss 11.3888 (11.4808)\tVB Loss 2.9420 (2.4617)\tF1 0.625 (0.711)\n",
            "Epoch: [31][200/1405]\tBatch Time 0.091 (0.220)\tData Load Time 0.005 (0.006)\tCE Loss 11.5862 (11.4879)\tVB Loss 1.5960 (2.4696)\tF1 0.635 (0.706)\n",
            "Epoch: [31][300/1405]\tBatch Time 0.209 (0.215)\tData Load Time 0.004 (0.005)\tCE Loss 11.8058 (11.4825)\tVB Loss 3.7054 (2.4571)\tF1 0.695 (0.712)\n",
            "Epoch: [31][400/1405]\tBatch Time 0.141 (0.210)\tData Load Time 0.005 (0.005)\tCE Loss 10.0072 (11.4975)\tVB Loss 1.0980 (2.4775)\tF1 0.689 (0.713)\n",
            "Epoch: [31][500/1405]\tBatch Time 0.207 (0.211)\tData Load Time 0.004 (0.005)\tCE Loss 11.3360 (11.5228)\tVB Loss 4.5560 (2.5338)\tF1 0.537 (0.711)\n",
            "Epoch: [31][600/1405]\tBatch Time 0.214 (0.210)\tData Load Time 0.005 (0.005)\tCE Loss 10.4789 (11.5447)\tVB Loss 1.8235 (2.5541)\tF1 0.729 (0.709)\n",
            "Epoch: [31][700/1405]\tBatch Time 0.221 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 10.5616 (11.5408)\tVB Loss 3.8206 (2.5454)\tF1 0.664 (0.708)\n",
            "Epoch: [31][800/1405]\tBatch Time 0.216 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 11.6053 (11.5480)\tVB Loss 1.8125 (2.5736)\tF1 0.878 (0.708)\n",
            "Epoch: [31][900/1405]\tBatch Time 0.260 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 11.2635 (11.5437)\tVB Loss 3.3125 (2.5507)\tF1 0.699 (0.709)\n",
            "Epoch: [31][1000/1405]\tBatch Time 0.175 (0.209)\tData Load Time 0.005 (0.005)\tCE Loss 11.9745 (11.5431)\tVB Loss 1.5690 (2.5792)\tF1 0.861 (0.709)\n",
            "Epoch: [31][1100/1405]\tBatch Time 0.202 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 11.0912 (11.5461)\tVB Loss 2.6148 (2.5885)\tF1 0.783 (0.709)\n",
            "Epoch: [31][1200/1405]\tBatch Time 0.215 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 11.5064 (11.5380)\tVB Loss 3.8527 (2.6068)\tF1 0.476 (0.708)\n",
            "Epoch: [31][1300/1405]\tBatch Time 0.194 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 11.7036 (11.5293)\tVB Loss 3.3517 (2.5922)\tF1 0.644 (0.709)\n",
            "Epoch: [31][1400/1405]\tBatch Time 0.228 (0.208)\tData Load Time 0.005 (0.005)\tCE Loss 11.7580 (11.5260)\tVB Loss 1.7387 (2.5887)\tF1 0.817 (0.710)\n",
            "Validation: [0/325]\tBatch Time 0.319 (0.319)\tVB Loss 1.0052 (1.0052)\tF1 Score 0.839 (0.839)\t\n",
            "Validation: [100/325]\tBatch Time 0.102 (0.103)\tVB Loss 1.7847 (2.4310)\tF1 Score 0.933 (0.776)\t\n",
            "Validation: [200/325]\tBatch Time 0.113 (0.100)\tVB Loss 1.8457 (2.2634)\tF1 Score 0.832 (0.778)\t\n",
            "Validation: [300/325]\tBatch Time 0.087 (0.100)\tVB Loss 5.4485 (2.2740)\tF1 Score 0.688 (0.778)\t\n",
            "\n",
            " * LOSS - 2.264, F1 SCORE - 0.776\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000577\n",
            "\n",
            "Epoch: [32][0/1405]\tBatch Time 0.553 (0.553)\tData Load Time 0.237 (0.237)\tCE Loss 10.9720 (10.9720)\tVB Loss 2.4184 (2.4184)\tF1 0.821 (0.821)\n",
            "Epoch: [32][100/1405]\tBatch Time 0.182 (0.231)\tData Load Time 0.004 (0.007)\tCE Loss 11.2929 (11.4939)\tVB Loss 0.5820 (2.5325)\tF1 0.981 (0.713)\n",
            "Epoch: [32][200/1405]\tBatch Time 0.278 (0.222)\tData Load Time 0.005 (0.006)\tCE Loss 11.6578 (11.4785)\tVB Loss 4.0928 (2.5358)\tF1 0.609 (0.722)\n",
            "Epoch: [32][300/1405]\tBatch Time 0.204 (0.217)\tData Load Time 0.006 (0.006)\tCE Loss 11.7153 (11.5220)\tVB Loss 2.6226 (2.5390)\tF1 0.750 (0.721)\n",
            "Epoch: [32][400/1405]\tBatch Time 0.201 (0.214)\tData Load Time 0.005 (0.005)\tCE Loss 11.5116 (11.5209)\tVB Loss 1.2647 (2.5734)\tF1 0.849 (0.723)\n",
            "Epoch: [32][500/1405]\tBatch Time 0.208 (0.212)\tData Load Time 0.006 (0.005)\tCE Loss 12.5027 (11.5207)\tVB Loss 2.4143 (2.5789)\tF1 0.829 (0.724)\n",
            "Epoch: [32][600/1405]\tBatch Time 0.160 (0.211)\tData Load Time 0.005 (0.005)\tCE Loss 11.3749 (11.5320)\tVB Loss 1.7720 (2.5584)\tF1 0.559 (0.724)\n",
            "Epoch: [32][700/1405]\tBatch Time 0.227 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 11.7960 (11.5297)\tVB Loss 3.6657 (2.5633)\tF1 0.724 (0.720)\n",
            "Epoch: [32][800/1405]\tBatch Time 0.214 (0.210)\tData Load Time 0.005 (0.005)\tCE Loss 11.4927 (11.5232)\tVB Loss 3.4783 (2.5699)\tF1 0.417 (0.719)\n",
            "Epoch: [32][900/1405]\tBatch Time 0.228 (0.210)\tData Load Time 0.005 (0.005)\tCE Loss 11.4238 (11.5213)\tVB Loss 6.3345 (2.5512)\tF1 0.489 (0.717)\n",
            "Epoch: [32][1000/1405]\tBatch Time 0.155 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 11.0393 (11.5118)\tVB Loss 0.9732 (2.5501)\tF1 0.843 (0.717)\n",
            "Epoch: [32][1100/1405]\tBatch Time 0.157 (0.209)\tData Load Time 0.005 (0.005)\tCE Loss 11.4967 (11.5092)\tVB Loss 2.1083 (2.5606)\tF1 0.682 (0.714)\n",
            "Epoch: [32][1200/1405]\tBatch Time 0.309 (0.208)\tData Load Time 0.005 (0.005)\tCE Loss 10.7082 (11.5126)\tVB Loss 2.6152 (2.5479)\tF1 0.692 (0.715)\n",
            "Epoch: [32][1300/1405]\tBatch Time 0.199 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 11.1493 (11.5159)\tVB Loss 2.4121 (2.5528)\tF1 0.829 (0.716)\n",
            "Epoch: [32][1400/1405]\tBatch Time 0.133 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 10.1244 (11.5200)\tVB Loss 1.3485 (2.5539)\tF1 0.687 (0.714)\n",
            "Validation: [0/325]\tBatch Time 0.345 (0.345)\tVB Loss 2.9185 (2.9185)\tF1 Score 0.907 (0.907)\t\n",
            "Validation: [100/325]\tBatch Time 0.124 (0.111)\tVB Loss 3.1281 (2.2850)\tF1 Score 0.743 (0.776)\t\n",
            "Validation: [200/325]\tBatch Time 0.097 (0.116)\tVB Loss 3.3442 (2.2562)\tF1 Score 0.622 (0.778)\t\n",
            "Validation: [300/325]\tBatch Time 0.130 (0.115)\tVB Loss 2.1074 (2.2695)\tF1 Score 0.854 (0.783)\t\n",
            "\n",
            " * LOSS - 2.230, F1 SCORE - 0.785\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000566\n",
            "\n",
            "Epoch: [33][0/1405]\tBatch Time 0.448 (0.448)\tData Load Time 0.241 (0.241)\tCE Loss 12.1441 (12.1441)\tVB Loss 3.2651 (3.2651)\tF1 0.680 (0.680)\n",
            "Epoch: [33][100/1405]\tBatch Time 0.279 (0.228)\tData Load Time 0.005 (0.008)\tCE Loss 11.3890 (11.4434)\tVB Loss 4.5270 (2.4925)\tF1 0.699 (0.703)\n",
            "Epoch: [33][200/1405]\tBatch Time 0.222 (0.222)\tData Load Time 0.006 (0.006)\tCE Loss 11.4964 (11.4078)\tVB Loss 1.0955 (2.5036)\tF1 0.949 (0.707)\n",
            "Epoch: [33][300/1405]\tBatch Time 0.187 (0.217)\tData Load Time 0.004 (0.006)\tCE Loss 10.5739 (11.4533)\tVB Loss 2.0911 (2.5042)\tF1 0.720 (0.710)\n",
            "Epoch: [33][400/1405]\tBatch Time 0.245 (0.214)\tData Load Time 0.004 (0.005)\tCE Loss 11.6320 (11.4940)\tVB Loss 2.7696 (2.5253)\tF1 0.441 (0.709)\n",
            "Epoch: [33][500/1405]\tBatch Time 0.173 (0.213)\tData Load Time 0.004 (0.005)\tCE Loss 11.0901 (11.4997)\tVB Loss 2.5615 (2.5789)\tF1 0.667 (0.707)\n",
            "Epoch: [33][600/1405]\tBatch Time 0.211 (0.211)\tData Load Time 0.005 (0.005)\tCE Loss 11.9443 (11.5099)\tVB Loss 0.9519 (2.5478)\tF1 0.962 (0.710)\n",
            "Epoch: [33][700/1405]\tBatch Time 0.214 (0.211)\tData Load Time 0.006 (0.005)\tCE Loss 9.7970 (11.5155)\tVB Loss 2.3162 (2.5483)\tF1 0.606 (0.710)\n",
            "Epoch: [33][800/1405]\tBatch Time 0.186 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 11.8167 (11.5090)\tVB Loss 1.4067 (2.5473)\tF1 0.932 (0.709)\n",
            "Epoch: [33][900/1405]\tBatch Time 0.200 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 11.8804 (11.5218)\tVB Loss 3.9039 (2.5367)\tF1 0.591 (0.712)\n",
            "Epoch: [33][1000/1405]\tBatch Time 0.174 (0.209)\tData Load Time 0.005 (0.005)\tCE Loss 11.1561 (11.5276)\tVB Loss 2.7968 (2.5542)\tF1 0.766 (0.711)\n",
            "Epoch: [33][1100/1405]\tBatch Time 0.216 (0.208)\tData Load Time 0.005 (0.005)\tCE Loss 11.7723 (11.5252)\tVB Loss 3.2796 (2.5569)\tF1 0.692 (0.712)\n",
            "Epoch: [33][1200/1405]\tBatch Time 0.162 (0.208)\tData Load Time 0.006 (0.005)\tCE Loss 11.7888 (11.5151)\tVB Loss 1.6457 (2.5573)\tF1 0.813 (0.712)\n",
            "Epoch: [33][1300/1405]\tBatch Time 0.112 (0.207)\tData Load Time 0.005 (0.005)\tCE Loss 11.6931 (11.5188)\tVB Loss 1.0057 (2.5622)\tF1 0.833 (0.711)\n",
            "Epoch: [33][1400/1405]\tBatch Time 0.173 (0.207)\tData Load Time 0.004 (0.005)\tCE Loss 11.3062 (11.5141)\tVB Loss 1.3354 (2.5615)\tF1 0.732 (0.713)\n",
            "Validation: [0/325]\tBatch Time 0.311 (0.311)\tVB Loss 3.8433 (3.8433)\tF1 Score 0.741 (0.741)\t\n",
            "Validation: [100/325]\tBatch Time 0.109 (0.112)\tVB Loss 2.1579 (2.3877)\tF1 Score 0.789 (0.773)\t\n",
            "Validation: [200/325]\tBatch Time 0.103 (0.113)\tVB Loss 3.5177 (2.3137)\tF1 Score 0.817 (0.783)\t\n",
            "Validation: [300/325]\tBatch Time 0.121 (0.114)\tVB Loss 2.5401 (2.2611)\tF1 Score 0.681 (0.780)\t\n",
            "\n",
            " * LOSS - 2.250, F1 SCORE - 0.782\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000556\n",
            "\n",
            "Epoch: [34][0/1405]\tBatch Time 0.484 (0.484)\tData Load Time 0.246 (0.246)\tCE Loss 11.5847 (11.5847)\tVB Loss 2.7091 (2.7091)\tF1 0.794 (0.794)\n",
            "Epoch: [34][100/1405]\tBatch Time 0.174 (0.231)\tData Load Time 0.006 (0.008)\tCE Loss 11.1217 (11.5990)\tVB Loss 1.2738 (2.5305)\tF1 0.647 (0.712)\n",
            "Epoch: [34][200/1405]\tBatch Time 0.189 (0.224)\tData Load Time 0.005 (0.006)\tCE Loss 11.0313 (11.5474)\tVB Loss 3.0709 (2.4591)\tF1 0.790 (0.712)\n",
            "Epoch: [34][300/1405]\tBatch Time 0.224 (0.220)\tData Load Time 0.004 (0.006)\tCE Loss 11.4922 (11.5468)\tVB Loss 3.1149 (2.5412)\tF1 0.633 (0.711)\n",
            "Epoch: [34][400/1405]\tBatch Time 0.246 (0.216)\tData Load Time 0.004 (0.006)\tCE Loss 11.4735 (11.5207)\tVB Loss 3.8289 (2.5263)\tF1 0.625 (0.712)\n",
            "Epoch: [34][500/1405]\tBatch Time 0.106 (0.214)\tData Load Time 0.004 (0.005)\tCE Loss 9.4362 (11.5110)\tVB Loss 1.1686 (2.4949)\tF1 0.701 (0.711)\n",
            "Epoch: [34][600/1405]\tBatch Time 0.165 (0.213)\tData Load Time 0.005 (0.005)\tCE Loss 9.5100 (11.5170)\tVB Loss 2.9901 (2.5253)\tF1 0.737 (0.711)\n",
            "Epoch: [34][700/1405]\tBatch Time 0.226 (0.213)\tData Load Time 0.005 (0.005)\tCE Loss 11.4832 (11.5036)\tVB Loss 3.1054 (2.5286)\tF1 0.726 (0.712)\n",
            "Epoch: [34][800/1405]\tBatch Time 0.218 (0.211)\tData Load Time 0.004 (0.005)\tCE Loss 10.9491 (11.4977)\tVB Loss 1.4257 (2.5243)\tF1 0.556 (0.712)\n",
            "Epoch: [34][900/1405]\tBatch Time 0.200 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 12.2777 (11.4996)\tVB Loss 1.2160 (2.5312)\tF1 0.838 (0.713)\n",
            "Epoch: [34][1000/1405]\tBatch Time 0.159 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 10.9059 (11.4879)\tVB Loss 0.6880 (2.5428)\tF1 0.879 (0.712)\n",
            "Epoch: [34][1100/1405]\tBatch Time 0.222 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 10.9011 (11.5001)\tVB Loss 3.0626 (2.5358)\tF1 0.593 (0.713)\n",
            "Epoch: [34][1200/1405]\tBatch Time 0.143 (0.209)\tData Load Time 0.005 (0.005)\tCE Loss 11.0704 (11.5028)\tVB Loss 2.4112 (2.5400)\tF1 0.597 (0.713)\n",
            "Epoch: [34][1300/1405]\tBatch Time 0.183 (0.209)\tData Load Time 0.005 (0.005)\tCE Loss 12.3426 (11.5006)\tVB Loss 2.7295 (2.5315)\tF1 0.665 (0.714)\n",
            "Epoch: [34][1400/1405]\tBatch Time 0.212 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 10.9900 (11.5027)\tVB Loss 5.0662 (2.5355)\tF1 0.676 (0.716)\n",
            "Validation: [0/325]\tBatch Time 0.323 (0.323)\tVB Loss 1.1032 (1.1032)\tF1 Score 0.879 (0.879)\t\n",
            "Validation: [100/325]\tBatch Time 0.090 (0.107)\tVB Loss 2.2141 (2.2713)\tF1 Score 0.591 (0.784)\t\n",
            "Validation: [200/325]\tBatch Time 0.078 (0.104)\tVB Loss 2.0925 (2.3636)\tF1 Score 0.767 (0.775)\t\n",
            "Validation: [300/325]\tBatch Time 0.113 (0.104)\tVB Loss 2.4712 (2.2742)\tF1 Score 0.774 (0.779)\t\n",
            "\n",
            " * LOSS - 2.262, F1 SCORE - 0.780\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000545\n",
            "\n",
            "Epoch: [35][0/1405]\tBatch Time 0.468 (0.468)\tData Load Time 0.261 (0.261)\tCE Loss 11.5315 (11.5315)\tVB Loss 1.6039 (1.6039)\tF1 0.763 (0.763)\n",
            "Epoch: [35][100/1405]\tBatch Time 0.194 (0.225)\tData Load Time 0.004 (0.008)\tCE Loss 12.3861 (11.4832)\tVB Loss 1.9105 (2.5324)\tF1 0.886 (0.706)\n",
            "Epoch: [35][200/1405]\tBatch Time 0.201 (0.215)\tData Load Time 0.005 (0.006)\tCE Loss 10.7566 (11.4653)\tVB Loss 1.8320 (2.5631)\tF1 0.894 (0.716)\n",
            "Epoch: [35][300/1405]\tBatch Time 0.207 (0.212)\tData Load Time 0.004 (0.006)\tCE Loss 11.6979 (11.4770)\tVB Loss 2.8985 (2.5178)\tF1 0.695 (0.718)\n",
            "Epoch: [35][400/1405]\tBatch Time 0.172 (0.208)\tData Load Time 0.005 (0.005)\tCE Loss 11.7055 (11.4753)\tVB Loss 1.5599 (2.4737)\tF1 0.859 (0.722)\n",
            "Epoch: [35][500/1405]\tBatch Time 0.273 (0.208)\tData Load Time 0.005 (0.005)\tCE Loss 11.2898 (11.4934)\tVB Loss 2.2423 (2.5093)\tF1 0.650 (0.719)\n",
            "Epoch: [35][600/1405]\tBatch Time 0.257 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 11.5926 (11.4851)\tVB Loss 3.6264 (2.5430)\tF1 0.764 (0.717)\n",
            "Epoch: [35][700/1405]\tBatch Time 0.197 (0.207)\tData Load Time 0.005 (0.005)\tCE Loss 11.8632 (11.4875)\tVB Loss 1.5239 (2.5367)\tF1 0.877 (0.720)\n",
            "Epoch: [35][800/1405]\tBatch Time 0.190 (0.207)\tData Load Time 0.005 (0.005)\tCE Loss 10.6945 (11.4843)\tVB Loss 3.2305 (2.5297)\tF1 0.673 (0.722)\n",
            "Epoch: [35][900/1405]\tBatch Time 0.269 (0.206)\tData Load Time 0.004 (0.005)\tCE Loss 11.5269 (11.4806)\tVB Loss 4.5122 (2.5261)\tF1 0.690 (0.722)\n",
            "Epoch: [35][1000/1405]\tBatch Time 0.137 (0.206)\tData Load Time 0.004 (0.005)\tCE Loss 11.5206 (11.4869)\tVB Loss 1.2220 (2.5251)\tF1 0.694 (0.720)\n",
            "Epoch: [35][1100/1405]\tBatch Time 0.163 (0.205)\tData Load Time 0.005 (0.005)\tCE Loss 11.1625 (11.4819)\tVB Loss 0.8380 (2.5147)\tF1 0.897 (0.720)\n",
            "Epoch: [35][1200/1405]\tBatch Time 0.165 (0.205)\tData Load Time 0.004 (0.005)\tCE Loss 11.3167 (11.4831)\tVB Loss 1.2257 (2.5199)\tF1 0.802 (0.720)\n",
            "Epoch: [35][1300/1405]\tBatch Time 0.200 (0.206)\tData Load Time 0.005 (0.005)\tCE Loss 10.9840 (11.4916)\tVB Loss 3.7890 (2.5385)\tF1 0.535 (0.719)\n",
            "Epoch: [35][1400/1405]\tBatch Time 0.210 (0.205)\tData Load Time 0.004 (0.005)\tCE Loss 12.0842 (11.4950)\tVB Loss 1.4246 (2.5340)\tF1 0.528 (0.719)\n",
            "Validation: [0/325]\tBatch Time 0.299 (0.299)\tVB Loss 2.3217 (2.3217)\tF1 Score 0.750 (0.750)\t\n",
            "Validation: [100/325]\tBatch Time 0.110 (0.106)\tVB Loss 2.5692 (2.0694)\tF1 Score 0.728 (0.793)\t\n",
            "Validation: [200/325]\tBatch Time 0.112 (0.103)\tVB Loss 1.8623 (2.0123)\tF1 Score 0.881 (0.792)\t\n",
            "Validation: [300/325]\tBatch Time 0.097 (0.108)\tVB Loss 2.2748 (2.1783)\tF1 Score 0.815 (0.789)\t\n",
            "\n",
            " * LOSS - 2.207, F1 SCORE - 0.785\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000536\n",
            "\n",
            "Epoch: [36][0/1405]\tBatch Time 0.491 (0.491)\tData Load Time 0.242 (0.242)\tCE Loss 11.9134 (11.9134)\tVB Loss 2.6829 (2.6829)\tF1 0.564 (0.564)\n",
            "Epoch: [36][100/1405]\tBatch Time 0.248 (0.234)\tData Load Time 0.005 (0.008)\tCE Loss 11.9716 (11.5940)\tVB Loss 2.4263 (2.4086)\tF1 0.804 (0.733)\n",
            "Epoch: [36][200/1405]\tBatch Time 0.294 (0.221)\tData Load Time 0.004 (0.006)\tCE Loss 11.8171 (11.5552)\tVB Loss 3.7250 (2.4456)\tF1 0.703 (0.728)\n",
            "Epoch: [36][300/1405]\tBatch Time 0.286 (0.213)\tData Load Time 0.004 (0.006)\tCE Loss 10.9866 (11.5486)\tVB Loss 2.9260 (2.4531)\tF1 0.874 (0.726)\n",
            "Epoch: [36][400/1405]\tBatch Time 0.230 (0.212)\tData Load Time 0.004 (0.005)\tCE Loss 11.9818 (11.5468)\tVB Loss 3.3420 (2.4820)\tF1 0.554 (0.728)\n",
            "Epoch: [36][500/1405]\tBatch Time 0.230 (0.212)\tData Load Time 0.005 (0.005)\tCE Loss 11.2982 (11.5363)\tVB Loss 1.6076 (2.4995)\tF1 0.897 (0.727)\n",
            "Epoch: [36][600/1405]\tBatch Time 0.185 (0.211)\tData Load Time 0.004 (0.005)\tCE Loss 10.9433 (11.5337)\tVB Loss 2.7646 (2.5319)\tF1 0.833 (0.724)\n",
            "Epoch: [36][700/1405]\tBatch Time 0.242 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 10.2757 (11.5179)\tVB Loss 2.7102 (2.5103)\tF1 0.758 (0.724)\n",
            "Epoch: [36][800/1405]\tBatch Time 0.216 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 11.3337 (11.5066)\tVB Loss 1.8661 (2.4906)\tF1 0.797 (0.723)\n",
            "Epoch: [36][900/1405]\tBatch Time 0.217 (0.208)\tData Load Time 0.005 (0.005)\tCE Loss 10.9800 (11.4951)\tVB Loss 1.8254 (2.4807)\tF1 0.769 (0.722)\n",
            "Epoch: [36][1000/1405]\tBatch Time 0.134 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 10.9126 (11.4897)\tVB Loss 1.1156 (2.4578)\tF1 0.729 (0.723)\n",
            "Epoch: [36][1100/1405]\tBatch Time 0.227 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 11.8368 (11.4955)\tVB Loss 2.3660 (2.4649)\tF1 0.518 (0.722)\n",
            "Epoch: [36][1200/1405]\tBatch Time 0.214 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 11.7958 (11.4939)\tVB Loss 1.9907 (2.4694)\tF1 0.612 (0.723)\n",
            "Epoch: [36][1300/1405]\tBatch Time 0.215 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 10.9962 (11.4870)\tVB Loss 2.6421 (2.4831)\tF1 0.672 (0.722)\n",
            "Epoch: [36][1400/1405]\tBatch Time 0.264 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 11.7257 (11.4843)\tVB Loss 0.4034 (2.4704)\tF1 0.667 (0.722)\n",
            "Validation: [0/325]\tBatch Time 0.336 (0.336)\tVB Loss 1.9567 (1.9567)\tF1 Score 0.810 (0.810)\t\n",
            "Validation: [100/325]\tBatch Time 0.107 (0.106)\tVB Loss 3.3667 (2.1078)\tF1 Score 0.693 (0.792)\t\n",
            "Validation: [200/325]\tBatch Time 0.083 (0.112)\tVB Loss 3.0523 (2.2397)\tF1 Score 0.715 (0.788)\t\n",
            "Validation: [300/325]\tBatch Time 0.077 (0.114)\tVB Loss 0.8514 (2.2074)\tF1 Score 0.868 (0.785)\t\n",
            "\n",
            " * LOSS - 2.211, F1 SCORE - 0.782\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000526\n",
            "\n",
            "Epoch: [37][0/1405]\tBatch Time 0.489 (0.489)\tData Load Time 0.244 (0.244)\tCE Loss 11.7008 (11.7008)\tVB Loss 2.4187 (2.4187)\tF1 0.541 (0.541)\n",
            "Epoch: [37][100/1405]\tBatch Time 0.250 (0.234)\tData Load Time 0.006 (0.008)\tCE Loss 11.7217 (11.5116)\tVB Loss 3.2669 (2.6359)\tF1 0.817 (0.698)\n",
            "Epoch: [37][200/1405]\tBatch Time 0.210 (0.220)\tData Load Time 0.005 (0.006)\tCE Loss 11.2349 (11.4798)\tVB Loss 1.0947 (2.4616)\tF1 0.859 (0.717)\n",
            "Epoch: [37][300/1405]\tBatch Time 0.152 (0.215)\tData Load Time 0.005 (0.006)\tCE Loss 11.1465 (11.4775)\tVB Loss 2.1894 (2.4062)\tF1 0.723 (0.723)\n",
            "Epoch: [37][400/1405]\tBatch Time 0.212 (0.211)\tData Load Time 0.004 (0.005)\tCE Loss 11.3659 (11.4598)\tVB Loss 2.1468 (2.4329)\tF1 0.736 (0.719)\n",
            "Epoch: [37][500/1405]\tBatch Time 0.152 (0.211)\tData Load Time 0.005 (0.005)\tCE Loss 10.7664 (11.4767)\tVB Loss 0.9107 (2.4403)\tF1 0.949 (0.719)\n",
            "Epoch: [37][600/1405]\tBatch Time 0.254 (0.211)\tData Load Time 0.005 (0.005)\tCE Loss 11.8749 (11.4931)\tVB Loss 4.7123 (2.4679)\tF1 0.483 (0.716)\n",
            "Epoch: [37][700/1405]\tBatch Time 0.286 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 12.1102 (11.4864)\tVB Loss 4.0785 (2.4644)\tF1 0.641 (0.716)\n",
            "Epoch: [37][800/1405]\tBatch Time 0.184 (0.209)\tData Load Time 0.005 (0.005)\tCE Loss 11.4782 (11.4849)\tVB Loss 2.6253 (2.4499)\tF1 0.756 (0.716)\n",
            "Epoch: [37][900/1405]\tBatch Time 0.245 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 12.0329 (11.4808)\tVB Loss 3.7776 (2.4592)\tF1 0.598 (0.715)\n",
            "Epoch: [37][1000/1405]\tBatch Time 0.094 (0.209)\tData Load Time 0.005 (0.005)\tCE Loss 9.7984 (11.4819)\tVB Loss 1.0411 (2.4732)\tF1 0.681 (0.716)\n",
            "Epoch: [37][1100/1405]\tBatch Time 0.183 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 11.1243 (11.4801)\tVB Loss 1.1775 (2.4596)\tF1 0.851 (0.718)\n",
            "Epoch: [37][1200/1405]\tBatch Time 0.285 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 10.5252 (11.4790)\tVB Loss 2.4250 (2.4628)\tF1 0.664 (0.717)\n",
            "Epoch: [37][1300/1405]\tBatch Time 0.142 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 11.5552 (11.4773)\tVB Loss 2.3013 (2.4594)\tF1 0.595 (0.718)\n",
            "Epoch: [37][1400/1405]\tBatch Time 0.210 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 12.6620 (11.4828)\tVB Loss 2.5402 (2.4621)\tF1 0.733 (0.719)\n",
            "Validation: [0/325]\tBatch Time 0.343 (0.343)\tVB Loss 1.9455 (1.9455)\tF1 Score 0.906 (0.906)\t\n",
            "Validation: [100/325]\tBatch Time 0.144 (0.113)\tVB Loss 3.1464 (2.1107)\tF1 Score 0.879 (0.808)\t\n",
            "Validation: [200/325]\tBatch Time 0.072 (0.111)\tVB Loss 2.1118 (2.1148)\tF1 Score 0.844 (0.797)\t\n",
            "Validation: [300/325]\tBatch Time 0.084 (0.113)\tVB Loss 0.7262 (2.1296)\tF1 Score 0.871 (0.788)\t\n",
            "\n",
            " * LOSS - 2.165, F1 SCORE - 0.788\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000517\n",
            "\n",
            "Epoch: [38][0/1405]\tBatch Time 0.511 (0.511)\tData Load Time 0.233 (0.233)\tCE Loss 12.2166 (12.2166)\tVB Loss 4.2177 (4.2177)\tF1 0.710 (0.710)\n",
            "Epoch: [38][100/1405]\tBatch Time 0.213 (0.231)\tData Load Time 0.004 (0.008)\tCE Loss 10.6986 (11.4254)\tVB Loss 3.0641 (2.4793)\tF1 0.758 (0.702)\n",
            "Epoch: [38][200/1405]\tBatch Time 0.218 (0.217)\tData Load Time 0.004 (0.006)\tCE Loss 11.8554 (11.4192)\tVB Loss 3.6197 (2.4657)\tF1 0.620 (0.706)\n",
            "Epoch: [38][300/1405]\tBatch Time 0.238 (0.212)\tData Load Time 0.004 (0.005)\tCE Loss 11.2325 (11.4411)\tVB Loss 6.3683 (2.4048)\tF1 0.472 (0.716)\n",
            "Epoch: [38][400/1405]\tBatch Time 0.133 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 11.4615 (11.4701)\tVB Loss 2.8059 (2.3935)\tF1 0.725 (0.722)\n",
            "Epoch: [38][500/1405]\tBatch Time 0.204 (0.209)\tData Load Time 0.005 (0.005)\tCE Loss 11.4072 (11.4824)\tVB Loss 1.3167 (2.4309)\tF1 0.691 (0.720)\n",
            "Epoch: [38][600/1405]\tBatch Time 0.241 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 12.0858 (11.4885)\tVB Loss 3.1838 (2.4947)\tF1 0.671 (0.719)\n",
            "Epoch: [38][700/1405]\tBatch Time 0.243 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 11.6988 (11.4824)\tVB Loss 5.1578 (2.4927)\tF1 0.673 (0.720)\n",
            "Epoch: [38][800/1405]\tBatch Time 0.139 (0.207)\tData Load Time 0.004 (0.005)\tCE Loss 11.0131 (11.4806)\tVB Loss 2.0128 (2.4839)\tF1 0.695 (0.719)\n",
            "Epoch: [38][900/1405]\tBatch Time 0.201 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 12.0862 (11.4784)\tVB Loss 2.8546 (2.4855)\tF1 0.731 (0.719)\n",
            "Epoch: [38][1000/1405]\tBatch Time 0.167 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 10.8908 (11.4787)\tVB Loss 3.6873 (2.5009)\tF1 0.662 (0.719)\n",
            "Epoch: [38][1100/1405]\tBatch Time 0.209 (0.208)\tData Load Time 0.005 (0.005)\tCE Loss 12.1713 (11.4763)\tVB Loss 3.8964 (2.4883)\tF1 0.644 (0.722)\n",
            "Epoch: [38][1200/1405]\tBatch Time 0.245 (0.208)\tData Load Time 0.005 (0.005)\tCE Loss 12.3727 (11.4747)\tVB Loss 2.9806 (2.4885)\tF1 0.680 (0.723)\n",
            "Epoch: [38][1300/1405]\tBatch Time 0.191 (0.207)\tData Load Time 0.004 (0.005)\tCE Loss 11.8499 (11.4758)\tVB Loss 2.4188 (2.4861)\tF1 0.558 (0.722)\n",
            "Epoch: [38][1400/1405]\tBatch Time 0.165 (0.207)\tData Load Time 0.004 (0.005)\tCE Loss 10.5930 (11.4746)\tVB Loss 1.0767 (2.4696)\tF1 0.668 (0.725)\n",
            "Validation: [0/325]\tBatch Time 0.325 (0.325)\tVB Loss 2.6944 (2.6944)\tF1 Score 0.707 (0.707)\t\n",
            "Validation: [100/325]\tBatch Time 0.100 (0.100)\tVB Loss 2.8944 (2.2677)\tF1 Score 0.727 (0.780)\t\n",
            "Validation: [200/325]\tBatch Time 0.116 (0.105)\tVB Loss 1.1842 (2.1809)\tF1 Score 0.804 (0.794)\t\n",
            "Validation: [300/325]\tBatch Time 0.119 (0.108)\tVB Loss 4.4507 (2.1961)\tF1 Score 0.744 (0.786)\t\n",
            "\n",
            " * LOSS - 2.216, F1 SCORE - 0.783\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000508\n",
            "\n",
            "Epoch: [39][0/1405]\tBatch Time 0.519 (0.519)\tData Load Time 0.242 (0.242)\tCE Loss 11.8854 (11.8854)\tVB Loss 3.4316 (3.4316)\tF1 0.644 (0.644)\n",
            "Epoch: [39][100/1405]\tBatch Time 0.173 (0.234)\tData Load Time 0.005 (0.008)\tCE Loss 11.2013 (11.5093)\tVB Loss 1.8526 (2.2994)\tF1 0.820 (0.730)\n",
            "Epoch: [39][200/1405]\tBatch Time 0.180 (0.225)\tData Load Time 0.004 (0.006)\tCE Loss 12.1121 (11.5291)\tVB Loss 2.7265 (2.3930)\tF1 0.690 (0.734)\n",
            "Epoch: [39][300/1405]\tBatch Time 0.240 (0.221)\tData Load Time 0.004 (0.006)\tCE Loss 11.5150 (11.5371)\tVB Loss 2.6688 (2.4273)\tF1 0.576 (0.730)\n",
            "Epoch: [39][400/1405]\tBatch Time 0.268 (0.219)\tData Load Time 0.004 (0.005)\tCE Loss 10.7959 (11.5102)\tVB Loss 2.5541 (2.4279)\tF1 0.825 (0.730)\n",
            "Epoch: [39][500/1405]\tBatch Time 0.136 (0.216)\tData Load Time 0.004 (0.005)\tCE Loss 11.8762 (11.4984)\tVB Loss 1.2198 (2.4079)\tF1 0.766 (0.729)\n",
            "Epoch: [39][600/1405]\tBatch Time 0.196 (0.214)\tData Load Time 0.004 (0.005)\tCE Loss 12.7229 (11.5056)\tVB Loss 3.1629 (2.3962)\tF1 0.623 (0.728)\n",
            "Epoch: [39][700/1405]\tBatch Time 0.159 (0.213)\tData Load Time 0.004 (0.005)\tCE Loss 10.8018 (11.4831)\tVB Loss 1.4770 (2.3855)\tF1 0.573 (0.729)\n",
            "Epoch: [39][800/1405]\tBatch Time 0.205 (0.212)\tData Load Time 0.005 (0.005)\tCE Loss 11.5288 (11.4748)\tVB Loss 2.9980 (2.4056)\tF1 0.805 (0.726)\n",
            "Epoch: [39][900/1405]\tBatch Time 0.238 (0.212)\tData Load Time 0.005 (0.005)\tCE Loss 11.9635 (11.4733)\tVB Loss 2.8000 (2.4023)\tF1 0.681 (0.729)\n",
            "Epoch: [39][1000/1405]\tBatch Time 0.161 (0.212)\tData Load Time 0.004 (0.005)\tCE Loss 11.5775 (11.4714)\tVB Loss 1.0577 (2.3957)\tF1 0.847 (0.732)\n",
            "Epoch: [39][1100/1405]\tBatch Time 0.096 (0.211)\tData Load Time 0.005 (0.005)\tCE Loss 10.9949 (11.4676)\tVB Loss 0.8609 (2.4056)\tF1 0.954 (0.730)\n",
            "Epoch: [39][1200/1405]\tBatch Time 0.130 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 11.4371 (11.4624)\tVB Loss 0.5730 (2.4174)\tF1 0.936 (0.729)\n",
            "Epoch: [39][1300/1405]\tBatch Time 0.187 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 11.8446 (11.4641)\tVB Loss 1.0926 (2.4235)\tF1 0.533 (0.727)\n",
            "Epoch: [39][1400/1405]\tBatch Time 0.219 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 12.3390 (11.4680)\tVB Loss 2.9799 (2.4204)\tF1 0.778 (0.728)\n",
            "Validation: [0/325]\tBatch Time 0.355 (0.355)\tVB Loss 1.1933 (1.1933)\tF1 Score 0.877 (0.877)\t\n",
            "Validation: [100/325]\tBatch Time 0.056 (0.113)\tVB Loss 0.8312 (2.0683)\tF1 Score 0.847 (0.800)\t\n",
            "Validation: [200/325]\tBatch Time 0.113 (0.115)\tVB Loss 1.2855 (2.0793)\tF1 Score 0.834 (0.796)\t\n",
            "Validation: [300/325]\tBatch Time 0.125 (0.115)\tVB Loss 2.6315 (2.0897)\tF1 Score 0.566 (0.793)\t\n",
            "\n",
            " * LOSS - 2.155, F1 SCORE - 0.788\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000500\n",
            "\n",
            "Epoch: [40][0/1405]\tBatch Time 0.432 (0.432)\tData Load Time 0.273 (0.273)\tCE Loss 10.7701 (10.7701)\tVB Loss 0.5240 (0.5240)\tF1 0.953 (0.953)\n",
            "Epoch: [40][100/1405]\tBatch Time 0.159 (0.233)\tData Load Time 0.004 (0.008)\tCE Loss 11.4812 (11.4668)\tVB Loss 1.6438 (2.5873)\tF1 0.805 (0.728)\n",
            "Epoch: [40][200/1405]\tBatch Time 0.219 (0.224)\tData Load Time 0.004 (0.006)\tCE Loss 11.2729 (11.4653)\tVB Loss 5.4591 (2.4580)\tF1 0.473 (0.733)\n",
            "Epoch: [40][300/1405]\tBatch Time 0.205 (0.215)\tData Load Time 0.005 (0.006)\tCE Loss 10.3132 (11.4681)\tVB Loss 1.9462 (2.4003)\tF1 0.870 (0.734)\n",
            "Epoch: [40][400/1405]\tBatch Time 0.124 (0.212)\tData Load Time 0.005 (0.005)\tCE Loss 11.1650 (11.4487)\tVB Loss 1.5074 (2.3995)\tF1 0.717 (0.732)\n",
            "Epoch: [40][500/1405]\tBatch Time 0.179 (0.212)\tData Load Time 0.004 (0.005)\tCE Loss 11.6178 (11.4618)\tVB Loss 3.0943 (2.4070)\tF1 0.487 (0.731)\n",
            "Epoch: [40][600/1405]\tBatch Time 0.203 (0.213)\tData Load Time 0.004 (0.005)\tCE Loss 11.9728 (11.4678)\tVB Loss 2.7021 (2.4186)\tF1 0.851 (0.730)\n",
            "Epoch: [40][700/1405]\tBatch Time 0.223 (0.211)\tData Load Time 0.004 (0.005)\tCE Loss 11.0078 (11.4737)\tVB Loss 2.4991 (2.3970)\tF1 0.694 (0.733)\n",
            "Epoch: [40][800/1405]\tBatch Time 0.233 (0.211)\tData Load Time 0.004 (0.005)\tCE Loss 10.7992 (11.4655)\tVB Loss 2.5298 (2.3856)\tF1 0.667 (0.733)\n",
            "Epoch: [40][900/1405]\tBatch Time 0.176 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 10.5555 (11.4676)\tVB Loss 1.9704 (2.3837)\tF1 0.838 (0.731)\n",
            "Epoch: [40][1000/1405]\tBatch Time 0.230 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 10.8257 (11.4700)\tVB Loss 1.6542 (2.3961)\tF1 0.915 (0.730)\n",
            "Epoch: [40][1100/1405]\tBatch Time 0.237 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 12.3910 (11.4676)\tVB Loss 2.9337 (2.4055)\tF1 0.838 (0.730)\n",
            "Epoch: [40][1200/1405]\tBatch Time 0.195 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 11.8493 (11.4636)\tVB Loss 0.4496 (2.4164)\tF1 0.935 (0.728)\n",
            "Epoch: [40][1300/1405]\tBatch Time 0.194 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 11.0470 (11.4632)\tVB Loss 3.6488 (2.4184)\tF1 0.634 (0.728)\n",
            "Epoch: [40][1400/1405]\tBatch Time 0.203 (0.209)\tData Load Time 0.005 (0.005)\tCE Loss 10.8301 (11.4623)\tVB Loss 5.1433 (2.4279)\tF1 0.677 (0.726)\n",
            "Validation: [0/325]\tBatch Time 0.346 (0.346)\tVB Loss 1.3330 (1.3330)\tF1 Score 0.939 (0.939)\t\n",
            "Validation: [100/325]\tBatch Time 0.094 (0.100)\tVB Loss 0.8091 (2.0294)\tF1 Score 0.580 (0.801)\t\n",
            "Validation: [200/325]\tBatch Time 0.101 (0.100)\tVB Loss 2.6978 (2.0483)\tF1 Score 0.771 (0.792)\t\n",
            "Validation: [300/325]\tBatch Time 0.107 (0.101)\tVB Loss 0.7089 (2.0931)\tF1 Score 0.836 (0.792)\t\n",
            "\n",
            " * LOSS - 2.111, F1 SCORE - 0.790\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000492\n",
            "\n",
            "Epoch: [41][0/1405]\tBatch Time 0.543 (0.543)\tData Load Time 0.248 (0.248)\tCE Loss 11.6219 (11.6219)\tVB Loss 2.8955 (2.8955)\tF1 0.826 (0.826)\n",
            "Epoch: [41][100/1405]\tBatch Time 0.244 (0.230)\tData Load Time 0.004 (0.007)\tCE Loss 11.5837 (11.4411)\tVB Loss 2.5846 (2.4602)\tF1 0.777 (0.723)\n",
            "Epoch: [41][200/1405]\tBatch Time 0.231 (0.216)\tData Load Time 0.005 (0.006)\tCE Loss 11.5029 (11.4372)\tVB Loss 3.8483 (2.3503)\tF1 0.825 (0.733)\n",
            "Epoch: [41][300/1405]\tBatch Time 0.221 (0.214)\tData Load Time 0.006 (0.005)\tCE Loss 11.3297 (11.4547)\tVB Loss 1.9057 (2.3729)\tF1 0.880 (0.732)\n",
            "Epoch: [41][400/1405]\tBatch Time 0.208 (0.211)\tData Load Time 0.005 (0.005)\tCE Loss 12.0550 (11.4591)\tVB Loss 2.9109 (2.3703)\tF1 0.567 (0.731)\n",
            "Epoch: [41][500/1405]\tBatch Time 0.228 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 11.7830 (11.4401)\tVB Loss 4.2389 (2.3532)\tF1 0.753 (0.733)\n",
            "Epoch: [41][600/1405]\tBatch Time 0.146 (0.207)\tData Load Time 0.005 (0.005)\tCE Loss 11.4851 (11.4387)\tVB Loss 1.2031 (2.3507)\tF1 0.885 (0.734)\n",
            "Epoch: [41][700/1405]\tBatch Time 0.190 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 12.0228 (11.4418)\tVB Loss 2.7386 (2.3628)\tF1 0.821 (0.734)\n",
            "Epoch: [41][800/1405]\tBatch Time 0.221 (0.208)\tData Load Time 0.005 (0.005)\tCE Loss 10.4747 (11.4559)\tVB Loss 1.5370 (2.3604)\tF1 0.718 (0.732)\n",
            "Epoch: [41][900/1405]\tBatch Time 0.222 (0.207)\tData Load Time 0.004 (0.005)\tCE Loss 11.4962 (11.4573)\tVB Loss 2.9562 (2.3821)\tF1 0.724 (0.731)\n",
            "Epoch: [41][1000/1405]\tBatch Time 0.228 (0.208)\tData Load Time 0.004 (0.005)\tCE Loss 12.0493 (11.4623)\tVB Loss 1.5029 (2.3858)\tF1 0.750 (0.730)\n",
            "Epoch: [41][1100/1405]\tBatch Time 0.241 (0.207)\tData Load Time 0.005 (0.005)\tCE Loss 11.4875 (11.4617)\tVB Loss 3.0518 (2.3851)\tF1 0.556 (0.729)\n",
            "Epoch: [41][1200/1405]\tBatch Time 0.197 (0.207)\tData Load Time 0.004 (0.005)\tCE Loss 11.4488 (11.4650)\tVB Loss 2.1502 (2.4050)\tF1 0.727 (0.730)\n",
            "Epoch: [41][1300/1405]\tBatch Time 0.199 (0.207)\tData Load Time 0.005 (0.005)\tCE Loss 11.4118 (11.4614)\tVB Loss 1.5702 (2.4106)\tF1 0.295 (0.729)\n",
            "Epoch: [41][1400/1405]\tBatch Time 0.203 (0.207)\tData Load Time 0.004 (0.005)\tCE Loss 10.7649 (11.4564)\tVB Loss 1.5281 (2.4068)\tF1 0.795 (0.729)\n",
            "Validation: [0/325]\tBatch Time 0.351 (0.351)\tVB Loss 2.2298 (2.2298)\tF1 Score 0.838 (0.838)\t\n",
            "Validation: [100/325]\tBatch Time 0.154 (0.114)\tVB Loss 2.5779 (1.9824)\tF1 Score 0.882 (0.800)\t\n",
            "Validation: [200/325]\tBatch Time 0.115 (0.115)\tVB Loss 3.9277 (2.1068)\tF1 Score 0.687 (0.789)\t\n",
            "Validation: [300/325]\tBatch Time 0.154 (0.115)\tVB Loss 3.0342 (2.1226)\tF1 Score 0.700 (0.787)\t\n",
            "\n",
            " * LOSS - 2.136, F1 SCORE - 0.789\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000484\n",
            "\n",
            "Epoch: [42][0/1405]\tBatch Time 0.626 (0.626)\tData Load Time 0.263 (0.263)\tCE Loss 11.2841 (11.2841)\tVB Loss 1.9588 (1.9588)\tF1 0.822 (0.822)\n",
            "Epoch: [42][100/1405]\tBatch Time 0.225 (0.239)\tData Load Time 0.004 (0.007)\tCE Loss 12.5471 (11.4048)\tVB Loss 4.7301 (2.5248)\tF1 0.604 (0.711)\n",
            "Epoch: [42][200/1405]\tBatch Time 0.147 (0.226)\tData Load Time 0.005 (0.006)\tCE Loss 11.0524 (11.4282)\tVB Loss 1.6519 (2.4612)\tF1 0.701 (0.710)\n",
            "Epoch: [42][300/1405]\tBatch Time 0.289 (0.221)\tData Load Time 0.005 (0.006)\tCE Loss 11.8080 (11.4136)\tVB Loss 3.4966 (2.3965)\tF1 0.762 (0.713)\n",
            "Epoch: [42][400/1405]\tBatch Time 0.153 (0.219)\tData Load Time 0.004 (0.005)\tCE Loss 10.8170 (11.4200)\tVB Loss 1.8896 (2.3843)\tF1 0.673 (0.719)\n",
            "Epoch: [42][500/1405]\tBatch Time 0.195 (0.217)\tData Load Time 0.005 (0.005)\tCE Loss 11.9382 (11.4413)\tVB Loss 2.7350 (2.4162)\tF1 0.717 (0.720)\n",
            "Epoch: [42][600/1405]\tBatch Time 0.235 (0.215)\tData Load Time 0.004 (0.005)\tCE Loss 11.1500 (11.4305)\tVB Loss 5.0349 (2.3831)\tF1 0.691 (0.722)\n",
            "Epoch: [42][700/1405]\tBatch Time 0.119 (0.213)\tData Load Time 0.005 (0.005)\tCE Loss 11.2305 (11.4380)\tVB Loss 0.9378 (2.3841)\tF1 0.388 (0.724)\n",
            "Epoch: [42][800/1405]\tBatch Time 0.232 (0.213)\tData Load Time 0.004 (0.005)\tCE Loss 11.4114 (11.4538)\tVB Loss 2.5417 (2.4245)\tF1 0.825 (0.722)\n",
            "Epoch: [42][900/1405]\tBatch Time 0.178 (0.212)\tData Load Time 0.004 (0.005)\tCE Loss 11.9263 (11.4493)\tVB Loss 2.0250 (2.4153)\tF1 0.635 (0.722)\n",
            "Epoch: [42][1000/1405]\tBatch Time 0.234 (0.212)\tData Load Time 0.004 (0.005)\tCE Loss 11.7681 (11.4520)\tVB Loss 2.4585 (2.4079)\tF1 0.734 (0.723)\n",
            "Epoch: [42][1100/1405]\tBatch Time 0.113 (0.211)\tData Load Time 0.005 (0.005)\tCE Loss 11.4072 (11.4492)\tVB Loss 0.7762 (2.3998)\tF1 0.761 (0.723)\n",
            "Epoch: [42][1200/1405]\tBatch Time 0.205 (0.211)\tData Load Time 0.004 (0.005)\tCE Loss 10.7457 (11.4535)\tVB Loss 2.6289 (2.4168)\tF1 0.755 (0.723)\n",
            "Epoch: [42][1300/1405]\tBatch Time 0.247 (0.211)\tData Load Time 0.005 (0.005)\tCE Loss 11.3913 (11.4529)\tVB Loss 2.0410 (2.4088)\tF1 0.865 (0.723)\n",
            "Epoch: [42][1400/1405]\tBatch Time 0.212 (0.211)\tData Load Time 0.005 (0.005)\tCE Loss 10.9617 (11.4483)\tVB Loss 1.4992 (2.4077)\tF1 0.675 (0.725)\n",
            "Validation: [0/325]\tBatch Time 0.304 (0.304)\tVB Loss 2.5657 (2.5657)\tF1 Score 0.879 (0.879)\t\n",
            "Validation: [100/325]\tBatch Time 0.072 (0.100)\tVB Loss 3.3107 (2.1079)\tF1 Score 0.730 (0.791)\t\n",
            "Validation: [200/325]\tBatch Time 0.079 (0.099)\tVB Loss 1.0521 (2.1860)\tF1 Score 0.863 (0.785)\t\n",
            "Validation: [300/325]\tBatch Time 0.084 (0.105)\tVB Loss 1.3726 (2.1674)\tF1 Score 0.865 (0.785)\t\n",
            "\n",
            " * LOSS - 2.163, F1 SCORE - 0.785\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000476\n",
            "\n",
            "Epoch: [43][0/1405]\tBatch Time 0.613 (0.613)\tData Load Time 0.236 (0.236)\tCE Loss 11.5697 (11.5697)\tVB Loss 2.9995 (2.9995)\tF1 0.822 (0.822)\n",
            "Epoch: [43][100/1405]\tBatch Time 0.286 (0.235)\tData Load Time 0.004 (0.007)\tCE Loss 11.8768 (11.4592)\tVB Loss 4.6156 (2.2504)\tF1 0.566 (0.734)\n",
            "Epoch: [43][200/1405]\tBatch Time 0.283 (0.224)\tData Load Time 0.004 (0.006)\tCE Loss 12.1212 (11.4217)\tVB Loss 1.3379 (2.2762)\tF1 0.855 (0.732)\n",
            "Epoch: [43][300/1405]\tBatch Time 0.114 (0.217)\tData Load Time 0.004 (0.006)\tCE Loss 10.3789 (11.3957)\tVB Loss 0.6070 (2.2797)\tF1 0.912 (0.736)\n",
            "Epoch: [43][400/1405]\tBatch Time 0.243 (0.214)\tData Load Time 0.004 (0.005)\tCE Loss 11.7746 (11.4151)\tVB Loss 1.4163 (2.3660)\tF1 0.944 (0.729)\n",
            "Epoch: [43][500/1405]\tBatch Time 0.211 (0.213)\tData Load Time 0.006 (0.005)\tCE Loss 10.6058 (11.4396)\tVB Loss 1.4321 (2.3620)\tF1 0.857 (0.733)\n",
            "Epoch: [43][600/1405]\tBatch Time 0.306 (0.212)\tData Load Time 0.007 (0.005)\tCE Loss 11.1851 (11.4407)\tVB Loss 1.8718 (2.4065)\tF1 0.939 (0.729)\n",
            "Epoch: [43][700/1405]\tBatch Time 0.254 (0.211)\tData Load Time 0.005 (0.005)\tCE Loss 11.8727 (11.4313)\tVB Loss 2.4964 (2.3905)\tF1 0.661 (0.730)\n",
            "Epoch: [43][800/1405]\tBatch Time 0.264 (0.212)\tData Load Time 0.004 (0.005)\tCE Loss 10.9117 (11.4334)\tVB Loss 1.6764 (2.3815)\tF1 0.821 (0.731)\n",
            "Epoch: [43][900/1405]\tBatch Time 0.209 (0.212)\tData Load Time 0.004 (0.005)\tCE Loss 11.9588 (11.4459)\tVB Loss 1.6322 (2.3840)\tF1 0.758 (0.731)\n",
            "Epoch: [43][1000/1405]\tBatch Time 0.235 (0.211)\tData Load Time 0.005 (0.005)\tCE Loss 11.0296 (11.4507)\tVB Loss 2.9835 (2.3747)\tF1 0.593 (0.731)\n",
            "Epoch: [43][1100/1405]\tBatch Time 0.153 (0.211)\tData Load Time 0.005 (0.005)\tCE Loss 10.4005 (11.4522)\tVB Loss 2.8897 (2.3721)\tF1 0.795 (0.732)\n",
            "Epoch: [43][1200/1405]\tBatch Time 0.249 (0.210)\tData Load Time 0.005 (0.005)\tCE Loss 10.8965 (11.4509)\tVB Loss 2.0576 (2.3566)\tF1 0.883 (0.733)\n",
            "Epoch: [43][1300/1405]\tBatch Time 0.110 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 11.8220 (11.4456)\tVB Loss 1.1003 (2.3656)\tF1 0.916 (0.733)\n",
            "Epoch: [43][1400/1405]\tBatch Time 0.220 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 12.1426 (11.4407)\tVB Loss 2.4517 (2.3706)\tF1 0.689 (0.732)\n",
            "Validation: [0/325]\tBatch Time 0.325 (0.325)\tVB Loss 2.0710 (2.0710)\tF1 Score 0.895 (0.895)\t\n",
            "Validation: [100/325]\tBatch Time 0.112 (0.117)\tVB Loss 0.6273 (2.0738)\tF1 Score 0.837 (0.794)\t\n",
            "Validation: [200/325]\tBatch Time 0.108 (0.118)\tVB Loss 2.3988 (2.1307)\tF1 Score 0.835 (0.796)\t\n",
            "Validation: [300/325]\tBatch Time 0.131 (0.116)\tVB Loss 1.6650 (2.1158)\tF1 Score 0.914 (0.798)\t\n",
            "\n",
            " * LOSS - 2.134, F1 SCORE - 0.798\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000469\n",
            "\n",
            "Epoch: [44][0/1405]\tBatch Time 0.526 (0.526)\tData Load Time 0.278 (0.278)\tCE Loss 10.7873 (10.7873)\tVB Loss 2.2002 (2.2002)\tF1 0.753 (0.753)\n",
            "Epoch: [44][100/1405]\tBatch Time 0.234 (0.235)\tData Load Time 0.004 (0.008)\tCE Loss 10.8830 (11.4315)\tVB Loss 2.6748 (2.3421)\tF1 0.763 (0.721)\n",
            "Epoch: [44][200/1405]\tBatch Time 0.250 (0.220)\tData Load Time 0.005 (0.006)\tCE Loss 11.9556 (11.3853)\tVB Loss 2.8514 (2.2953)\tF1 0.792 (0.733)\n",
            "Epoch: [44][300/1405]\tBatch Time 0.198 (0.218)\tData Load Time 0.005 (0.006)\tCE Loss 10.1873 (11.4192)\tVB Loss 2.6678 (2.3366)\tF1 0.628 (0.733)\n",
            "Epoch: [44][400/1405]\tBatch Time 0.245 (0.215)\tData Load Time 0.005 (0.005)\tCE Loss 11.3712 (11.4175)\tVB Loss 3.0437 (2.3184)\tF1 0.803 (0.739)\n",
            "Epoch: [44][500/1405]\tBatch Time 0.231 (0.214)\tData Load Time 0.004 (0.005)\tCE Loss 10.8447 (11.4299)\tVB Loss 2.9851 (2.3392)\tF1 0.810 (0.733)\n",
            "Epoch: [44][600/1405]\tBatch Time 0.198 (0.212)\tData Load Time 0.005 (0.005)\tCE Loss 11.5618 (11.4367)\tVB Loss 1.6485 (2.3524)\tF1 0.578 (0.732)\n",
            "Epoch: [44][700/1405]\tBatch Time 0.210 (0.212)\tData Load Time 0.004 (0.005)\tCE Loss 11.2656 (11.4430)\tVB Loss 1.7187 (2.3841)\tF1 0.853 (0.732)\n",
            "Epoch: [44][800/1405]\tBatch Time 0.179 (0.211)\tData Load Time 0.004 (0.005)\tCE Loss 10.7819 (11.4453)\tVB Loss 3.7967 (2.3751)\tF1 0.764 (0.731)\n",
            "Epoch: [44][900/1405]\tBatch Time 0.178 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 11.8249 (11.4382)\tVB Loss 3.1822 (2.3830)\tF1 0.736 (0.730)\n",
            "Epoch: [44][1000/1405]\tBatch Time 0.244 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 11.0963 (11.4368)\tVB Loss 1.7070 (2.3722)\tF1 0.666 (0.731)\n",
            "Epoch: [44][1100/1405]\tBatch Time 0.232 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 11.6548 (11.4399)\tVB Loss 1.1576 (2.3627)\tF1 0.836 (0.731)\n",
            "Epoch: [44][1200/1405]\tBatch Time 0.246 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 11.3811 (11.4408)\tVB Loss 2.8777 (2.3796)\tF1 0.796 (0.731)\n",
            "Epoch: [44][1300/1405]\tBatch Time 0.221 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 11.7568 (11.4348)\tVB Loss 2.1150 (2.3833)\tF1 0.753 (0.731)\n",
            "Epoch: [44][1400/1405]\tBatch Time 0.194 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 11.8203 (11.4357)\tVB Loss 2.6816 (2.3772)\tF1 0.808 (0.733)\n",
            "Validation: [0/325]\tBatch Time 0.285 (0.285)\tVB Loss 0.0631 (0.0631)\tF1 Score 1.000 (1.000)\t\n",
            "Validation: [100/325]\tBatch Time 0.082 (0.098)\tVB Loss 2.4878 (2.2064)\tF1 Score 0.870 (0.781)\t\n",
            "Validation: [200/325]\tBatch Time 0.095 (0.098)\tVB Loss 2.9002 (2.0940)\tF1 Score 0.863 (0.796)\t\n",
            "Validation: [300/325]\tBatch Time 0.100 (0.101)\tVB Loss 1.5995 (2.1318)\tF1 Score 0.763 (0.792)\t\n",
            "\n",
            " * LOSS - 2.138, F1 SCORE - 0.792\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000462\n",
            "\n",
            "Epoch: [45][0/1405]\tBatch Time 0.588 (0.588)\tData Load Time 0.223 (0.223)\tCE Loss 11.4104 (11.4104)\tVB Loss 1.6792 (1.6792)\tF1 0.679 (0.679)\n",
            "Epoch: [45][100/1405]\tBatch Time 0.224 (0.232)\tData Load Time 0.004 (0.007)\tCE Loss 11.7349 (11.3889)\tVB Loss 2.3318 (2.2205)\tF1 0.837 (0.742)\n",
            "Epoch: [45][200/1405]\tBatch Time 0.306 (0.222)\tData Load Time 0.004 (0.006)\tCE Loss 12.1027 (11.4348)\tVB Loss 5.4323 (2.3601)\tF1 0.696 (0.733)\n",
            "Epoch: [45][300/1405]\tBatch Time 0.236 (0.219)\tData Load Time 0.004 (0.006)\tCE Loss 11.7331 (11.4284)\tVB Loss 3.3036 (2.3511)\tF1 0.761 (0.729)\n",
            "Epoch: [45][400/1405]\tBatch Time 0.222 (0.220)\tData Load Time 0.005 (0.005)\tCE Loss 11.0920 (11.4312)\tVB Loss 2.9143 (2.3911)\tF1 0.611 (0.728)\n",
            "Epoch: [45][500/1405]\tBatch Time 0.194 (0.218)\tData Load Time 0.005 (0.005)\tCE Loss 10.7661 (11.4494)\tVB Loss 3.3146 (2.4048)\tF1 0.703 (0.728)\n",
            "Epoch: [45][600/1405]\tBatch Time 0.250 (0.216)\tData Load Time 0.004 (0.005)\tCE Loss 11.4583 (11.4578)\tVB Loss 4.3310 (2.4358)\tF1 0.601 (0.727)\n",
            "Epoch: [45][700/1405]\tBatch Time 0.226 (0.213)\tData Load Time 0.004 (0.005)\tCE Loss 10.9774 (11.4507)\tVB Loss 2.0583 (2.4126)\tF1 0.877 (0.728)\n",
            "Epoch: [45][800/1405]\tBatch Time 0.250 (0.213)\tData Load Time 0.004 (0.005)\tCE Loss 11.1717 (11.4447)\tVB Loss 2.0715 (2.4028)\tF1 0.700 (0.729)\n",
            "Epoch: [45][900/1405]\tBatch Time 0.150 (0.213)\tData Load Time 0.004 (0.005)\tCE Loss 11.1616 (11.4457)\tVB Loss 1.4613 (2.3933)\tF1 0.881 (0.731)\n",
            "Epoch: [45][1000/1405]\tBatch Time 0.266 (0.212)\tData Load Time 0.004 (0.005)\tCE Loss 12.3943 (11.4488)\tVB Loss 2.7781 (2.4006)\tF1 0.776 (0.731)\n",
            "Epoch: [45][1100/1405]\tBatch Time 0.214 (0.211)\tData Load Time 0.005 (0.005)\tCE Loss 11.3546 (11.4381)\tVB Loss 2.3239 (2.3700)\tF1 0.803 (0.732)\n",
            "Epoch: [45][1200/1405]\tBatch Time 0.191 (0.211)\tData Load Time 0.005 (0.005)\tCE Loss 11.2600 (11.4333)\tVB Loss 2.0051 (2.3562)\tF1 0.748 (0.733)\n",
            "Epoch: [45][1300/1405]\tBatch Time 0.248 (0.210)\tData Load Time 0.004 (0.005)\tCE Loss 11.3194 (11.4327)\tVB Loss 3.2801 (2.3472)\tF1 0.673 (0.733)\n",
            "Epoch: [45][1400/1405]\tBatch Time 0.186 (0.209)\tData Load Time 0.004 (0.005)\tCE Loss 11.5340 (11.4298)\tVB Loss 2.7425 (2.3366)\tF1 0.670 (0.735)\n",
            "Validation: [0/325]\tBatch Time 0.309 (0.309)\tVB Loss 1.4411 (1.4411)\tF1 Score 0.822 (0.822)\t\n",
            "Validation: [100/325]\tBatch Time 0.067 (0.098)\tVB Loss 1.3520 (2.1167)\tF1 Score 0.652 (0.781)\t\n",
            "Validation: [200/325]\tBatch Time 0.059 (0.099)\tVB Loss 0.5437 (2.1654)\tF1 Score 0.743 (0.785)\t\n",
            "Validation: [300/325]\tBatch Time 0.100 (0.103)\tVB Loss 2.7698 (2.1294)\tF1 Score 0.769 (0.790)\t\n",
            "\n",
            " * LOSS - 2.108, F1 SCORE - 0.791\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000455\n",
            "\n",
            "Epoch: [46][0/1405]\tBatch Time 0.514 (0.514)\tData Load Time 0.242 (0.242)\tCE Loss 12.0322 (12.0322)\tVB Loss 1.3494 (1.3494)\tF1 0.819 (0.819)\n",
            "Epoch: [46][100/1405]\tBatch Time 0.247 (0.235)\tData Load Time 0.005 (0.007)\tCE Loss 11.2058 (11.3943)\tVB Loss 4.9304 (2.3625)\tF1 0.704 (0.724)\n",
            "Epoch: [46][200/1405]\tBatch Time 0.266 (0.222)\tData Load Time 0.004 (0.006)\tCE Loss 12.1113 (11.4459)\tVB Loss 3.3645 (2.3873)\tF1 0.735 (0.732)\n",
            "Epoch: [46][300/1405]\tBatch Time 0.244 (0.215)\tData Load Time 0.004 (0.005)\tCE Loss 11.3284 (11.4170)\tVB Loss 1.3324 (2.3329)\tF1 0.830 (0.734)\n",
            "Epoch: [46][400/1405]\tBatch Time 0.208 (0.213)\tData Load Time 0.004 (0.005)\tCE Loss 11.7500 (11.4232)\tVB Loss 2.8369 (2.3132)\tF1 0.775 (0.735)\n",
            "Epoch: [46][500/1405]\tBatch Time 0.292 (0.211)\tData Load Time 0.004 (0.005)\tCE Loss 11.9493 (11.4124)\tVB Loss 2.9111 (2.3317)\tF1 0.607 (0.735)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OipsVUg24hin",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}