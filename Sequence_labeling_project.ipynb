{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sequence_labeling_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sylar257/Sequence_Labeling_Project/blob/master/Sequence_labeling_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEyZ1KroE9iI",
        "colab_type": "text"
      },
      "source": [
        "We will be implementing the [Empower Sequence Labeling with Task-Aware Neural Language Model](https://arxiv.org/pdf/1709.04109.pdf) paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tmFs0zFHDQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from utils import *\n",
        "import torch.nn.functional as F\n",
        "decive = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QENba6KHo59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Highway(nn.Module):\n",
        "    \"\"\"\n",
        "    Highway network\n",
        "    \"\"\"\n",
        "    def __init__(self, size, num_layers=1, dropout=0.5):\n",
        "        \"\"\"\n",
        "        size: size of Linear layer (should match input size)\n",
        "        num_layers: number of transform and gate layers\n",
        "        dropout: dropout rate\n",
        "        \"\"\"\n",
        "        super(Highway, self).__init__()\n",
        "        self.size = size\n",
        "        self.num_layers = num_layers\n",
        "        self.transform = nn.ModuleList() # A list of transform layers\n",
        "        self.gate = nn.ModuleList()      # A list of gate layers\n",
        "        self.dropout = torch.nn.Dropout(p=dropout)\n",
        "\n",
        "        for i in range(num_layers):\n",
        "            transform = nn.Linear(size, size)\n",
        "            gate = nn.Linear(size, size)\n",
        "            self.transform.append(transform)\n",
        "            self.gate.append(gate)\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward-prop.\n",
        "        Returns a tensor with the same dimensions as input tensor\n",
        "        \"\"\"\n",
        "\n",
        "        transformed = F.relu(self.transform[0](x))  # transform with the first transform layer\n",
        "        g = F.sigmoid(self.gate[0](x))              # calculate how much of the transformed input to keep\n",
        "\n",
        "        out = self.dropout(g*transformed + (1-g)*x)               # combine input and transformed input with ratio of g\n",
        "\n",
        "        # If there are additional layers\n",
        "        for i in range(self.num_layers):\n",
        "            transformed = F.relu(self.transform[i](out))\n",
        "            g = F.sigmoid(self.gate[i](out))\n",
        "            out = self.dropout(g*transformed+(1-g)*out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zaftUlVLOad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CRF(nn.Module):\n",
        "    \"\"\"\n",
        "    Confitional Random Field\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dim, tagset_size):\n",
        "        \"\"\"\n",
        "        hidden_dim: the size of word/BLSTM's output (which is the input size for CRF)\n",
        "        tagset_size: number of tags(depending on our dataset)\n",
        "        \"\"\"\n",
        "\n",
        "        super(CRF, self).__init__()\n",
        "        self.tagset_size = tagset_size\n",
        "        self.emission = nn.Linear(hidden_dim, self.tagset_size)\n",
        "        self.transition = nn.Parameter(torch.Tensor(self.tagset_size, self.tagset_size))\n",
        "        self.transition.data.zero_() # initializa the transition matrix to be all zeros\n",
        "\n",
        "    def forward(self, feats):\n",
        "        \"\"\"\n",
        "        feats:   output of word/BLSTM, a tensor of dimensions-(batch_size, timesteps, hidden_dim)\n",
        "        returns: CRF scores, a tensor of dimensions-(batch_size, timesteps, tagset_size, tagset_size)\n",
        "        \"\"\"\n",
        "        self.batch_size = feats.size(0)\n",
        "        self.timesteps  = feats.size(1)\n",
        "\n",
        "        emission_scores = self.emission(feats)  # (batch_size, timesteps, tagset_size)\n",
        "        # here we broadcast emission_scores in order to compute the total score later with transition score\n",
        "        emission_scores = emission_scores.unsqueeze(2).expand(self.batch_size, self.timesteps, self.tagset_size, self.tagset_size)  # (batch_size, timesteps, tagset_size, tagset_size)\n",
        "\n",
        "        crf_scores = emission_scores + self.transition.unsqueeze(0).unsqueeze(0)  # (batch_size, timesteps, tagset_size, tagset_size)\n",
        "        return crf_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAK0OBqVT61P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ViterbiLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Viterbi Loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tag_map):\n",
        "        \"\"\"\n",
        "        :param tag_map: tag map\n",
        "        \"\"\"\n",
        "        super(ViterbiLoss, self).__init__()\n",
        "        self.tagset_size = len(tag_map)\n",
        "        self.start_tag = tag_map['<start>']\n",
        "        self.end_tag = tag_map['<end>']\n",
        "\n",
        "    def forward(self, scores, targets, lengths):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param scores: CRF scores\n",
        "        :param targets: true tags indices in unrolled CRF scores\n",
        "        :param lengths: word sequence lengths\n",
        "        :return: viterbi loss\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size = scores.size(0)\n",
        "        word_pad_len = scores.size(1)\n",
        "\n",
        "        # Gold score\n",
        "\n",
        "        targets = targets.unsqueeze(2)\n",
        "        scores_at_targets = torch.gather(scores.view(batch_size, word_pad_len, -1), 2, targets).squeeze(\n",
        "            2)  # (batch_size, word_pad_len)\n",
        "\n",
        "        # Everything is already sorted by lengths\n",
        "        scores_at_targets = pack_padded_sequence(scores_at_targets, lengths, batch_first=True)\n",
        "        gold_score = scores_at_targets.data.sum()\n",
        "\n",
        "        # All paths' scores\n",
        "\n",
        "        # Create a tensor to hold accumulated sequence scores at each current tag\n",
        "        scores_upto_t = torch.zeros(batch_size, self.tagset_size).to(device)\n",
        "\n",
        "        for t in range(max(lengths)):\n",
        "            batch_size_t = sum([l > t for l in lengths])  # effective batch size (sans pads) at this timestep\n",
        "            if t == 0:\n",
        "                scores_upto_t[:batch_size_t] = scores[:batch_size_t, t, self.start_tag, :]  # (batch_size, tagset_size)\n",
        "            else:\n",
        "                # We add scores at current timestep to scores accumulated up to previous timestep, and log-sum-exp\n",
        "                # Remember, the cur_tag of the previous timestep is the prev_tag of this timestep\n",
        "                # So, broadcast prev. timestep's cur_tag scores along cur. timestep's cur_tag dimension\n",
        "                scores_upto_t[:batch_size_t] = log_sum_exp(\n",
        "                    scores[:batch_size_t, t, :, :] + scores_upto_t[:batch_size_t].unsqueeze(2),\n",
        "                    dim=1)  # (batch_size, tagset_size)\n",
        "\n",
        "        # We only need the final accumulated scores at the <end> tag\n",
        "        all_paths_scores = scores_upto_t[:, self.end_tag].sum()\n",
        "\n",
        "        viterbi_loss = all_paths_scores - gold_score\n",
        "        viterbi_loss = viterbi_loss / batch_size\n",
        "\n",
        "        return viterbi_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4w6o7q1jQxtd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LM_LSTM_CRF(nn.Module):\n",
        "    \"\"\"\n",
        "    The encompassing LM-LSTM-CRF\n",
        "    \"\"\"\n",
        "    def __init__(self, tagset_size, charset_size, char_emb_dim, char_rnn_dim, char_rnn_layers, vocab_size,\n",
        "                 lm_vocab_size, word_emb_dim, word_rnn_dim, word_rnn_layers, dropout, highway_layers=1):\n",
        "        \"\"\"\n",
        "        tagset_size:   number of tags\n",
        "        charset_size:  size of character vocabulary\n",
        "        char_emb_dim:  size of character embeddings\n",
        "        char_rnn_dim:  size of charactor RNNS/LSTMs\n",
        "        char_rnn_layers: number of layers in character RNN/LSTMs\n",
        "        vocab_size:    input vocabulary size\n",
        "        lm_vocab_size: vocabulary size of language models (in-corpus words subject to word frequency threshold)\n",
        "        word_emb_dim:  size of word embeddings\n",
        "        word_rnn_dim:  size of word RNN/BLSTM\n",
        "        word_rnn_layers: number of layers in word RNNs/LSTMs\n",
        "        dropout:       dropout\n",
        "        highway_layers: number of transform and gate layers\n",
        "        \"\"\"\n",
        "        \n",
        "        super(LM_LSTM_CRF, self).__init__()\n",
        "\n",
        "        self.tagset_size  = tagset_size # this is the size of the outout vocab of the tagging model\n",
        "\n",
        "        self.charset_size = charset_size\n",
        "        self.char_emb_dim = char_emb_dim\n",
        "        self.char_rnn_dim = char_rnn_dim\n",
        "        self.char_rnn_layers = char_rnn_layers\n",
        "\n",
        "        self.wordset_size  = vocab_size     # this is the size of the input vocab (embedding layer) of the tagging model\n",
        "        self.lm_vocab_size = lm_vocab_size  # this is the size of the output vocab of the language model\n",
        "        self.word_emb_dim  = word_emb_dim\n",
        "        self.word_rnn_dim  = word_rnn_dim\n",
        "        self.word_rnn_layers = word_rnn_layers\n",
        "        \n",
        "        self.highway_layers = highway_layers\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # charactor embedding layer\n",
        "        self.char_embeds = nn.Embedding(self.charset_size, self.char_emb_dim) \n",
        "\n",
        "        # forward char LSTM\n",
        "        self.forw_char_lstm = nn.LSTM(input_size=self.char_emb_dim, hidden_size=self.char_rnn_dim, \n",
        "                                      num_layers=self.char_rnn_layers, bidirectional=False, dropout = dropout)\n",
        "        # backward char LSTM\n",
        "        self.back_char_lstm = nn.LSTM(input_size=self.char_emb_dim, hidden_size=self.char_rnn_dim,\n",
        "                                      num_layers=self.char_rnn_layers, bidirectional=False, dropout = dropout)\n",
        "        \n",
        "        # word embedding layer\n",
        "        self.word_embeds = nn.Embedding(num_embeddings=self.wordset_size,embedding_dim=self.word_emb_dim)\n",
        "        # Define word-level bidirection LSTM\n",
        "        # Take note on the hidden_size\n",
        "        self.word_blstm   = nn.LSTM(self.word_emb_dim + self.char_rnn_dim * 2, # input_size\n",
        "                                    self.word_rnn_dim // 2,                    # hidden_size\n",
        "                                    # This is because Bi-directional LSTM will concat forward and backward output\n",
        "                                    # therefore we specify word_rnn_dim//2 but will get output size of word_rnn_dim\n",
        "                                    num_layers=self.word_rnn_layers,\n",
        "                                    bidirectional=True,\n",
        "                                    dropout=dropout\n",
        "                                    )\n",
        "        \n",
        "        # Conditinoal Random Field layer\n",
        "        self.crf = CRF(hidden_dim=self.word_rnn_dim,tagset_size=self.tagset_size)\n",
        "\n",
        "        # 3 places that we implemented highway connections\n",
        "        self.forw_lm_hw = Highway(size=self.char_rnn_dim,\n",
        "                                  num_layers=self.highway_layers,\n",
        "                                  dropout=dropout)\n",
        "        self.back_lm_hw = Highway(size=self.char_rnn_dim,\n",
        "                                  num_layers=self.highway_layers,\n",
        "                                  dropout=dropout)\n",
        "        self.subword_hw = Highway(2 * self.char_rnn_dim, \n",
        "                                  num_layers=self.highway_layers,\n",
        "                                  dropout=dropout)\n",
        "        \n",
        "        # Linear layers for language models, They are used for \"muti-task training\" for language models (predicting next word)\n",
        "        self.forw_lm_out = nn.Linear(self.char_rnn_dim, self.lm_vocab_size)\n",
        "        self.back_lm_out = nn.Linear(self.char_rnn_dim, self.lm_vocab_size)\n",
        "\n",
        "    def init_word_embedding(self, embedding):\n",
        "        \"\"\"\n",
        "        Initialize embeddings with pre-trained embeddings.\n",
        "\n",
        "        embedding: pre-trained embeddings to be loaded\n",
        "        \"\"\"\n",
        "        self.word_embeds.weights = nn.Parameter(embeddings)\n",
        "\n",
        "    def fine_tune_word_embeddings(self, fine_tune=False):\n",
        "        \"\"\"\n",
        "        Fine-tune embedding layer? (if using pre-trained embedding layer, consider no fine-tuning)\n",
        "\n",
        "        fine_tune: bool decides if fine_tune\n",
        "        \"\"\"\n",
        "        for p in self.word_embeds.parameters():\n",
        "            p.requires_grad = fine_tune\n",
        "    \n",
        "    def forward(self, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, wmaps, tmaps, wmap_lengths, cmap_lengths):\n",
        "        \"\"\"\n",
        "        cmaps_f: padded encoded forward  character sequences. (batch_size, char_pad_len)\n",
        "        cmaps_b: padded encoded backward character sequences. (batch_size, char_pad_len)\n",
        "        cmarkers_f: padded forward character markers.          (batch_size, word_pad_len)\n",
        "        cmarkers_b: padded backward character markers.         (batch_size, word_pad_len)\n",
        "        wmaps: padded encoded word sequences.                 (batch_size, word_pad_len)\n",
        "        tmaps: padded tag sequences.                          (batch_size, word_pad_len)\n",
        "        wmap_lengths: word sequence lengths.                  (batch_size)\n",
        "        cmap_lengths: character sequence lengths              (batch_size)\n",
        "        \"\"\"\n",
        "\n",
        "        self.batch_size   = cmaps_f.size(0)\n",
        "        self.word_pad_len = wmaps.size(1)\n",
        "\n",
        "        # Sort by decreasing true char. sequence length for grouping up for padding later\n",
        "        cmap_lengths, char_sort_ind = cmap_lengths.sort(dim=0, descending=True)\n",
        "        cmaps_f = cmaps_f[char_sort_ind]\n",
        "        cmaps_b = cmaps_b[char_sort_ind]\n",
        "        cmarkers_f = cmarkers_f[char_sort_ind]\n",
        "        cmarkers_b = cmarkers_b[char_sort_ind]\n",
        "        wmaps = wmaps[char_sort_ind]\n",
        "        tmaps = tmaps[char_sort_ind]\n",
        "        wmap_lengths = wmap_lengths[char_sort_ind]\n",
        "\n",
        "        # Embedding look-up for characters, turning each character to its embedding of char_emb_dim size\n",
        "        cf = self.char_embeds(cmaps_f)  # (batch_size, char_pad_len, char_emb_dim)\n",
        "        cb = self.char_embeds(cmaps_b)  # (batch_size, char_pad_len, char_emb_dim)\n",
        "\n",
        "        # Dropout\n",
        "        cf = self.dropout(cf)  # (batch_size, char_pad_len, char_emb_dim)\n",
        "        cb = self.dropout(cb)\n",
        "\n",
        "        # Pack padded sequence\n",
        "        cf = pack_padded_sequence(cf, lengths=cmap_lengths.tolist(), batch_first=True) # packed sequence of char_emb_dim, with real sequence lengths\n",
        "        cb = pack_padded_sequence(cb, lengths=cmap_lengths.tolist(), batch_first=True)\n",
        "\n",
        "        # LSTM for forward and backword language model & feature extraction for Bi-directional LSTM\n",
        "        cf, _ = self.forw_char_lstm(cf)  # packed sequence of char_rnn_dim, with real sequence lengths\n",
        "        cb, _ = self.back_char_lstm(cb)   \n",
        "\n",
        "        # Unpack packed sequence\n",
        "        cf, _ = pad_packed_sequence(cf, batch_first=True) # (batch, max_char_len_in_batch, char_rnn_dim)\n",
        "        cb, _ = pad_packed_sequence(cb, batch_first=True) \n",
        "\n",
        "        # Sanity check\n",
        "        assert cf.size(1) == max(cmap_lengths.tolist()) == list(cmap_lengths)[0]\n",
        "\n",
        "        # Select RNN outpus only at marker points (spaces in the character sequence)\n",
        "        cmarkers_f = cmarkers_f.unsqueeze(2).expand(self.batch_size, self.word_pad_len, self.char_rnn_dim)\n",
        "        cmarkers_b = cmarkers_b.unsqueeze(2).expand(self.batch_size, self.word_pad_len, self.char_rnn_dim)\n",
        "        # torch.gather return output same dim as index, with value taken from cf. In this case we can see that dim=1 of output might be different from input(cf)\n",
        "        cf_selected = torch.gather(cf, 1, index=cmarkers_f) # (batch_size, word_pad_len, char_rnn_dim)\n",
        "        cb_selected = torch.gather(cb, 1, index=cmarkers_b)\n",
        "\n",
        "        # Only for co-training language model(to boost performance), not useful for tagging after model is trained\n",
        "        if self.training:   # this toggle is true when we set model.train(), false if we set model.eval()\n",
        "            lm_f = self.forw_lm_hw(self.dropout(cf_selected))  # (batch_size, word_pad_len, char_rnn_dim)\n",
        "            lm_b = self.back_lm_hw(self.dropout(cb_selected))\n",
        "            lm_f_scores = self.forw_lm_out(self.dropout(lm_f))  # (batch_size, word_pad_len, lm_vocab_size)\n",
        "            lm_b_scores = self.back_lm_out(self.dropout(lm_b))\n",
        "\n",
        "        # Sort by decreasing true word sequence length\n",
        "        wmap_lengths, word_sort_ind = wmap_lengths.sort(dim=0, descending=True)\n",
        "        wmaps = wmaps[word_sort_ind]\n",
        "        tmaps = tmaps[word_sort_ind]\n",
        "        cf_selected = cf_selected[word_sort_ind]  # for language model\n",
        "        cb_selected = cb_selected[word_sort_ind]\n",
        "        if self.training:\n",
        "            lm_f_scores = lm_f_scores[word_sort_ind]\n",
        "            lm_b_scores = lm_b_scores[word_sort_ind]\n",
        "\n",
        "        # Embedding look-up for words\n",
        "        w = self.word_embeds(wmaps)  # (batch_size, word_pad_len, word_emb_dim)\n",
        "        w = self.dropout(w)\n",
        "\n",
        "        # Sub-word information at each word\n",
        "        subword = self.subword_hw(self.dropout(torch.cat((cf_selected, cb_selected), dim=2)))  # (batch_size, word_pad_len, 2 * char_rnn_dim)\n",
        "        subword = self.dropout(subword)\n",
        "\n",
        "        # Concatenate word embeddings and sub-word features\n",
        "        w = torch.cat((w, subword), dim=2)  # (batch_size, word_pad_len, 2*char_rnn_dim+word_emb_dim)\n",
        "\n",
        "        # Pack padded sequence\n",
        "        w = pack_padded_sequence(w, list(wmap_lengths), batch_first=True)   # packed sequence of word_emb_dim+2*char_rnn_dim\n",
        "\n",
        "        # Bi-directional LSTM\n",
        "        w, _ = self.word_blstm(w)   # packed sequence of word_rnn_dim, with real sequence lengths\n",
        "        \n",
        "        # Unpack packed sequence\n",
        "        w, _ = pad_packed_sequence(w, batch_first=True)  # (batch_size, max_word_len_in_batch, word_rnn_dim)\n",
        "        w = self.dropout(w)\n",
        "\n",
        "        crf_scores = self.crf(w)     # (batch_size, max_word_len_in_batch, tagset_size, tagset_size)\n",
        "\n",
        "        if self.training:\n",
        "            return crf_scores, lm_f_scores, lm_b_scores, wmaps, tmaps, wmap_lengths, word_sort_ind, char_sort_ind\n",
        "        else:\n",
        "            return crf_scores, wmaps, tmaps, wmap_lengths, word_sort_ind, char_sort_ind  # sort inds to reorder, if req."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XRFQhNj_Bfr",
        "colab_type": "code",
        "outputId": "e9cf700a-aa28-42c5-cfa0-8716cf9166aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My\\ Drive"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n",
            "/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ixkzwHv_ye6",
        "colab_type": "code",
        "outputId": "acbcf2dc-94c7-4731-8f39-b93502e8d9e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 2018\n",
            " 2019\n",
            "'Attention in deep learning.md'\n",
            " BEST_checkpoint_lm_lstm_crf.pth.tar\n",
            " checkpoint_lm_lstm_crf.pth.tar\n",
            "'Colab Notebooks'\n",
            " data\n",
            " Enoava\n",
            "'LSTM in PyTorch.md'\n",
            "'LSTM in PyTorch.pdf'\n",
            "'Response to comments_MECHMT_2018_1107_-V4_17_Dec.docx'\n",
            " Snaps\n",
            " Stack-presentation-Dermotologist.png\n",
            "'Starwars Project Video V2.mp4'\n",
            " YOLO.md\n",
            " YOLO.pdf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIwrkKoX_zxO",
        "colab_type": "code",
        "outputId": "d4595d3c-98c6-458e-fd48-a623fde65eef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /gdrive/My\\ Drive/data/embeddings"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive/data/embeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obm_y4aE__pm",
        "colab_type": "code",
        "outputId": "8e7b1ce3-8813-4fed-91ee-de4f20f761c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "glove.6B.100d.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATeBjYOFA5pg",
        "colab_type": "code",
        "outputId": "0dfa8142-bad8-4a13-80d4-1ccebc97cc44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive/data/embeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUmg97Kl9Z-v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "# Rewrite the __getitem__ and add __len__\n",
        "class WCDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for the LM-LSTM-CRF model. To be used by a PyTorch DataLoader to feed batches to the model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, wmaps, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, tmaps, wmap_lengths, cmap_lengths):\n",
        "        \"\"\"\n",
        "        :param wmaps: padded encoded word sequences\n",
        "        :param cmaps_f: padded encoded forward character sequences\n",
        "        :param cmaps_b: padded encoded backward character sequences\n",
        "        :param cmarkers_f: padded forward character markers\n",
        "        :param cmarkers_b: padded backward character markers\n",
        "        :param tmaps: padded encoded tag sequences (indices in unrolled CRF scores)\n",
        "        :param wmap_lengths: word sequence lengths\n",
        "        :param cmap_lengths: character sequence lengths\n",
        "        \"\"\"\n",
        "        self.wmaps = wmaps\n",
        "        self.cmaps_f = cmaps_f\n",
        "        self.cmaps_b = cmaps_b\n",
        "        self.cmarkers_f = cmarkers_f\n",
        "        self.cmarkers_b = cmarkers_b\n",
        "        self.tmaps = tmaps\n",
        "        self.wmap_lengths = wmap_lengths\n",
        "        self.cmap_lengths = cmap_lengths\n",
        "\n",
        "        self.data_size = self.wmaps.size(0)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.wmaps[i], self.cmaps_f[i], self.cmaps_b[i], self.cmarkers_f[i], self.cmarkers_b[i], self.tmaps[i], \\\n",
        "               self.wmap_lengths[i], self.cmap_lengths[i]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHc2_4Nj-iTF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ViterbiDecoder():\n",
        "    \"\"\"\n",
        "    Viterbi Decoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tag_map):\n",
        "        \"\"\"\n",
        "        :param tag_map: tag map\n",
        "        \"\"\"\n",
        "        self.tagset_size = len(tag_map)\n",
        "        self.start_tag = tag_map['<start>']\n",
        "        self.end_tag = tag_map['<end>']\n",
        "\n",
        "    def decode(self, scores, lengths):\n",
        "        \"\"\"\n",
        "        :param scores: CRF scores\n",
        "        :param lengths: word sequence lengths\n",
        "        :return: decoded sequences\n",
        "        \"\"\"\n",
        "        batch_size = scores.size(0)\n",
        "        word_pad_len = scores.size(1)\n",
        "\n",
        "        # Create a tensor to hold accumulated sequence scores at each current tag\n",
        "        scores_upto_t = torch.zeros(batch_size, self.tagset_size)\n",
        "\n",
        "        # Create a tensor to hold back-pointers\n",
        "        # i.e., indices of the previous_tag that corresponds to maximum accumulated score at current tag\n",
        "        # Let pads be the <end> tag index, since that was the last tag in the decoded sequence\n",
        "        backpointers = torch.ones((batch_size, max(lengths), self.tagset_size), dtype=torch.long) * self.end_tag\n",
        "\n",
        "        for t in range(max(lengths)):\n",
        "            batch_size_t = sum([l > t for l in lengths])  # effective batch size (sans pads) at this timestep\n",
        "            if t == 0:\n",
        "                scores_upto_t[:batch_size_t] = scores[:batch_size_t, t, self.start_tag, :]  # (batch_size, tagset_size)\n",
        "                backpointers[:batch_size_t, t, :] = torch.ones((batch_size_t, self.tagset_size),\n",
        "                                                               dtype=torch.long) * self.start_tag\n",
        "            else:\n",
        "                # We add scores at current timestep to scores accumulated up to previous timestep, and\n",
        "                # choose the previous timestep that corresponds to the max. accumulated score for each current timestep\n",
        "                scores_upto_t[:batch_size_t], backpointers[:batch_size_t, t, :] = torch.max(\n",
        "                    scores[:batch_size_t, t, :, :] + scores_upto_t[:batch_size_t].unsqueeze(2),\n",
        "                    dim=1)  # (batch_size, tagset_size)\n",
        "\n",
        "        # Decode/trace best path backwards\n",
        "        decoded = torch.zeros((batch_size, backpointers.size(1)), dtype=torch.long)\n",
        "        pointer = torch.ones((batch_size, 1),\n",
        "                             dtype=torch.long) * self.end_tag  # the pointers at the ends are all <end> tags\n",
        "\n",
        "        for t in list(reversed(range(backpointers.size(1)))):\n",
        "            decoded[:, t] = torch.gather(backpointers[:, t, :], 1, pointer).squeeze(1)\n",
        "            pointer = decoded[:, t].unsqueeze(1)  # (batch_size, 1)\n",
        "\n",
        "        # Sanity check\n",
        "        assert torch.equal(decoded[:, 0], torch.ones((batch_size), dtype=torch.long) * self.start_tag)\n",
        "\n",
        "        # Remove the <starts> at the beginning, and append with <ends> (to compare to targets, if any)\n",
        "        decoded = torch.cat([decoded[:, 1:], torch.ones((batch_size, 1), dtype=torch.long) * self.start_tag],\n",
        "                            dim=1)\n",
        "\n",
        "        return decoded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGatVIt56gXe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import sys\n",
        "from utils import *\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI_mY6W265rh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "task = 'ner'        # tagging task, choose between [ner, pos]\n",
        "train_file = '/gdrive/My Drive/data/CoNLL-2003/eng.train'\n",
        "val_file   = '/gdrive/My Drive/data/CoNLL-2003/eng.testa'\n",
        "test_file  = '/gdrive/My Drive/data/CoNLL-2003/eng.testb'\n",
        "emb_file   = '/gdrive/My Drive/data/embeddings/glove.6B.100d.txt'\n",
        "min_word_freq = 5 # threshold for word frequency to be recognized not as xxunk\n",
        "min_char_freq = 1 # same thing for char frequency\n",
        "caseless   = True # lowercase everything?\n",
        "expand_vocab = True # expand model's input vocabulary to the pre-trained embedding vocabulary?\n",
        "\n",
        "# Model parameters\n",
        "char_emb_dim = 30 # character embedding size\n",
        "with open(emb_file, 'r') as f:\n",
        "    word_emb_dim = len(f.readline().split(' ')) - 1  # word embdding size, \"-1\" is because in the txt file the first place is the word itself, followed by the actual embeddings\n",
        "word_rnn_dim = 300  # word BLSTM hidden size\n",
        "char_rnn_dim = 300  # character RNN size\n",
        "char_rnn_layers = 1 # number of layers in character RNN\n",
        "word_rnn_layers = 1 # number of layers in word BLSTM\n",
        "highway_layers  = 1 # number of layers in highway network\n",
        "dropout = 0.55       # universal dropout rate\n",
        "fine_tune_word_embeddings = False\n",
        "\n",
        "# Training parameters\n",
        "start_epoch = 0   # start at this epoch\n",
        "batch_size  = 10  # batch size\n",
        "lr = 0.015  \n",
        "lr_decay = 0.05\n",
        "momentum = 0.9\n",
        "workers  = 4\n",
        "epochs   = 200    # number of epochs without triggering early stoping\n",
        "grad_clip = 5.\n",
        "print_freq = 300  # print every ___ batches\n",
        "best_f1  = 0.\n",
        "checkpoint = None # Model checkpoint to load. None if training from scratch\n",
        "\n",
        "tag_ind = 1 if task == 'pos' else 3 # choose column in CoNLL 2003 dataset\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnLn_7JoDGUf",
        "colab_type": "code",
        "outputId": "1730c21c-0655-4bd4-af34-f41912f0218d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "global best_f1, epochs_since_improvement, checkpoint, start_epoch, word_map, char_map, tag_map\n",
        "\n",
        "# Read training and validation data\n",
        "train_words, train_tags = read_words_tags(train_file, tag_ind, caseless)\n",
        "val_words, val_tags = read_words_tags(val_file, tag_ind, caseless)\n",
        "\n",
        "if checkpoint is not None:\n",
        "    checkpoint = torch.load(checkpoint)\n",
        "    model = checkpoint['model']\n",
        "    optimizer = checkpoint['optimizer']\n",
        "    word_map  = checkpoint['word_map']\n",
        "    lm_vocab_size = checkpoint['lm_vocab_size']\n",
        "    tag_map   = checkpoint['tag_map']\n",
        "    char_map  = checkpoint['char_map']\n",
        "    start_epoch = checkpoint['epoch'] +1\n",
        "    best_f1   = checkpoint['f1']\n",
        "else:\n",
        "    # create word, char, tag maps\n",
        "    # maps are essentially dictionaries that map a token to an integer\n",
        "    word_map, char_map, tag_map = create_maps(train_words+val_words,train_tags+val_tags, min_word_freq, min_char_freq)\n",
        "\n",
        "    # load pre-trained embeddings, if expand_vocab==True, word_map expand to embedding_word_map\n",
        "    # lm_vocab_size is the word_map size before expand to \"out-of-corpus vocab\"\n",
        "    embeddings, word_map, lm_vocab_size = load_embeddings(emb_file, word_map, expand_vocab)\n",
        "\n",
        "    model = LM_LSTM_CRF(tagset_size=len(tag_map),\n",
        "                        charset_size=len(char_map),\n",
        "                        char_emb_dim=char_emb_dim,\n",
        "                        char_rnn_dim=char_rnn_dim,\n",
        "                        char_rnn_layers=char_rnn_layers,\n",
        "                        vocab_size=len(word_map),       # This is the length after expand\n",
        "                        lm_vocab_size=lm_vocab_size,    # len(word_map) before expand, not influenced by the embedding vocab\n",
        "                        word_emb_dim=word_emb_dim,\n",
        "                        word_rnn_dim=word_rnn_dim,\n",
        "                        word_rnn_layers=word_rnn_layers,\n",
        "                        dropout=dropout,\n",
        "                        highway_layers=highway_layers).to(device)\n",
        "    model.init_word_embedding(embeddings.to(device)) # initializa embedding layers with pre-trained embeddings.(Essentially we just make it nn.Parameter)\n",
        "    model.fine_tune_word_embeddings(fine_tune_word_embeddings)    # decide if these nn.Parameters has requires_grad = True (trainable)\n",
        "    optimizer = optim.SGD(params=filter(lambda p: p.requires_grad, model.parameters()), lr=lr, momentum=momentum)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedding length is 100.\n",
            "You have elected to include embeddings that are out-of-corpus.\n",
            "\n",
            "Loading embeddings...\n",
            "'word_map' is being updated accordingly.\n",
            "\n",
            "Done.\n",
            " Embedding vocabulary: 400054\n",
            " Language Model vocabulary: 4671.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re6HfRf1tGAy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from functools import partial\n",
        "\n",
        "def annealer(f):\n",
        "    def _inner(start, end): return partial(f, start, end)\n",
        "    return _inner\n",
        "\n",
        "@annealer\n",
        "def sched_lin(start, end, pos): return start + pos*(end-start)\n",
        "@annealer\n",
        "def sched_cos(start, end, pos): return start + (1 + math.cos(math.pi*(1-pos))) * (end-start) / 2\n",
        "@annealer\n",
        "def sched_no(start, end, pos): return start\n",
        "@annealer\n",
        "def sched_exp(start, end, pos): return start*(end/start)**pos\n",
        "\n",
        "# This monkey-path is here to enable plotting tensors\n",
        "torch.Tensor.ndim = property(lambda x: len(x.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd0BzLrdtmkC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "7acd8963-fd91-4449-e0e4-4dc07b91e8b4"
      },
      "source": [
        "from torch import tensor\n",
        "def combine_scheds(pcts, scheds):\n",
        "    assert sum(pcts) == 1.\n",
        "    pcts = tensor([0] + list(pcts))\n",
        "    assert torch.all(pcts >= 0)\n",
        "    pcts = torch.cumsum(pcts, 0)\n",
        "    def _inner(pos):\n",
        "        idx = (pos >= pcts).nonzero().max()\n",
        "        actual_pos = (pos-pcts[idx]) / (pcts[idx+1]-pcts[idx])\n",
        "        return scheds[idx](actual_pos)\n",
        "    return _inner\n",
        "sched = combine_scheds([0.3, 0.7], [sched_cos(0.3, 1.1), sched_cos(1.1, 0.1)])\n",
        "a = torch.arange(0, 100)\n",
        "p = torch.linspace(0.01,1,100)\n",
        "plt.plot(a, [sched(o) for o in p])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f4661542a58>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dd3hUZd7/8fd3ZtIbJQklAUIJJbQA\nASmiqKhgARsC6irq6upaUdfyc9euz6q7AvqgrutaWZFmiX2VIrsoJYHQWyghBUggJIGElEnu3x+Z\n3YdFSoBJzsyZ7+u6uMycOWQ+xwMfTs65z7nFGINSSin/57A6gFJKKe/QQldKKZvQQldKKZvQQldK\nKZvQQldKKZtwWfXBsbGxJikpyaqPV0opv5SZmbnPGBN3rPcsK/SkpCQyMjKs+nillPJLIpJzvPf0\nlItSStmEFrpSStmEFrpSStmEFrpSStmEFrpSStmEFrpSStmEFrpSStmEZePQle8pLKtkx75ycvZX\nUFB6mCCngxCXg9AgJ7GRwcRFhdIqOoQ2MWE4HWJ1XKXUUbTQA1y1u45v1u3mg59zyMw50KDfE+Jy\n0CkukuT4SPokxtC/Q3N6to0mxOVs5LRKqRPRQg9QxhhmZ+Ty8ndb2HeoiqSW4Twyqjs920aT1DKC\nNs1CqTOGypo6DlfXsu9QFUUHq9hTVsn2okNkFx4iM+cA6asLAAh2ORiY1JzzusVzXvd4OsVGIKJH\n8Uo1JbFqxqK0tDSjt/5bY9+hKh6dt5YfNu5lUFIL7jq/C8O7xOI4jdMohQcrWZlTQsbOYhZvLWLL\n3kMAdImPZGzftoxNTaB9y3Bvb4JSAUtEMo0xacd8Tws9sPy0bR/3zlxFWaWbhy/uxi3DOp5WkR9P\nbnEFCzcX8uXq3SzfWQzAoKQW3DCkA6N6tibYpdfhlToTWugKgAWb9nLHjJV0aBHO/17Xn26toxr1\n8/JLDvN5Vj4fL89lV3EFsZHBXH9WByYNTaJ5RHCjfrZSdqWFrvh23R7umbmS7q2j+eCWQU1aqHV1\nhsVbi/jg5xwWbCokPNjJ9We157bhnYiPDm2yHErZgRZ6gPt67W7umbmKvokxvHvzIGLCgizLsnnP\nQd5YlE366gKCXQ5uHtaRO87tbGkmpfyJFnoAW5NXwrg3f6ZXQgzv3zKIyBDfGNiUs7+cKd9v4bOs\nAmLCgrjn/C7cNDSJIKeeY1fqRE5U6Pq3x8YKyyq5/YNMYiND+MuvBvhMmQN0aBnB1An9+Ores+nb\nrhnPfbWR0dP+yU/Z+6yOppTf0kK3qSp3LXfMyKT0cA1v3TiA2MgQqyMdU8+2MXxwyyDevjGNKnct\n1729jLs/WknRwSqroynld7TQberpLzawclcJr1zbl55tY6yOc1IjU1rx/eRzmTyyK/9Yv5cLp/zI\nZ6vyseqUoFL+SAvdhhZuKuSjZbv4zTmdGN27jdVxGiw0yMl9I5P5+r6z6Rgbwf2zsvj1+xl6tK5U\nA2mh20xJRTWPzFtDt1ZRPHBRV6vjnJYu8VHMvWMov7+0B//K3seoqYtZsGmv1bGU8nla6DbzxOfr\nKS6v5s/X9vXrh2U5HcKvh3fii3vOJi4qhFvey+DJz9dRWVNrdTSlfJYWuo18tWY36asLuO+CZHol\n+P5584bo2iqKz+8exq1nd+T9n3MY9+bP5BZXWB1LKZ+khW4TZZU1PJm+nj6JMdw5orPVcbwqxOXk\nD5el8Ncb09i5v5zLXvuXnoJR6hi00G1i2g9b2V9exfNX9MZl05tzLkxpxVf3DCexeRi3vJfBq/O3\n6igYpY5w0r/5IvKOiBSKyLrjvC8i8qqIZIvIGhHp7/2Y6kS27j3I+z/tZMLAdvROtMepluNp3zKc\neXcO5ap+Cbzy/Rbu/mgVFdVuq2Mp5RMacij3HjDqBO+PBpI9v24H3jjzWKqhjDE89cV6woOdPHRR\nN6vjNInQICd/vrYv/++S7ny9bjfj3vyZ3aWHrY6llOVOWujGmMVA8QlWGQt8YOotBZqJiP8MfvZz\n367bw5Ls/Tx4UTda+ujdoI1BRLj9nM68c9NAcvZXcOX0n9i4u8zqWEpZyhsnWxOA3CNe53mW/YKI\n3C4iGSKSUVRU5IWPDmzV7jqe/3oj3VtHcf1Z7a2OY4nzuscz544hAIx782f+uVX/XKnA1aRXz4wx\nbxlj0owxaXFxcU350bb08Ypd5B04zGOX9LDthdCG6NEmmk/vGkpi8zBufncFn63KtzqSUpbwRgvk\nA+2OeJ3oWaYaUUW1m1fnZzOoYwvOSY61Oo7l2sSEMeeOIQzq2IL7Z2Xx3pIdVkdSqsl5o9DTgRs9\no10GA6XGmN1e+L7qBN77aSf7DlXx8MXdEPHenKD+LCo0iHcmDeTinq146osNTPl+iw5rVAHlpA/I\nFpGZwAggVkTygCeBIABjzJvA18AlQDZQAdzcWGFVvdKKGt5ctI3zu8eTltTC6jg+JTTIyfTr+vP4\np+uYNn8rZZU1PHFZiv6jpwLCSQvdGDPxJO8b4C6vJVIn9dY/t1FW6eZBP334VmNzOR388ereRIS4\neGfJDmpq63hmTC8cDi11ZW++M4WNapAD5dW8u2Qnl/Vp4xfPObeKiPCHy3oQ7HLw5o/bqHEbXriq\nN04tdWVjWuh+5t2fdlJRXcs95ydbHcXniQiPjOpGsFN4dUE2tcbw0tV99Ehd2ZYWuh85VOXmvSU7\nuCilFd1aR1kdxy+ICA9cVH/heNr8rQQ5heev6K2lrmxJC92PzFiaQ1mlm9+e18XqKH7n/pHJuOvq\nmL5wG06H8OzYXnqhVNmOFrqfqKyp5e1/7uDsLrGktmtmdRy/IyI8dFE33LWGvyzeTrDTyR8u66Gl\nrmxFC91PzMnIZd+hKu46r5/VUfyWiPDo6O5Uuet4Z8kOosNc3D9SRwop+9BC9wPu2jre/HE7/ds3\nY3AnHXd+JkSEJy5L4VCVm6k/bCUqNIhbz+5odSylvEIL3Q98s24P+SWHefJyvUHGGxwO4Y9X9aa8\nys2zX24gOtTFuLR2J/+NSvm4wH2ikx/52792kNQynJE9WlkdxTZcTgdTJ6QyPDmWRz9Zy/yNOqWd\n8n9a6D4uM+cAWbkl3Dysow6187IQl5M3bhhASpto7vpoJZk5B6yOpNQZ0UL3cX/713aiQ11cMyDR\n6ii2FBni4t2bB9I6OpRb3lvB1r0HrY6k1GnTQvdhucUVfLtuDxPPak9EiF7uaCyxkSF8cMtZBDkd\nTHp3BYVllVZHUuq0aKH7sPd+2olDhElDk6yOYnvtW4bz7qSBFJdXc8v7Kyiv0omnlf/RQvdRh6rc\nzFqRyyW929AmJszqOAGhd2IM/3tdPzYUlHHPzFW4a+usjqTUKdFC91GfrszjUJWbScOSrI4SUC7o\n0Yqnx/ZiwaZCnv5ig06QofyKnpj1QcYYPlyaQ6+EaPrpbf5N7leDO5BbXMFbi7fTOS6CScP0xiPl\nH/QI3Qct21HMlr2HuHFwkt5IZJFHRnXnwpRWPPPlBhZuLrQ6jlINooXugz5cmkNMWBCX921rdZSA\n5XQIU8en0r11NPd8tIrNe3Q4o/J9Wug+prCsku/W7WHcgETCgp1WxwloESEu/jYpjfBgJ7e+v4Li\n8mqrIyl1QlroPuaj5btw1xluGNzB6igKaBMTxl9vTKPwYBV3zsikRke+KB+mhe5DamrrmLl8F+d2\njSMpNsLqOMqjb7tmvHh1b5btKObpL9ZbHUep49JC9yHzNxayt6xKj8590JX9EvnNOZ2YsXQXHy7N\nsTqOUsekhe5DZi7fRevoUM7rFmd1FHUMD4/qzohucTydvp4VO4utjqPUL2ih+4i8AxUs3lrEtQPb\n4XLqbvFFTocwbUI/EpuHceeMlewp1We+KN+izeEjZq/IBWD8QJ1owZfFhAXx1o1pVFS7uWNGJlXu\nWqsjKfUfWug+wF1bx6yMXM7tGkdCM31ui6/r2iqKP4/rS1ZuCU+l60VS5Tu00H3Aws1F7C2rYuKg\n9lZHUQ00uncbfjuiMzOX5/7npyulrKaF7gNmLt9FfFQI53ePtzqKOgUPXtSNs7vE8vvP17Euv9Tq\nOEppoVutoOQwizYXcm1aO4L0Yqhfqb9ImkpsRDB3zMikpELvJFXW0gax2NzMPOoMXKuzzvullpEh\nvH7DAArLqrh/VhZ1dfq4XWWdBhW6iIwSkc0iki0ijx7j/fYislBEVonIGhG5xPtR7aeuzjAnM5eh\nnVvSvmW41XHUaUpt14wnLk9h0eYiXl+UbXUcFcBOWugi4gSmA6OBFGCiiKQctdrvgdnGmH7ABOB1\nbwe1o6Xb95NbfFiHKtrA9We1Z2xqW175fgs/bdtndRwVoBpyhD4IyDbGbDfGVAMfA2OPWscA0Z6v\nY4AC70W0r9kZuUSFuri4Z2uro6gzJCK8cGVvOsVFcu/MLJ1oWlmiIYWeABw5LivPs+xITwE3iEge\n8DVwj1fS2Vjp4Rq+WbeHK1ITCA3Sx+TaQUSIizeu7095lZu7dU5SZQFvXRSdCLxnjEkELgE+FJFf\nfG8RuV1EMkQko6ioyEsf7Z/SVxdQ5a7Ti6E2k9wqiheu6sXyHcVM+WGL1XFUgGlIoecDR7ZOomfZ\nkW4FZgMYY34GQoHYo7+RMeYtY0yaMSYtLi6wH0A1e0UuPdpE0ysh+uQrK79yZb9Exqe14/VF21i8\nJbAPXFTTakihrwCSRaSjiARTf9Ez/ah1dgEXAIhID+oLXf8kH8fG3WWszS/l2rREnTPUpp4a05Ou\n8VFMnpXFXj2frprISQvdGOMG7ga+AzZSP5plvYg8IyJjPKs9CNwmIquBmcAkY4wOyD2OORl5BDmF\nK1KPvhSh7CIs2Mn06/tRUV3LvTNXUavj01UTcDVkJWPM19Rf7Dxy2RNHfL0BGObdaPZU7a7js6x8\nRvZoRfOIYKvjqEbUJT6KZ6/oxUNzVvPagq3cP7Kr1ZGUzemdok1s0eZCisuruWZAotVRVBO4ZkAi\nV/VP4NX5W1m6fb/VcZTNaaE3sbmZecRGhnBu18C+KBxInh3biw4tI7j/4yyKy/V5L6rxaKE3oX2H\nqliwqZCr+iforEQBJCLExWsT+1FcXs3v5qxGLy+pxqKt0oQ+zyrAXWe4ur+ebgk0vRJieOyS7szf\nVMi7S3ZaHUfZlBZ6E5qbmUefxBi6tY6yOoqywKShSVzQPZ4/frOJ9QX6/HTlfVroTWR9QSkbd5cx\nTi+GBiwR4eVxfWkWHsS9M1dRUe22OpKyGS30JjI3M49gp4PL+7a1OoqyUIuIYKaMT2X7vnKe/XKD\n1XGUzWihN4Ga2jrSswoYmRJPs3Adex7ohnWJ5Y5z6+cj/WbtbqvjKBvRQm8CizYXsb+8Wi+Gqv94\n4MKu9G3XjEc/WUtByWGr4yib0EJvAnMzc4mNDOYcHXuuPIKcDqaNT8VdW8cDs7P00QDKK7TQG9mB\n8moWbCpkbGqCTgKt/ktSbARPjenJ0u3F/GXxNqvjKBvQhmlk6asLqKnVsefq2K4ZkMilfdrwyj+2\nsDq3xOo4ys9poTeyeSvzSGkTTUpbfe65+iUR4YUrehMfFcLkWVk6lFGdES30RrRl70HW5JVytY49\nVycQEx7EK+NT2bG/nGe/3Gh1HOXHtNAb0bzMPFwOYWyqjj1XJza4U0t+c05nZi7fxT/W77E6jvJT\nWuiNxF1bx6er8hnRLY7YyBCr4yg/8MCFXemVEM2jn6ylUGc5UqdBC72R/Ct7H4UHq/S556rBgl0O\npo7vR0W1m9/NXaNPZVSnTAu9kcxbmU+z8CDO6x5vdRTlR7rER/L4pSn8uKWID5fmWB1H+Rkt9EZQ\neriG79bvYUzftoS4nFbHUX7mhrPac163OJ7/aiPZhQetjqP8iBZ6I/hqzW6q3XV6ukWdFhHhxWv6\nEBHi4r6Ps6h211kdSfkJLfRGMG9lHsnxkfROiLE6ivJT8VGh/PGq3qwvKGPKD1usjqP8hBa6l+3Y\nV05mzgGuHpCIiFgdR/mxi3q2ZnxaO978cRsrdhZbHUf5AS10L/tkZR4OgSv7JVgdRdnAHy5PoV3z\ncCbPyuJgZY3VcZSP00L3oto6w7zMPM7pGker6FCr4ygbiAxxMWV8XwpKDvPMFzohhjoxLXQv+nnb\nfgpKK/ViqPKqAR1a8NsRXZiTmce36/QuUnV8WuheNDczl+hQFyN7tLI6irKZ+0Ym0zshhv/36VoK\nD+pdpOrYtNC9pKyyhm/W7WFsagKhQTr2XHlXkNPBlPF9Ka9y8+i8tXoXqTomLXQv+WrNbqp07Llq\nRF3io3hsdHcWbCpk5vJcq+MoH6SF7iVzM+vHnvdJ1LHnqvHcOCSJs7vE8uyXG9i5r9zqOMrHaKF7\nwbaiQ2TmHOAaHXuuGpnDIbw8rg9BTmHy7CzctXoXqfo/WuheMDczD6dDdOy5ahJtYsJ47srerNpV\nwps/6lyk6v80qNBFZJSIbBaRbBF59DjrXCsiG0RkvYh85N2YvstdW8e8zDzO7RpHvI49V01kTN+2\nXN63LVN/2MravFKr4ygfcdJCFxEnMB0YDaQAE0Uk5ah1koHHgGHGmJ7A/Y2Q1Sct3lpE4cEqrk1r\nZ3UUFWCeHduT2MgQJs/OorKm1uo4ygc05Ah9EJBtjNlujKkGPgbGHrXObcB0Y8wBAGNMoXdj+q7Z\nK/KIjQzmgh763HPVtJqFB/PyuD5kFx7ixW83WR1H+YCGFHoCcOQYqTzPsiN1BbqKyBIRWSoio471\njUTkdhHJEJGMoqKi00vsQ/YdquKHjXu5sl8CQU69HKGa3vDkOCYNTeLdJTtZkr3P6jjKYt5qIReQ\nDIwAJgJ/FZFmR69kjHnLGJNmjEmLi4vz0kdb57NV+bjrjJ5uUZZ6ZFR3OsVF8NCc1ZQe1gd4BbKG\nFHo+cGRjJXqWHSkPSDfG1BhjdgBbqC942zLGMGtFLv3aNyO5VZTVcVQACwt2MnV8KkUHq3jy83VW\nx1EWakihrwCSRaSjiAQDE4D0o9b5jPqjc0QklvpTMNu9mNPnZOWWsLXwEOP16Fz5gD6Jzbjn/GQ+\nyyrgyzUFVsdRFjlpoRtj3MDdwHfARmC2MWa9iDwjImM8q30H7BeRDcBC4HfGmP2NFdoXzM7IJSzI\nyaV92lgdRSkA7jqvM6ntmvH4p+vYU6oP8ApEDTqHboz52hjT1RjT2RjzvGfZE8aYdM/XxhjzgDEm\nxRjT2xjzcWOGttqhKjfpWQVc1qcNUaFBVsdRCgCX08Er1/al2l3H7+au1gd4BSAdmnEavlhdQHl1\nLRPPam91FKX+S6e4SB6/tAf/3LqPD5fmWB1HNTEt9NMwc/kuureOol+7XwzkUcpy15/VnhHd4njh\n641kFx6yOo5qQlrop2hdfilr8kqZOKi9PohL+SQR4aWr+xAa5OSB2VnU6AO8AoYW+imauXwXIS4H\nV+iDuJQPi48O5X+u7M2avFJeW5BtdRzVRLTQT0FFtZvPswq4tE8bYsL0YqjybaN7t+Hq/olMX5jN\nyl0HrI6jmoAW+in4cvVuDlW5uW6QXgxV/uHJMSm0jg5l8qwsyqvcVsdRjUwL/RT8fVkOyfGRDOjQ\n3OooSjVIdGgQU8ansqu4gue+2mB1HNXItNAbaHVuCavzSrlhcAe9GKr8yqCOLfjNOZ2ZuTyXHzbs\ntTqOakRa6A304dIcIoKdXNVfL4Yq/zP5wmRS2kTzyLw1FB2ssjqOaiRa6A1woLyaL1YXcGX/BL0z\nVPmlEJeTaRNSOVTl5pF5a/QuUpvSQm+A2Rm5VLnr+NXgJKujKHXakltF8ejo7izYVMhHy3dZHUc1\nAi30k6itM8xYlsOgji3o1lofk6v8201DkhieHMuzX25gW5HeRWo3Wugn8eOWQnKLD3PjkA5WR1Hq\njDkcwp/G9SUsyMn9H2dR7da7SO1EC/0kPvg5h7ioEC5KaW11FKW8olV0KP9zVR/W5pcy9YctVsdR\nXqSFfgLZhYdYtLmI689qT7BL/1cp+xjVqzXj09rxxo/bWLbd1lMXBBRtqRN4Z8kOgl0Obhisp1uU\n/TxxeQodWoTzwGydi9QutNCP40B5NZ+szOPK1ARiI0OsjqOU10WEuJgyPpU9ZZX8/rN1OpTRBrTQ\nj+Oj5buorKnj1uEdrY6iVKPp1745k0cm88XqAj5ZefTc78rfaKEfQ7W7jvd/2snw5Fi6ttKhisre\n7hzRhUEdW/DE5+vI2V9udRx1BrTQj+HLNQUUHqzi18M7WR1FqUbndAhTxqfidAj3fawTYvgzLfSj\nGGN4+587SI6P5JzkWKvjKNUkEpqF8cJVvcnKLWHaD1utjqNOkxb6URZtKWLD7jJuG95Jn6qoAspl\nfdpybVoi0xdl8/M2Hcroj7TQj/L6wmzaxoTqFHMqID01picdW0YweVYWB8qrrY6jTpEW+hGWbd/P\nip0H+M25nfVGIhWQwoNdvDqxH8Xl1TysT2X0O9paR5i+aBuxkcGMH9jO6ihKWaZXQgwPj+rG9xv2\n8uHSHKvjqFOghe6xNq+UxVuKuPXsToQGOa2Oo5SlbhnWkfO6xfHcVxvZUFBmdRzVQFroHtMXZhMd\n6uKGwToBtFL/fipjs7Ag7pm5kopqnWDaH2ihA+vyS/l2/R4mDU3SGYmU8mgZGcLUCals31fOU+nr\nrY6jGkALHfjzPzYTExbErXojkVL/ZWjnWO45rwuzM/L4bJU+GsDXBXyhr9hZzMLNRdw5ojMxYXp0\nrtTR7r0gmUFJLXj807U6y5GPa1Chi8goEdksItki8ugJ1rtaRIyIpHkvYuMxxvDSt5uIjwrhpiFJ\nVsdRyie5nA6mTUwl2OXgrr+vpLKm1upI6jhOWugi4gSmA6OBFGCiiKQcY70o4D5gmbdDNpYftxSx\nYucB7rkgmbBgHdmi1PG0iQnjlfGpbNpzkGe+3GB1HHUcDTlCHwRkG2O2G2OqgY+BscdY71ngRaDS\ni/kaTV2d4eXvNtOuRRjj03TcuVInc163eO44tzMfLdtF+uoCq+OoY2hIoScAuUe8zvMs+w8R6Q+0\nM8Z85cVsjWpOZi7rC8p46KJueleoUg304EVdSevQnMfmrdHz6T7ojJtMRBzAK8CDDVj3dhHJEJGM\noqKiM/3o01Z6uIaXvt3MwKTmjOnb1rIcSvmbIKeD167rR0iQk9/OWMnhaj2f7ksaUuj5wJHnJBI9\ny/4tCugFLBKRncBgIP1YF0aNMW8ZY9KMMWlxcXGnn/oMTfl+CwcqqnlqTE99oqJSp6hNTBjTJqSy\npfCgTl3nYxpS6CuAZBHpKCLBwAQg/d9vGmNKjTGxxpgkY0wSsBQYY4zJaJTEZ2jTnjI+XJrDdWe1\np2fbGKvjKOWXhifHce/5ycxbmcfsjNyT/wbVJE5a6MYYN3A38B2wEZhtjFkvIs+IyJjGDuhNxhie\nSl9PVKiLBy/sZnUcpfzavRckMzw5lj98vp61eaVWx1E08By6MeZrY0xXY0xnY8zznmVPGGPSj7Hu\nCF89Ov94RS5Ltxfzu4u70Twi2Oo4Svk1p0OYNqEfsRHB3Pn3TEoq9PnpVguY4R25xRU89+UGhnVp\nycSB+gAupbyhRUQwr98wgMKyKu6flUVdnZ5Pt1JAFHpdneHBOatxiPDSNX1xOPRCqFLektquGU9c\nnsKizUVMm6/zkVopIAr9nSU7WL6jmCcuTyGhWZjVcZSynevPas/V/ROZNn8r32/Ya3WcgGX7Ql9f\nUMpL321mZI94rhmQaHUcpWxJRHj+yl70TojhgVlZetORRWxd6PsOVXHb+xm0jAjmf67qo2POlWpE\noUFO3vzVAIJcDm7/IIODlTVWRwo4ti30ancdd87IZH95NW/9Ko24qBCrIyllewnNwvjf6/qxc38F\nk2et1oukTcyWhW6M4cn0dazYeYCXx/Wld6LeQKRUUxnaOZbfX9qDHzbuZcoPW6yOE1BcVgfwNmMM\nf/rHZmYuz+W3Izrrs1qUssCkoUls3F3Gawuy6dY6isv66N/DpmCrI3RjDM99tZHpC7cxcVB7HrpI\n7wZVygoiwrNX9GJAh+Y8NGc16/L1TtKmYJtCr6sz/OHzdfztXzuYNDSJF67spePNlbJQiMvJmzcM\noHl4MLd/kEHhQb+YKsGv2aLQ80sOc/3by5ixdBd3jujMk5en6IgWpXxAXFQIb9+UxoGKGm77IFOn\nr2tkfl3oxhg+W5XPqKmLWZNXwotX9+bhi7tpmSvlQ3q2jWHqhFTW5JXw4Bwd+dKY/PKiaGVNLemr\nC/jw5xzW5pcyoENzplybSvuW4VZHU0odw8U9W/PIqO788ZtNdI6N4AG9vtUo/K7Q52bm8dxXGyip\nqCE5PpLnr+zFhIHtcer5cqV82m/O6cT2okO8uiCbdi3CGadz+Xqd3xV6bGQwQzq15MYhSQzu1EJP\nryjlJ+ofD9CbgpJKHvtkLW1iwjg7OdbqWLYiVk0flZaWZjIyfPKx6UqpRlRWWcO1b/5M/oHDzLlz\nCN1bR1sdya+ISKYx5hdTfIKfXxRVSvmf6NAg3r15IOEhTm5+dwUFJYetjmQbWuhKqSbXJiaMdycN\n4lClm5veWa6zHXmJFrpSyhIpbaN568Y0cvZX8Ov3M3SMuhdooSulLDOkc0umTkglc9cB7v5oJe7a\nOqsj+TUtdKWUpS7p3YZnxvTkh42FPDx3jd54dAb8btiiUsp+fjUkidLDNfzpH1uIDHXx9JieOiT5\nNGihK6V8wl3ndaGs0s1bi7cTFeridxd3tzqS39FCV0r5BBHhsdHdOVjpZvrCbYS6nNxzQbLVsfyK\nFrpSymeICM9d0Yuqmlr+/P0WXE4Hd47obHUsv6GFrpTyKU6H8PK4vrjrDC9+u4kgp/Dr4Z2sjuUX\ntNCVUj7H6RBeubYvtXX1s5DVGcPt5+iR+slooSulfJLL6WDqhFQQeOHrTVS767j7fD2nfiJa6Eop\nnxXkdDBtfCrBTgd/+scWqt11TL6wqw5pPA4tdKWUT3M5HfxpXF9cDuHVBdkcqqrl95f20DmDj0EL\nXSnl85wO4cWr+xAR4uKdJTsoOVzNS1f3weXUm92P1KD/GyIySkQ2i0i2iDx6jPcfEJENIrJGROaL\nSAfvR1VKBTKHQ3jy8hQeuIzcm0QAAAhcSURBVLArn6zM544ZK/WBXkc5aaGLiBOYDowGUoCJIpJy\n1GqrgDRjTB9gLvCSt4MqpZSIcO8FyTw7tifzN+3lur8upbhcH737bw05Qh8EZBtjthtjqoGPgbFH\nrmCMWWiMqfC8XAokejemUkr9n18NSeL16/qzvqCMq15fws595VZH8gkNKfQEIPeI13meZcdzK/DN\nsd4QkdtFJENEMoqKihqeUimljjK6dxs+uu0sSg/XcNUbP5Gxs9jqSJbz6hUFEbkBSANePtb7xpi3\njDFpxpi0uLg4b360UioADejQgk9+O4zoUBcT/7qU2StyT/6bbKwhhZ4PtDvidaJn2X8RkZHA48AY\nY0yVd+IppdSJdYyN4PO7zmZwp5Y8PG8Nz3yxIWAnymhIoa8AkkWko4gEAxOA9CNXEJF+wF+oL/NC\n78dUSqnjiwkP4t1JA7llWEfeWbKDG/62jMKDlVbHanInLXRjjBu4G/gO2AjMNsasF5FnRGSMZ7WX\ngUhgjohkiUj6cb6dUko1CpfTwROXp/DncX3Jyi3hslf/xfIdgXVeXYyxZrqntLQ0k5GRYclnK6Xs\nbdOeMu6csZJdxRU8cGFX7ji3M06b3FkqIpnGmLRjvae3WSmlbKd762jS7x7G6F6tefm7zVz316UU\nlBy2Olaj00JXStlSVGgQr03sx5/G9WVdfimjpi7m86x8rDor0RS00JVStiUiXDMgka/vG07n+Eju\n+ziL2z7IYE+pPS+YaqErpWyvQ8sI5t4xlN9f2oN/bt3HhVN+5O/Lcqits9fRuha6UiogOB31U9l9\nd/859GwbzeOfruPK15eQlVtidTSv0UJXSgWUpNgIZt42mKnjU9ldWskV05fw0JzVtrhoqs9DV0oF\nHBHhin4JXNAjntcWZPPekp2kry5g0tAkfjuiM83Cg62OeFp0HLpSKuDlHajgle+38OmqfCKCXdww\nuAO3nt2RuKgQq6P9wonGoWuhK6WUx6Y9ZUxfuI2v1hQQ5HRwzYBEbhySRLfWUVZH+w8tdKWUOgU7\n9pXzlx+38cmqfKrddQzq2ILrz2rPRSmtCQt2WppNC10ppU5DcXk1czJymbEsh9ziw4QHO7m4Z2su\n79uGoZ1jCQ1q+nLXQldKqTNQV2dYtqOY9NX5fLVmN2WVbkKDHAztHMu5XeMYmNSCbq2jmuR5MVro\nSinlJVXuWn7etp9Fm4tYsKmQXcX1s29GBDvpk9iMbq2j6BIfSee4SFrHhBIfFUJEiPcGFGqhK6VU\nIzDGkHfgMCt3HSAz5wCrc0vILjxEeXXtf60XHuwkPNhFiMtBSJCDySO7cnnftqf1mScqdB2HrpRS\np0lEaNcinHYtwhmbWj/VsjGG3aWVbC8qZ29ZJYUHq9h3qIqK6lqq3LVUuetoFh7UKHm00JVSyotE\nhLbNwmjbLKzJP1tv/VdKKZvQQldKKZvQQldKKZvQQldKKZvQQldKKZvQQldKKZvQQldKKZvQQldK\nKZuw7NZ/ESkCck7zt8cC+7wYx18E4nYH4jZDYG53IG4znPp2dzDGxB3rDcsK/UyISMbxnmVgZ4G4\n3YG4zRCY2x2I2wze3W495aKUUjahha6UUjbhr4X+ltUBLBKI2x2I2wyBud2BuM3gxe32y3PoSiml\nfslfj9CVUkodRQtdKaVswu8KXURGichmEckWkUetztMYRKSdiCwUkQ0isl5E7vMsbyEi34vIVs9/\nm1ud1dtExCkiq0TkS8/rjiKyzLO/Z4lIsNUZvU1EmonIXBHZJCIbRWRIgOzryZ4/3+tEZKaIhNpt\nf4vIOyJSKCLrjlh2zH0r9V71bPsaEel/qp/nV4UuIk5gOjAaSAEmikiKtakahRt40BiTAgwG7vJs\n56PAfGNMMjDf89pu7gM2HvH6RWCKMaYLcAC41ZJUjWsa8K0xpjvQl/rtt/W+FpEE4F4gzRjTC3AC\nE7Df/n4PGHXUsuPt29FAsufX7cAbp/phflXowCAg2xiz3RhTDXwMjLU4k9cZY3YbY1Z6vj5I/V/w\nBOq39X3Pau8DV1iTsHGISCJwKfC257UA5wNzPavYcZtjgHOAvwEYY6qNMSXYfF97uIAwEXEB4cBu\nbLa/jTGLgeKjFh9v344FPjD1lgLNRKTNqXyevxV6ApB7xOs8zzLbEpEkoB+wDGhljNnteWsP0Mqi\nWI1lKvAwUOd53RIoMca4Pa/tuL87AkXAu55TTW+LSAQ239fGmHzgT8Au6ou8FMjE/vsbjr9vz7jf\n/K3QA4qIRALzgPuNMWVHvmfqx5vaZsypiFwGFBpjMq3O0sRcQH/gDWNMP6Cco06v2G1fA3jOG4+l\n/h+0tkAEvzw1YXve3rf+Vuj5QLsjXid6ltmOiARRX+Z/N8Z84lm8998/gnn+W2hVvkYwDBgjIjup\nP5V2PvXnlpt5fiQHe+7vPCDPGLPM83ou9QVv530NMBLYYYwpMsbUAJ9Q/2fA7vsbjr9vz7jf/K3Q\nVwDJnivhwdRfREm3OJPXec4d/w3YaIx55Yi30oGbPF/fBHze1NkaizHmMWNMojEmifr9usAYcz2w\nELjGs5qtthnAGLMHyBWRbp5FFwAbsPG+9tgFDBaRcM+f939vt633t8fx9m06cKNntMtgoPSIUzMN\nY4zxq1/AJcAWYBvwuNV5Gmkbz6b+x7A1QJbn1yXUn1OeD2wFfgBaWJ21kbZ/BPCl5+tOwHIgG5gD\nhFidrxG2NxXI8Ozvz4DmgbCvgaeBTcA64EMgxG77G5hJ/TWCGup/Grv1ePsWEOpH8W0D1lI/AuiU\nPk9v/VdKKZvwt1MuSimljkMLXSmlbEILXSmlbEILXSmlbEILXSmlbEILXSmlbEILXSmlbOL/A0Js\nJ8UVncplAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7eiHeWJ7zSf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining how one training step will be computed.\n",
        "# train() is used in each epoch later\n",
        "sched = combine_scheds([0.3, 0.7], [sched_cos(1e-5, 1e-3), sched_cos(1e-3, 1e-6)])\n",
        "learning_rates = list()\n",
        "\n",
        "def train(train_loader, model, lm_criterion, crf_criterion, optimizer, epoch, vb_decoder, total_iters):\n",
        "    \"\"\"\n",
        "    train_loader: DataLoader for training data\n",
        "    model: LM_LSTM_CRF\n",
        "    lm_criterion:  nn.CrossEntropyLoss()\n",
        "    crf_criterion: ViterbiLoss()\n",
        "    optimizer: SGD/adam/adbound whatever your choice is\n",
        "    epoch: epoch number\n",
        "    vb_decoder: viterbi decoder(to decode and find F1 score)\n",
        "    \"\"\"\n",
        "    model.train()  # training mode, so dropout\n",
        "\n",
        "    batch_time = AverageMeter()  # forward prop. + back prop. time per batch\n",
        "    data_time = AverageMeter()  # data loading time per batch\n",
        "    ce_losses = AverageMeter()  # cross entropy loss\n",
        "    vb_losses = AverageMeter()  # viterbi loss\n",
        "    f1s = AverageMeter()  # f1 score\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    # Batches\n",
        "    for i, (wmaps, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, tmaps, wmap_lengths, cmap_lengths) in enumerate(train_loader):\n",
        "        data_time.update(time.time()-start)\n",
        "\n",
        "        max_word_len = max(wmap_lengths.tolist())\n",
        "        max_char_len = max(cmap_lengths.tolist())\n",
        "\n",
        "        # Reduce batch's padded length to maximum in-batch sequence. \n",
        "        # This saves some compute on nn.Linear layers (RNNs are not affected, since they don't compute over the pads)\n",
        "        wmaps = wmaps[:, :max_word_len].to(device)\n",
        "        cmaps_f = cmaps_f[:, :max_char_len].to(device)\n",
        "        cmaps_b = cmaps_b[:, :max_char_len].to(device)\n",
        "        cmarkers_f = cmarkers_f[:, :max_word_len].to(device)\n",
        "        cmarkers_b = cmarkers_b[:, :max_word_len].to(device)\n",
        "        tmaps = tmaps[:, :max_word_len].to(device)\n",
        "        wmap_lengths = wmap_lengths.to(device)\n",
        "        cmap_lengths = cmap_lengths.to(device)\n",
        "\n",
        "        # Forward prop.\n",
        "        crf_scores, lm_f_scores, lm_b_scores, wmaps_sorted, tmaps_sorted, wmap_lengths_sorted, _, __ = model(cmaps_f,\n",
        "                                                                                                             cmaps_b,\n",
        "                                                                                                             cmarkers_f,\n",
        "                                                                                                             cmarkers_b,\n",
        "                                                                                                             wmaps,\n",
        "                                                                                                             tmaps,\n",
        "                                                                                                             wmap_lengths,\n",
        "                                                                                                             cmap_lengths)\n",
        "        \n",
        "        # LM loss\n",
        "\n",
        "        # We don't predict the next word at the pads or <end> tokens\n",
        "        # Hence, only predict [word1, word2, word3, word4] among [word1, word2, word3, word4,<pad>,<pad>,<pad>,<pad>,<pad>,<end>]\n",
        "        # So prediction lengths are word sequence lengths -1\n",
        "        lm_lengths = wmap_lengths_sorted - 1 # (batch_size) the effective length of each row\n",
        "        lm_lengths = lm_lengths.tolist()\n",
        "\n",
        "        # Remove scores at timesteps we won't predict at\n",
        "        # pack_padded_sequence is a good trick to do this (145 PyTorch tricks(my other repo)---Trick #11)\n",
        "        lm_f_scores = pack_padded_sequence(lm_f_scores, lm_lengths, batch_first=True)\n",
        "        lm_b_scores = pack_padded_sequence(lm_b_scores, lm_lengths, batch_first=True)\n",
        "\n",
        "        # For the forward sequence, targets are from the second word onwards, up to <end>\n",
        "        # (timestep -> target) ...dunston -> checks, ...checks -> in, ...in -> <end>\n",
        "        lm_f_targets = wmaps_sorted[:, 1:]\n",
        "        lm_f_targets = pack_padded_sequence(lm_f_targets, lm_lengths, batch_first=True)\n",
        "\n",
        "        # For the backward sequence, targets are <end> followed by all words except the last word\n",
        "        # ...notsnud -> <end>, ...skcehc -> dunston, ...ni -> checks\n",
        "        lm_b_targets = torch.cat([torch.LongTensor([word_map['<end>']] * wmaps_sorted.size(0)).unsqueeze(1).to(device), \n",
        "                                  wmaps_sorted], dim=1)\n",
        "        lm_b_targets = pack_padded_sequence(lm_b_targets, lm_lengths, batch_first=True)\n",
        "        \n",
        "        # Calculate loss\n",
        "        ce_loss = lm_criterion(lm_f_scores.data, lm_f_targets.data) + lm_criterion(lm_b_scores.data, lm_b_targets.data)\n",
        "        vb_loss = crf_criterion(crf_scores, tmaps_sorted, wmap_lengths_sorted)\n",
        "        loss = ce_loss + vb_loss\n",
        "\n",
        "        # Learning rate annealing\n",
        "        current_iter = epoch*n_batches + i\n",
        "        new_lr = sched(current_iter/total_iters)\n",
        "        optimizer.param_groups[0]['lr'] = new_lr\n",
        "        learning_rates.append(new_lr)\n",
        "        print(f'the learning rate in Epoch {epoch}, step {i} is {new_lr}')\n",
        "\n",
        "        # Back-prop\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        if grad_clip is not None:\n",
        "            clip_gradient(optimizer, grad_clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        # Viterbi decode to find accuracy/F1\n",
        "        decoded = vb_decoder.decode(crf_scores.to(\"cpu\"), wmap_lengths_sorted.to(\"cpu\"))\n",
        "\n",
        "        # Remove timesteps we won't predict at, and also <end> tags, because to predict them would be cheating\n",
        "        decoded      = pack_padded_sequence(decoded, lm_lengths, batch_first=True)\n",
        "        tmaps_sorted = tmaps_sorted % vb_decoder.tagset_size  # actual target indices (see create_input_tensors())\n",
        "        tmaps_sorted = pack_padded_sequence(tmaps_sorted, lm_lengths, batch_first=True)\n",
        "\n",
        "        # Compute F1\n",
        "        f1 = f1_score(tmaps_sorted.data.to(\"cpu\").numpy(), decoded.data.numpy(), average='macro')\n",
        "\n",
        "        # Keep track of metrics\n",
        "        ce_losses.update(ce_loss.item(), sum(lm_lengths))\n",
        "        vb_losses.update(vb_loss.item(), crf_scores.size(0))\n",
        "        batch_time.update(time.time() - start)\n",
        "        f1s.update(f1, sum(lm_lengths))\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "        # Print training status\n",
        "        if i % print_freq == 0:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Data Load Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "                  'CE Loss {ce_loss.val:.4f} ({ce_loss.avg:.4f})\\t'\n",
        "                  'VB Loss {vb_loss.val:.4f} ({vb_loss.avg:.4f})\\t'\n",
        "                  'F1 {f1.val:.3f} ({f1.avg:.3f})'.format(epoch, i, len(train_loader),\n",
        "                                                          batch_time=batch_time,\n",
        "                                                          data_time=data_time, ce_loss=ce_losses,\n",
        "                                                          vb_loss=vb_losses, f1=f1s))\n",
        "def validate(val_loader, model, crf_criterion, vb_decoder):\n",
        "    \"\"\"\n",
        "    val_loader:    Dataloader for validation data\n",
        "    model:         Model\n",
        "    crf_criterion: Viterbi loss layer\n",
        "    return:        validation F1 score\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    batch_time = AverageMeter()\n",
        "    vb_losses  = AverageMeter()\n",
        "    f1s        = AverageMeter()\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    # validation loops\n",
        "    for i, (wmaps, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, tmaps, wmap_lengths, cmap_lengths) in enumerate(val_loader):\n",
        "        max_word_len = max(wmap_lengths.tolist())\n",
        "        max_char_len = max(cmap_lengths.tolist())\n",
        "\n",
        "        # Reduce batch's padded length to maximum in-batch sequence\n",
        "        # This saves some compute on nn.Linear layers (RNNs are unaffected, since they don't compute over the pads)\n",
        "        wmaps = wmaps[:, :max_word_len].to(device)\n",
        "        cmaps_f = cmaps_f[:, :max_char_len].to(device)\n",
        "        cmaps_b = cmaps_b[:, :max_char_len].to(device)\n",
        "        cmarkers_f = cmarkers_f[:, :max_word_len].to(device)\n",
        "        cmarkers_b = cmarkers_b[:, :max_word_len].to(device)\n",
        "        tmaps = tmaps[:, :max_word_len].to(device)\n",
        "        wmap_lengths = wmap_lengths.to(device)\n",
        "        cmap_lengths = cmap_lengths.to(device)\n",
        "\n",
        "        # Forward prop.\n",
        "        crf_scores, wmaps_sorted, tmaps_sorted, wmap_lengths_sorted, _, __ = model(cmaps_f,\n",
        "                                                                                    cmaps_b,\n",
        "                                                                                    cmarkers_f,\n",
        "                                                                                    cmarkers_b,\n",
        "                                                                                    wmaps,\n",
        "                                                                                    tmaps,\n",
        "                                                                                    wmap_lengths,\n",
        "                                                                                    cmap_lengths)\n",
        "\n",
        "        # Viterbi / CRF layer loss\n",
        "        vb_loss = crf_criterion(crf_scores, tmaps_sorted, wmap_lengths_sorted)\n",
        "\n",
        "        # Viterbi decode to find accuracy / f1\n",
        "        decoded = vb_decoder.decode(crf_scores.to(\"cpu\"), wmap_lengths_sorted.to(\"cpu\"))\n",
        "\n",
        "        # Remove timesteps we won't predict at, and also <end> tags, because to predict them would be cheating\n",
        "        decoded      = pack_padded_sequence(decoded, (wmap_lengths_sorted - 1).tolist(), batch_first=True)\n",
        "        tmaps_sorted = tmaps_sorted % vb_decoder.tagset_size  # actual target indices (see create_input_tensors())\n",
        "        tmaps_sorted = pack_padded_sequence(tmaps_sorted, (wmap_lengths_sorted - 1).tolist(), batch_first=True)\n",
        "\n",
        "        # f1\n",
        "        f1 = f1_score(tmaps_sorted.data.to(\"cpu\").numpy(), decoded.data.numpy(), average='macro')\n",
        "\n",
        "        # Keep track of metrics\n",
        "        vb_losses.update(vb_loss.item(), crf_scores.size(0))\n",
        "        f1s.update(f1, sum((wmap_lengths_sorted - 1).tolist()))\n",
        "        batch_time.update(time.time() - start)\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "        if i % print_freq == 0:\n",
        "            print('Validation: [{0}/{1}]\\t'\n",
        "                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'VB Loss {vb_loss.val:.4f} ({vb_loss.avg:.4f})\\t'\n",
        "                  'F1 Score {f1.val:.3f} ({f1.avg:.3f})\\t'.format(i, len(val_loader), batch_time=batch_time,\n",
        "                                                                  vb_loss=vb_losses, f1=f1s))\n",
        "\n",
        "    print(\n",
        "        '\\n * LOSS - {vb_loss.avg:.3f}, F1 SCORE - {f1.avg:.3f}\\n'.format(vb_loss=vb_losses,\n",
        "                                                                          f1=f1s))\n",
        "\n",
        "    return f1s.avg    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCUtUGyqDAuu",
        "colab_type": "code",
        "outputId": "964908bb-3ca3-40f3-ba0c-6bc590f2a71e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Loss funcitons\n",
        "lm_criterion  = nn.CrossEntropyLoss().to(device)\n",
        "crf_criterion = ViterbiLoss(tag_map).to(device)\n",
        "\n",
        "# Since the language model's vocab is restricted on in-corpus indices, encode training/val with only these!\n",
        "# word_map might have been expanded, and in-corpus words eliminated due to low frequency might still be added because\n",
        "# they exist in the pre-trained embeddings.(these embeddings are added after the <unk> token)\n",
        "temp_word_map = {k: v for k, v in word_map.items() if v <= word_map['<unk>']}\n",
        "\n",
        "# train_input = (padded_wmaps, padded_cmaps_f, padded_cmaps_b, padded_cmarkers_f, padded_cmarkers_b, \n",
        "#                padded_tmaps, wmap_lengths,   cmap_lengths)\n",
        "train_inputs = create_input_tensors(train_words, train_tags, temp_word_map, char_map,\n",
        "                                        tag_map)\n",
        "val_inputs = create_input_tensors(val_words, val_tags, temp_word_map, char_map, tag_map)\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = torch.utils.data.DataLoader(WCDataset(*train_inputs), batch_size=batch_size, shuffle=True,\n",
        "                                            num_workers=workers, pin_memory=False)\n",
        "val_loader = torch.utils.data.DataLoader(WCDataset(*val_inputs), batch_size=batch_size, shuffle=True,\n",
        "                                             num_workers=workers, pin_memory=False)\n",
        "\n",
        "# Viterbi decoder (to find accuracy during validation)\n",
        "vb_decoder = ViterbiDecoder(tag_map)\n",
        "\n",
        "# Calculate total number of iterations for learning rate annealing\n",
        "n_batches   = len(train_loader)\n",
        "total_iters = epochs*n_batches\n",
        "\n",
        "# Epochs\n",
        "for epoch in range(start_epoch, epochs):\n",
        "    # One epoch's training\n",
        "    train(train_loader  = train_loader,\n",
        "          model         = model,\n",
        "          lm_criterion  = lm_criterion,\n",
        "          crf_criterion = crf_criterion,\n",
        "          optimizer     = optimizer,\n",
        "          epoch         = epoch,\n",
        "          vb_decoder    = vb_decoder,\n",
        "          total_iters   = total_iters\n",
        "          )\n",
        "    # One epoch's validation\n",
        "    val_f1 = validate(val_loader = val_loader,\n",
        "                      model      = model,\n",
        "                      crf_criterion = crf_criterion,\n",
        "                      vb_decoder    = vb_decoder)\n",
        "    is_best = val_f1 > best_f1\n",
        "    if not is_best:\n",
        "        epochs_since_improvement += 1\n",
        "        print(\"\\nEpochs since improvement: %d\\n\" % (epochs_since_improvement,))\n",
        "    else:\n",
        "        epochs_since_improvement = 0\n",
        "    \n",
        "    # Save checkpoint\n",
        "    save_checkpoint(epoch, model, optimizer, val_f1, word_map, char_map, tag_map, lm_vocab_size, is_best)\n",
        "\n",
        "    # Decay learning rate every epoch\n",
        "    # adjust_learning_rate(optimizer, lr / (1 + (epoch + 1) * lr_decay))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: [0][0/1405]\tBatch Time 0.500 (0.500)\tData Load Time 0.185 (0.185)\tCE Loss 16.8818 (16.8818)\tVB Loss 41.2149 (41.2149)\tF1 0.028 (0.028)\n",
            "Epoch: [0][100/1405]\tBatch Time 0.226 (0.231)\tData Load Time 0.004 (0.007)\tCE Loss 16.8969 (16.8904)\tVB Loss 31.8005 (34.2676)\tF1 0.061 (0.040)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: [0][200/1405]\tBatch Time 0.180 (0.225)\tData Load Time 0.005 (0.006)\tCE Loss 16.8835 (16.8897)\tVB Loss 23.8137 (33.4883)\tF1 0.115 (0.059)\n",
            "Epoch: [0][300/1405]\tBatch Time 0.242 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 16.8948 (16.8894)\tVB Loss 29.4646 (32.4402)\tF1 0.123 (0.080)\n",
            "Epoch: [0][400/1405]\tBatch Time 0.221 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 16.8823 (16.8890)\tVB Loss 36.5122 (31.4790)\tF1 0.186 (0.104)\n",
            "Epoch: [0][500/1405]\tBatch Time 0.174 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 16.8840 (16.8887)\tVB Loss 17.7743 (30.5700)\tF1 0.213 (0.121)\n",
            "Epoch: [0][600/1405]\tBatch Time 0.268 (0.223)\tData Load Time 0.006 (0.005)\tCE Loss 16.8856 (16.8884)\tVB Loss 30.8625 (29.5563)\tF1 0.188 (0.131)\n",
            "Epoch: [0][700/1405]\tBatch Time 0.230 (0.221)\tData Load Time 0.004 (0.005)\tCE Loss 16.8878 (16.8880)\tVB Loss 25.3025 (28.5458)\tF1 0.184 (0.139)\n",
            "Epoch: [0][800/1405]\tBatch Time 0.176 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 16.8761 (16.8877)\tVB Loss 17.5773 (27.7713)\tF1 0.188 (0.145)\n",
            "Epoch: [0][900/1405]\tBatch Time 0.202 (0.221)\tData Load Time 0.005 (0.005)\tCE Loss 16.8867 (16.8874)\tVB Loss 22.5374 (26.8556)\tF1 0.188 (0.150)\n",
            "Epoch: [0][1000/1405]\tBatch Time 0.235 (0.221)\tData Load Time 0.004 (0.005)\tCE Loss 16.8885 (16.8869)\tVB Loss 17.4771 (26.0562)\tF1 0.181 (0.154)\n",
            "Epoch: [0][1100/1405]\tBatch Time 0.204 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 16.8876 (16.8866)\tVB Loss 12.6695 (25.2537)\tF1 0.233 (0.157)\n",
            "Epoch: [0][1200/1405]\tBatch Time 0.221 (0.221)\tData Load Time 0.004 (0.005)\tCE Loss 16.8800 (16.8863)\tVB Loss 14.9610 (24.4816)\tF1 0.175 (0.159)\n",
            "Epoch: [0][1300/1405]\tBatch Time 0.236 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 16.8888 (16.8860)\tVB Loss 15.3096 (23.7971)\tF1 0.176 (0.162)\n",
            "Epoch: [0][1400/1405]\tBatch Time 0.200 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 16.8799 (16.8856)\tVB Loss 12.5507 (23.1329)\tF1 0.185 (0.163)\n",
            "Validation: [0/325]\tBatch Time 0.275 (0.275)\tVB Loss 10.3855 (10.3855)\tF1 Score 0.173 (0.173)\t\n",
            "Validation: [100/325]\tBatch Time 0.096 (0.112)\tVB Loss 10.4248 (14.6745)\tF1 Score 0.223 (0.187)\t\n",
            "Validation: [200/325]\tBatch Time 0.094 (0.109)\tVB Loss 12.0265 (14.5414)\tF1 Score 0.192 (0.189)\t\n",
            "Validation: [300/325]\tBatch Time 0.059 (0.108)\tVB Loss 9.0912 (14.4231)\tF1 Score 0.220 (0.189)\t\n",
            "\n",
            " * LOSS - 14.501, F1 SCORE - 0.189\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LM_LSTM_CRF. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type CRF. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Highway. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ModuleList. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.001429\n",
            "\n",
            "Epoch: [1][0/1405]\tBatch Time 0.604 (0.604)\tData Load Time 0.203 (0.203)\tCE Loss 16.8792 (16.8792)\tVB Loss 15.3204 (15.3204)\tF1 0.184 (0.184)\n",
            "Epoch: [1][100/1405]\tBatch Time 0.256 (0.251)\tData Load Time 0.004 (0.007)\tCE Loss 16.8759 (16.8815)\tVB Loss 14.0225 (14.0400)\tF1 0.183 (0.185)\n",
            "Epoch: [1][200/1405]\tBatch Time 0.207 (0.241)\tData Load Time 0.004 (0.006)\tCE Loss 16.8719 (16.8807)\tVB Loss 15.6933 (13.6453)\tF1 0.178 (0.187)\n",
            "Epoch: [1][300/1405]\tBatch Time 0.208 (0.235)\tData Load Time 0.005 (0.006)\tCE Loss 16.8801 (16.8800)\tVB Loss 12.8792 (13.2602)\tF1 0.227 (0.188)\n",
            "Epoch: [1][400/1405]\tBatch Time 0.264 (0.232)\tData Load Time 0.005 (0.005)\tCE Loss 16.8625 (16.8795)\tVB Loss 9.7027 (13.1231)\tF1 0.184 (0.188)\n",
            "Epoch: [1][500/1405]\tBatch Time 0.253 (0.232)\tData Load Time 0.004 (0.005)\tCE Loss 16.8727 (16.8790)\tVB Loss 11.7363 (12.9029)\tF1 0.183 (0.188)\n",
            "Epoch: [1][600/1405]\tBatch Time 0.241 (0.230)\tData Load Time 0.005 (0.005)\tCE Loss 16.8805 (16.8786)\tVB Loss 11.0543 (12.8315)\tF1 0.178 (0.188)\n",
            "Epoch: [1][700/1405]\tBatch Time 0.113 (0.228)\tData Load Time 0.005 (0.005)\tCE Loss 16.8769 (16.8781)\tVB Loss 6.9559 (12.6919)\tF1 0.225 (0.187)\n",
            "Epoch: [1][800/1405]\tBatch Time 0.182 (0.226)\tData Load Time 0.005 (0.005)\tCE Loss 16.8706 (16.8778)\tVB Loss 8.2881 (12.5602)\tF1 0.181 (0.188)\n",
            "Epoch: [1][900/1405]\tBatch Time 0.189 (0.225)\tData Load Time 0.005 (0.005)\tCE Loss 16.8734 (16.8773)\tVB Loss 10.9073 (12.4750)\tF1 0.179 (0.188)\n",
            "Epoch: [1][1000/1405]\tBatch Time 0.255 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 16.8772 (16.8767)\tVB Loss 13.4250 (12.3999)\tF1 0.180 (0.188)\n",
            "Epoch: [1][1100/1405]\tBatch Time 0.207 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 16.8729 (16.8764)\tVB Loss 10.2152 (12.3111)\tF1 0.184 (0.187)\n",
            "Epoch: [1][1200/1405]\tBatch Time 0.205 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 16.8793 (16.8760)\tVB Loss 11.7976 (12.1890)\tF1 0.179 (0.187)\n",
            "Epoch: [1][1300/1405]\tBatch Time 0.247 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 16.8637 (16.8755)\tVB Loss 16.0404 (12.1105)\tF1 0.169 (0.188)\n",
            "Epoch: [1][1400/1405]\tBatch Time 0.216 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 16.8787 (16.8751)\tVB Loss 12.2128 (12.0171)\tF1 0.170 (0.188)\n",
            "Validation: [0/325]\tBatch Time 0.331 (0.331)\tVB Loss 10.6113 (10.6113)\tF1 Score 0.186 (0.186)\t\n",
            "Validation: [100/325]\tBatch Time 0.107 (0.111)\tVB Loss 6.2304 (11.6750)\tF1 Score 0.192 (0.190)\t\n",
            "Validation: [200/325]\tBatch Time 0.115 (0.110)\tVB Loss 10.2360 (11.8953)\tF1 Score 0.183 (0.188)\t\n",
            "Validation: [300/325]\tBatch Time 0.124 (0.109)\tVB Loss 15.2569 (11.9961)\tF1 Score 0.173 (0.187)\t\n",
            "\n",
            " * LOSS - 11.974, F1 SCORE - 0.187\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.001364\n",
            "\n",
            "Epoch: [2][0/1405]\tBatch Time 0.517 (0.517)\tData Load Time 0.192 (0.192)\tCE Loss 16.8548 (16.8548)\tVB Loss 11.3869 (11.3869)\tF1 0.225 (0.225)\n",
            "Epoch: [2][100/1405]\tBatch Time 0.230 (0.247)\tData Load Time 0.005 (0.007)\tCE Loss 16.8691 (16.8683)\tVB Loss 11.3019 (11.2041)\tF1 0.183 (0.188)\n",
            "Epoch: [2][200/1405]\tBatch Time 0.260 (0.235)\tData Load Time 0.004 (0.006)\tCE Loss 16.8711 (16.8678)\tVB Loss 11.9751 (11.1117)\tF1 0.189 (0.188)\n",
            "Epoch: [2][300/1405]\tBatch Time 0.272 (0.235)\tData Load Time 0.006 (0.005)\tCE Loss 16.8559 (16.8675)\tVB Loss 11.0760 (11.0948)\tF1 0.185 (0.189)\n",
            "Epoch: [2][400/1405]\tBatch Time 0.195 (0.231)\tData Load Time 0.004 (0.005)\tCE Loss 16.8634 (16.8671)\tVB Loss 11.9382 (11.0078)\tF1 0.186 (0.189)\n",
            "Epoch: [2][500/1405]\tBatch Time 0.154 (0.229)\tData Load Time 0.004 (0.005)\tCE Loss 16.8626 (16.8664)\tVB Loss 11.1334 (11.1034)\tF1 0.169 (0.188)\n",
            "Epoch: [2][600/1405]\tBatch Time 0.228 (0.227)\tData Load Time 0.005 (0.005)\tCE Loss 16.8787 (16.8660)\tVB Loss 12.1834 (10.9915)\tF1 0.177 (0.188)\n",
            "Epoch: [2][700/1405]\tBatch Time 0.214 (0.226)\tData Load Time 0.004 (0.005)\tCE Loss 16.8740 (16.8657)\tVB Loss 10.6808 (10.9170)\tF1 0.152 (0.188)\n",
            "Epoch: [2][800/1405]\tBatch Time 0.220 (0.226)\tData Load Time 0.005 (0.005)\tCE Loss 16.8565 (16.8654)\tVB Loss 19.0950 (10.9597)\tF1 0.160 (0.188)\n",
            "Epoch: [2][900/1405]\tBatch Time 0.180 (0.224)\tData Load Time 0.004 (0.005)\tCE Loss 16.8422 (16.8649)\tVB Loss 7.0132 (10.9169)\tF1 0.229 (0.188)\n",
            "Epoch: [2][1000/1405]\tBatch Time 0.218 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 16.8574 (16.8643)\tVB Loss 8.5208 (10.8216)\tF1 0.227 (0.188)\n",
            "Epoch: [2][1100/1405]\tBatch Time 0.167 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 16.8545 (16.8639)\tVB Loss 5.2839 (10.8482)\tF1 0.235 (0.188)\n",
            "Epoch: [2][1200/1405]\tBatch Time 0.245 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 16.8646 (16.8632)\tVB Loss 10.7398 (10.7742)\tF1 0.233 (0.188)\n",
            "Epoch: [2][1300/1405]\tBatch Time 0.222 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 16.8514 (16.8627)\tVB Loss 11.0604 (10.7642)\tF1 0.182 (0.188)\n",
            "Epoch: [2][1400/1405]\tBatch Time 0.191 (0.221)\tData Load Time 0.005 (0.005)\tCE Loss 16.8507 (16.8622)\tVB Loss 8.3154 (10.7153)\tF1 0.182 (0.188)\n",
            "Validation: [0/325]\tBatch Time 0.293 (0.293)\tVB Loss 7.6766 (7.6766)\tF1 Score 0.185 (0.185)\t\n",
            "Validation: [100/325]\tBatch Time 0.112 (0.108)\tVB Loss 9.2640 (11.1831)\tF1 Score 0.222 (0.187)\t\n",
            "Validation: [200/325]\tBatch Time 0.073 (0.112)\tVB Loss 4.6097 (11.0136)\tF1 Score 0.233 (0.187)\t\n",
            "Validation: [300/325]\tBatch Time 0.133 (0.115)\tVB Loss 9.9803 (10.9613)\tF1 Score 0.148 (0.187)\t\n",
            "\n",
            " * LOSS - 11.017, F1 SCORE - 0.187\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.001304\n",
            "\n",
            "Epoch: [3][0/1405]\tBatch Time 0.495 (0.495)\tData Load Time 0.206 (0.206)\tCE Loss 16.8498 (16.8498)\tVB Loss 9.9626 (9.9626)\tF1 0.183 (0.183)\n",
            "Epoch: [3][100/1405]\tBatch Time 0.275 (0.249)\tData Load Time 0.006 (0.007)\tCE Loss 16.8286 (16.8526)\tVB Loss 12.5538 (10.0492)\tF1 0.168 (0.187)\n",
            "Epoch: [3][200/1405]\tBatch Time 0.147 (0.236)\tData Load Time 0.004 (0.006)\tCE Loss 16.8407 (16.8528)\tVB Loss 8.4511 (10.1977)\tF1 0.173 (0.189)\n",
            "Epoch: [3][300/1405]\tBatch Time 0.276 (0.228)\tData Load Time 0.006 (0.006)\tCE Loss 16.8531 (16.8523)\tVB Loss 8.3882 (10.2040)\tF1 0.185 (0.188)\n",
            "Epoch: [3][400/1405]\tBatch Time 0.130 (0.226)\tData Load Time 0.004 (0.005)\tCE Loss 16.8555 (16.8517)\tVB Loss 8.5470 (10.1252)\tF1 0.207 (0.187)\n",
            "Epoch: [3][500/1405]\tBatch Time 0.269 (0.225)\tData Load Time 0.004 (0.005)\tCE Loss 16.8587 (16.8509)\tVB Loss 15.8664 (10.1240)\tF1 0.178 (0.187)\n",
            "Epoch: [3][600/1405]\tBatch Time 0.269 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 16.8322 (16.8506)\tVB Loss 9.7917 (10.1297)\tF1 0.179 (0.187)\n",
            "Epoch: [3][700/1405]\tBatch Time 0.264 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 16.8659 (16.8502)\tVB Loss 14.0678 (10.1044)\tF1 0.171 (0.187)\n",
            "Epoch: [3][800/1405]\tBatch Time 0.222 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 16.8462 (16.8495)\tVB Loss 7.6784 (10.0421)\tF1 0.185 (0.187)\n",
            "Epoch: [3][900/1405]\tBatch Time 0.244 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 16.8507 (16.8491)\tVB Loss 9.2648 (10.0829)\tF1 0.182 (0.187)\n",
            "Epoch: [3][1000/1405]\tBatch Time 0.180 (0.221)\tData Load Time 0.004 (0.005)\tCE Loss 16.8390 (16.8484)\tVB Loss 12.8443 (10.0446)\tF1 0.171 (0.187)\n",
            "Epoch: [3][1100/1405]\tBatch Time 0.254 (0.220)\tData Load Time 0.005 (0.005)\tCE Loss 16.8460 (16.8476)\tVB Loss 9.8470 (10.0086)\tF1 0.186 (0.187)\n",
            "Epoch: [3][1200/1405]\tBatch Time 0.187 (0.221)\tData Load Time 0.005 (0.005)\tCE Loss 16.8502 (16.8469)\tVB Loss 12.6466 (10.0202)\tF1 0.165 (0.187)\n",
            "Epoch: [3][1300/1405]\tBatch Time 0.181 (0.220)\tData Load Time 0.004 (0.005)\tCE Loss 16.8325 (16.8461)\tVB Loss 6.6726 (10.0295)\tF1 0.188 (0.187)\n",
            "Epoch: [3][1400/1405]\tBatch Time 0.208 (0.220)\tData Load Time 0.005 (0.005)\tCE Loss 16.8414 (16.8454)\tVB Loss 6.5873 (10.0212)\tF1 0.185 (0.187)\n",
            "Validation: [0/325]\tBatch Time 0.273 (0.273)\tVB Loss 9.3980 (9.3980)\tF1 Score 0.184 (0.184)\t\n",
            "Validation: [100/325]\tBatch Time 0.093 (0.108)\tVB Loss 14.3725 (10.5670)\tF1 Score 0.175 (0.186)\t\n",
            "Validation: [200/325]\tBatch Time 0.108 (0.108)\tVB Loss 12.0534 (10.4535)\tF1 Score 0.176 (0.186)\t\n",
            "Validation: [300/325]\tBatch Time 0.129 (0.115)\tVB Loss 9.2862 (10.4081)\tF1 Score 0.181 (0.187)\t\n",
            "\n",
            " * LOSS - 10.403, F1 SCORE - 0.186\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.001250\n",
            "\n",
            "Epoch: [4][0/1405]\tBatch Time 0.517 (0.517)\tData Load Time 0.214 (0.214)\tCE Loss 16.8426 (16.8426)\tVB Loss 15.1980 (15.1980)\tF1 0.179 (0.179)\n",
            "Epoch: [4][100/1405]\tBatch Time 0.245 (0.249)\tData Load Time 0.004 (0.007)\tCE Loss 16.8524 (16.8353)\tVB Loss 13.0888 (9.9173)\tF1 0.178 (0.185)\n",
            "Epoch: [4][200/1405]\tBatch Time 0.164 (0.236)\tData Load Time 0.011 (0.006)\tCE Loss 16.8368 (16.8352)\tVB Loss 5.0024 (9.8844)\tF1 0.182 (0.187)\n",
            "Epoch: [4][300/1405]\tBatch Time 0.213 (0.232)\tData Load Time 0.006 (0.006)\tCE Loss 16.8179 (16.8335)\tVB Loss 10.0490 (9.8737)\tF1 0.180 (0.187)\n",
            "Epoch: [4][400/1405]\tBatch Time 0.226 (0.231)\tData Load Time 0.004 (0.006)\tCE Loss 16.8230 (16.8319)\tVB Loss 7.2224 (9.8931)\tF1 0.190 (0.186)\n",
            "Epoch: [4][500/1405]\tBatch Time 0.239 (0.229)\tData Load Time 0.005 (0.005)\tCE Loss 16.8229 (16.8310)\tVB Loss 10.0002 (9.7472)\tF1 0.185 (0.187)\n",
            "Epoch: [4][600/1405]\tBatch Time 0.217 (0.228)\tData Load Time 0.005 (0.005)\tCE Loss 16.8517 (16.8306)\tVB Loss 14.6894 (9.7922)\tF1 0.174 (0.187)\n",
            "Epoch: [4][700/1405]\tBatch Time 0.277 (0.226)\tData Load Time 0.005 (0.005)\tCE Loss 16.7800 (16.8295)\tVB Loss 12.5501 (9.7355)\tF1 0.167 (0.188)\n",
            "Epoch: [4][800/1405]\tBatch Time 0.258 (0.225)\tData Load Time 0.005 (0.005)\tCE Loss 16.8250 (16.8288)\tVB Loss 8.5609 (9.6949)\tF1 0.180 (0.188)\n",
            "Epoch: [4][900/1405]\tBatch Time 0.192 (0.224)\tData Load Time 0.004 (0.005)\tCE Loss 16.7974 (16.8281)\tVB Loss 8.2891 (9.6210)\tF1 0.183 (0.188)\n",
            "Epoch: [4][1000/1405]\tBatch Time 0.284 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 16.8321 (16.8275)\tVB Loss 10.1831 (9.6393)\tF1 0.185 (0.188)\n",
            "Epoch: [4][1100/1405]\tBatch Time 0.194 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 16.8316 (16.8266)\tVB Loss 6.1231 (9.6129)\tF1 0.236 (0.188)\n",
            "Epoch: [4][1200/1405]\tBatch Time 0.152 (0.222)\tData Load Time 0.006 (0.005)\tCE Loss 16.7758 (16.8258)\tVB Loss 6.9234 (9.5847)\tF1 0.179 (0.188)\n",
            "Epoch: [4][1300/1405]\tBatch Time 0.265 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 16.7910 (16.8250)\tVB Loss 9.3341 (9.5846)\tF1 0.181 (0.188)\n",
            "Epoch: [4][1400/1405]\tBatch Time 0.232 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 16.8286 (16.8240)\tVB Loss 9.2724 (9.5575)\tF1 0.186 (0.188)\n",
            "Validation: [0/325]\tBatch Time 0.351 (0.351)\tVB Loss 14.1154 (14.1154)\tF1 Score 0.183 (0.183)\t\n",
            "Validation: [100/325]\tBatch Time 0.105 (0.114)\tVB Loss 9.8403 (10.0806)\tF1 Score 0.146 (0.187)\t\n",
            "Validation: [200/325]\tBatch Time 0.093 (0.113)\tVB Loss 8.9061 (9.9461)\tF1 Score 0.182 (0.188)\t\n",
            "Validation: [300/325]\tBatch Time 0.113 (0.112)\tVB Loss 13.0832 (9.9652)\tF1 Score 0.269 (0.188)\t\n",
            "\n",
            " * LOSS - 10.007, F1 SCORE - 0.188\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.001200\n",
            "\n",
            "Epoch: [5][0/1405]\tBatch Time 0.482 (0.482)\tData Load Time 0.201 (0.201)\tCE Loss 16.7998 (16.7998)\tVB Loss 13.7950 (13.7950)\tF1 0.171 (0.171)\n",
            "Epoch: [5][100/1405]\tBatch Time 0.198 (0.242)\tData Load Time 0.005 (0.007)\tCE Loss 16.8191 (16.8138)\tVB Loss 8.2214 (9.5736)\tF1 0.183 (0.187)\n",
            "Epoch: [5][200/1405]\tBatch Time 0.291 (0.233)\tData Load Time 0.006 (0.006)\tCE Loss 16.8058 (16.8099)\tVB Loss 9.4236 (9.3520)\tF1 0.183 (0.187)\n",
            "Epoch: [5][300/1405]\tBatch Time 0.226 (0.229)\tData Load Time 0.005 (0.006)\tCE Loss 16.7937 (16.8080)\tVB Loss 7.6561 (9.3591)\tF1 0.184 (0.187)\n",
            "Epoch: [5][400/1405]\tBatch Time 0.270 (0.226)\tData Load Time 0.005 (0.005)\tCE Loss 16.7900 (16.8071)\tVB Loss 14.3719 (9.3723)\tF1 0.175 (0.188)\n",
            "Epoch: [5][500/1405]\tBatch Time 0.082 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 16.7904 (16.8060)\tVB Loss 2.7194 (9.2927)\tF1 0.313 (0.189)\n",
            "Epoch: [5][600/1405]\tBatch Time 0.189 (0.221)\tData Load Time 0.005 (0.005)\tCE Loss 16.7610 (16.8050)\tVB Loss 5.9938 (9.3202)\tF1 0.183 (0.189)\n",
            "Epoch: [5][700/1405]\tBatch Time 0.252 (0.220)\tData Load Time 0.004 (0.005)\tCE Loss 16.8241 (16.8040)\tVB Loss 8.1144 (9.3142)\tF1 0.187 (0.189)\n",
            "Epoch: [5][800/1405]\tBatch Time 0.342 (0.220)\tData Load Time 0.006 (0.005)\tCE Loss 16.7823 (16.8027)\tVB Loss 7.0605 (9.2701)\tF1 0.193 (0.189)\n",
            "Epoch: [5][900/1405]\tBatch Time 0.208 (0.219)\tData Load Time 0.005 (0.005)\tCE Loss 16.7969 (16.8015)\tVB Loss 8.0069 (9.2923)\tF1 0.183 (0.189)\n",
            "Epoch: [5][1000/1405]\tBatch Time 0.152 (0.218)\tData Load Time 0.004 (0.005)\tCE Loss 16.8028 (16.8007)\tVB Loss 4.9002 (9.2745)\tF1 0.233 (0.190)\n",
            "Epoch: [5][1100/1405]\tBatch Time 0.211 (0.219)\tData Load Time 0.004 (0.005)\tCE Loss 16.7872 (16.7996)\tVB Loss 18.5525 (9.3082)\tF1 0.154 (0.189)\n",
            "Epoch: [5][1200/1405]\tBatch Time 0.267 (0.218)\tData Load Time 0.005 (0.005)\tCE Loss 16.7664 (16.7983)\tVB Loss 17.6326 (9.2885)\tF1 0.164 (0.189)\n",
            "Epoch: [5][1300/1405]\tBatch Time 0.240 (0.218)\tData Load Time 0.005 (0.005)\tCE Loss 16.7618 (16.7970)\tVB Loss 10.6190 (9.2734)\tF1 0.184 (0.189)\n",
            "Epoch: [5][1400/1405]\tBatch Time 0.163 (0.218)\tData Load Time 0.005 (0.005)\tCE Loss 16.8032 (16.7958)\tVB Loss 9.4975 (9.2253)\tF1 0.176 (0.189)\n",
            "Validation: [0/325]\tBatch Time 0.337 (0.337)\tVB Loss 11.9065 (11.9065)\tF1 Score 0.179 (0.179)\t\n",
            "Validation: [100/325]\tBatch Time 0.123 (0.121)\tVB Loss 13.7792 (9.4848)\tF1 Score 0.182 (0.191)\t\n",
            "Validation: [200/325]\tBatch Time 0.146 (0.121)\tVB Loss 8.8532 (9.4991)\tF1 Score 0.229 (0.190)\t\n",
            "Validation: [300/325]\tBatch Time 0.146 (0.123)\tVB Loss 11.9379 (9.6377)\tF1 Score 0.183 (0.190)\t\n",
            "\n",
            " * LOSS - 9.688, F1 SCORE - 0.190\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.001154\n",
            "\n",
            "Epoch: [6][0/1405]\tBatch Time 0.408 (0.408)\tData Load Time 0.193 (0.193)\tCE Loss 16.8055 (16.8055)\tVB Loss 5.5923 (5.5923)\tF1 0.235 (0.235)\n",
            "Epoch: [6][100/1405]\tBatch Time 0.254 (0.249)\tData Load Time 0.005 (0.007)\tCE Loss 16.7839 (16.7779)\tVB Loss 10.6912 (9.4788)\tF1 0.184 (0.190)\n",
            "Epoch: [6][200/1405]\tBatch Time 0.130 (0.238)\tData Load Time 0.004 (0.006)\tCE Loss 16.7075 (16.7762)\tVB Loss 5.7502 (9.3568)\tF1 0.174 (0.191)\n",
            "Epoch: [6][300/1405]\tBatch Time 0.244 (0.230)\tData Load Time 0.005 (0.006)\tCE Loss 16.7822 (16.7747)\tVB Loss 13.2642 (9.1102)\tF1 0.179 (0.191)\n",
            "Epoch: [6][400/1405]\tBatch Time 0.191 (0.228)\tData Load Time 0.004 (0.005)\tCE Loss 16.7856 (16.7735)\tVB Loss 6.3646 (9.0535)\tF1 0.189 (0.191)\n",
            "Epoch: [6][500/1405]\tBatch Time 0.225 (0.226)\tData Load Time 0.004 (0.005)\tCE Loss 16.7847 (16.7729)\tVB Loss 12.5054 (8.9998)\tF1 0.182 (0.191)\n",
            "Epoch: [6][600/1405]\tBatch Time 0.131 (0.225)\tData Load Time 0.004 (0.005)\tCE Loss 16.7563 (16.7715)\tVB Loss 7.2852 (9.0349)\tF1 0.177 (0.191)\n",
            "Epoch: [6][700/1405]\tBatch Time 0.195 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 16.7492 (16.7694)\tVB Loss 7.7042 (9.0082)\tF1 0.179 (0.191)\n",
            "Epoch: [6][800/1405]\tBatch Time 0.196 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 16.7245 (16.7679)\tVB Loss 6.1654 (8.9929)\tF1 0.185 (0.191)\n",
            "Epoch: [6][900/1405]\tBatch Time 0.191 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 16.7183 (16.7665)\tVB Loss 9.0939 (9.0212)\tF1 0.183 (0.191)\n",
            "Epoch: [6][1000/1405]\tBatch Time 0.175 (0.222)\tData Load Time 0.006 (0.005)\tCE Loss 16.7569 (16.7652)\tVB Loss 3.7756 (8.9765)\tF1 0.237 (0.191)\n",
            "Epoch: [6][1100/1405]\tBatch Time 0.253 (0.222)\tData Load Time 0.006 (0.005)\tCE Loss 16.7639 (16.7638)\tVB Loss 9.1601 (8.9749)\tF1 0.187 (0.191)\n",
            "Epoch: [6][1200/1405]\tBatch Time 0.334 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 16.7586 (16.7624)\tVB Loss 9.3928 (8.9528)\tF1 0.189 (0.192)\n",
            "Epoch: [6][1300/1405]\tBatch Time 0.201 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 16.7238 (16.7607)\tVB Loss 9.5323 (8.9571)\tF1 0.246 (0.192)\n",
            "Epoch: [6][1400/1405]\tBatch Time 0.263 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 16.7819 (16.7589)\tVB Loss 9.3054 (8.9493)\tF1 0.185 (0.192)\n",
            "Validation: [0/325]\tBatch Time 0.307 (0.307)\tVB Loss 7.7797 (7.7797)\tF1 Score 0.183 (0.183)\t\n",
            "Validation: [100/325]\tBatch Time 0.139 (0.104)\tVB Loss 11.7687 (9.2619)\tF1 Score 0.151 (0.194)\t\n",
            "Validation: [200/325]\tBatch Time 0.062 (0.117)\tVB Loss 4.5179 (9.4486)\tF1 Score 0.226 (0.191)\t\n",
            "Validation: [300/325]\tBatch Time 0.112 (0.119)\tVB Loss 10.2505 (9.4551)\tF1 Score 0.227 (0.191)\t\n",
            "\n",
            " * LOSS - 9.393, F1 SCORE - 0.191\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.001111\n",
            "\n",
            "Epoch: [7][0/1405]\tBatch Time 0.511 (0.511)\tData Load Time 0.187 (0.187)\tCE Loss 16.7233 (16.7233)\tVB Loss 10.5851 (10.5851)\tF1 0.187 (0.187)\n",
            "Epoch: [7][100/1405]\tBatch Time 0.230 (0.249)\tData Load Time 0.005 (0.007)\tCE Loss 16.7325 (16.7359)\tVB Loss 8.6733 (8.4327)\tF1 0.302 (0.196)\n",
            "Epoch: [7][200/1405]\tBatch Time 0.272 (0.241)\tData Load Time 0.005 (0.006)\tCE Loss 16.7495 (16.7322)\tVB Loss 8.2969 (8.6349)\tF1 0.188 (0.194)\n",
            "Epoch: [7][300/1405]\tBatch Time 0.225 (0.235)\tData Load Time 0.006 (0.005)\tCE Loss 16.7221 (16.7296)\tVB Loss 11.0913 (8.7714)\tF1 0.181 (0.194)\n",
            "Epoch: [7][400/1405]\tBatch Time 0.188 (0.230)\tData Load Time 0.004 (0.005)\tCE Loss 16.7822 (16.7277)\tVB Loss 7.6600 (8.7447)\tF1 0.181 (0.194)\n",
            "Epoch: [7][500/1405]\tBatch Time 0.138 (0.227)\tData Load Time 0.005 (0.005)\tCE Loss 16.7046 (16.7262)\tVB Loss 7.6298 (8.7020)\tF1 0.172 (0.194)\n",
            "Epoch: [7][600/1405]\tBatch Time 0.175 (0.225)\tData Load Time 0.004 (0.005)\tCE Loss 16.6750 (16.7248)\tVB Loss 8.3367 (8.7659)\tF1 0.215 (0.194)\n",
            "Epoch: [7][700/1405]\tBatch Time 0.211 (0.224)\tData Load Time 0.006 (0.005)\tCE Loss 16.6956 (16.7238)\tVB Loss 11.9721 (8.7134)\tF1 0.235 (0.194)\n",
            "Epoch: [7][800/1405]\tBatch Time 0.224 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 16.6901 (16.7223)\tVB Loss 7.7002 (8.7494)\tF1 0.185 (0.193)\n",
            "Epoch: [7][900/1405]\tBatch Time 0.234 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 16.7332 (16.7203)\tVB Loss 6.9303 (8.7550)\tF1 0.296 (0.193)\n",
            "Epoch: [7][1000/1405]\tBatch Time 0.227 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 16.7056 (16.7187)\tVB Loss 8.8660 (8.7063)\tF1 0.154 (0.194)\n",
            "Epoch: [7][1100/1405]\tBatch Time 0.241 (0.224)\tData Load Time 0.004 (0.005)\tCE Loss 16.7232 (16.7169)\tVB Loss 8.3237 (8.7069)\tF1 0.180 (0.193)\n",
            "Epoch: [7][1200/1405]\tBatch Time 0.114 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 16.7177 (16.7154)\tVB Loss 5.9170 (8.6984)\tF1 0.210 (0.193)\n",
            "Epoch: [7][1300/1405]\tBatch Time 0.192 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 16.6787 (16.7133)\tVB Loss 6.2227 (8.6778)\tF1 0.234 (0.193)\n",
            "Epoch: [7][1400/1405]\tBatch Time 0.166 (0.222)\tData Load Time 0.007 (0.005)\tCE Loss 16.6813 (16.7110)\tVB Loss 7.8108 (8.6876)\tF1 0.175 (0.193)\n",
            "Validation: [0/325]\tBatch Time 0.305 (0.305)\tVB Loss 5.8873 (5.8873)\tF1 Score 0.185 (0.185)\t\n",
            "Validation: [100/325]\tBatch Time 0.176 (0.119)\tVB Loss 9.5635 (9.2092)\tF1 Score 0.171 (0.198)\t\n",
            "Validation: [200/325]\tBatch Time 0.132 (0.114)\tVB Loss 9.7272 (9.1312)\tF1 Score 0.184 (0.196)\t\n",
            "Validation: [300/325]\tBatch Time 0.093 (0.113)\tVB Loss 8.1773 (9.1538)\tF1 Score 0.241 (0.196)\t\n",
            "\n",
            " * LOSS - 9.110, F1 SCORE - 0.196\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.001071\n",
            "\n",
            "Epoch: [8][0/1405]\tBatch Time 0.519 (0.519)\tData Load Time 0.201 (0.201)\tCE Loss 16.6562 (16.6562)\tVB Loss 7.0928 (7.0928)\tF1 0.185 (0.185)\n",
            "Epoch: [8][100/1405]\tBatch Time 0.283 (0.252)\tData Load Time 0.004 (0.007)\tCE Loss 16.6687 (16.6802)\tVB Loss 8.8063 (8.3282)\tF1 0.187 (0.198)\n",
            "Epoch: [8][200/1405]\tBatch Time 0.292 (0.235)\tData Load Time 0.005 (0.006)\tCE Loss 16.7089 (16.6797)\tVB Loss 12.0261 (8.2624)\tF1 0.186 (0.198)\n",
            "Epoch: [8][300/1405]\tBatch Time 0.237 (0.227)\tData Load Time 0.004 (0.006)\tCE Loss 16.6961 (16.6761)\tVB Loss 5.9219 (8.2947)\tF1 0.236 (0.198)\n",
            "Epoch: [8][400/1405]\tBatch Time 0.277 (0.226)\tData Load Time 0.004 (0.005)\tCE Loss 16.6079 (16.6734)\tVB Loss 13.3300 (8.3790)\tF1 0.176 (0.196)\n",
            "Epoch: [8][500/1405]\tBatch Time 0.275 (0.225)\tData Load Time 0.004 (0.005)\tCE Loss 16.6666 (16.6700)\tVB Loss 6.4203 (8.4029)\tF1 0.191 (0.196)\n",
            "Epoch: [8][600/1405]\tBatch Time 0.220 (0.225)\tData Load Time 0.004 (0.005)\tCE Loss 16.6845 (16.6683)\tVB Loss 12.6595 (8.3664)\tF1 0.181 (0.197)\n",
            "Epoch: [8][700/1405]\tBatch Time 0.195 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 16.6368 (16.6655)\tVB Loss 7.7216 (8.3820)\tF1 0.182 (0.197)\n",
            "Epoch: [8][800/1405]\tBatch Time 0.180 (0.224)\tData Load Time 0.006 (0.005)\tCE Loss 16.6399 (16.6638)\tVB Loss 8.2128 (8.3991)\tF1 0.175 (0.197)\n",
            "Epoch: [8][900/1405]\tBatch Time 0.231 (0.224)\tData Load Time 0.004 (0.005)\tCE Loss 16.6044 (16.6614)\tVB Loss 9.7859 (8.4489)\tF1 0.182 (0.196)\n",
            "Epoch: [8][1000/1405]\tBatch Time 0.180 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 16.5532 (16.6583)\tVB Loss 8.7561 (8.4368)\tF1 0.172 (0.196)\n",
            "Epoch: [8][1100/1405]\tBatch Time 0.244 (0.224)\tData Load Time 0.006 (0.005)\tCE Loss 16.6440 (16.6552)\tVB Loss 6.2729 (8.4571)\tF1 0.232 (0.196)\n",
            "Epoch: [8][1200/1405]\tBatch Time 0.266 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 16.5589 (16.6523)\tVB Loss 6.4675 (8.4141)\tF1 0.187 (0.196)\n",
            "Epoch: [8][1300/1405]\tBatch Time 0.226 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 16.5518 (16.6500)\tVB Loss 12.2386 (8.4314)\tF1 0.170 (0.195)\n",
            "Epoch: [8][1400/1405]\tBatch Time 0.259 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 16.6390 (16.6476)\tVB Loss 11.8785 (8.4543)\tF1 0.183 (0.195)\n",
            "Validation: [0/325]\tBatch Time 0.294 (0.294)\tVB Loss 12.2758 (12.2758)\tF1 Score 0.257 (0.257)\t\n",
            "Validation: [100/325]\tBatch Time 0.190 (0.125)\tVB Loss 7.3556 (8.8042)\tF1 Score 0.352 (0.202)\t\n",
            "Validation: [200/325]\tBatch Time 0.127 (0.124)\tVB Loss 9.5411 (8.7869)\tF1 Score 0.233 (0.202)\t\n",
            "Validation: [300/325]\tBatch Time 0.115 (0.123)\tVB Loss 10.5299 (8.7757)\tF1 Score 0.172 (0.199)\t\n",
            "\n",
            " * LOSS - 8.834, F1 SCORE - 0.199\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.001034\n",
            "\n",
            "Epoch: [9][0/1405]\tBatch Time 0.536 (0.536)\tData Load Time 0.187 (0.187)\tCE Loss 16.6591 (16.6591)\tVB Loss 11.0951 (11.0951)\tF1 0.175 (0.175)\n",
            "Epoch: [9][100/1405]\tBatch Time 0.269 (0.264)\tData Load Time 0.005 (0.007)\tCE Loss 16.6554 (16.6127)\tVB Loss 7.8397 (8.4868)\tF1 0.184 (0.195)\n",
            "Epoch: [9][200/1405]\tBatch Time 0.299 (0.242)\tData Load Time 0.004 (0.006)\tCE Loss 16.6018 (16.6074)\tVB Loss 11.8003 (8.4593)\tF1 0.180 (0.195)\n",
            "Epoch: [9][300/1405]\tBatch Time 0.289 (0.239)\tData Load Time 0.005 (0.006)\tCE Loss 16.6208 (16.6052)\tVB Loss 12.1855 (8.3689)\tF1 0.186 (0.196)\n",
            "Epoch: [9][400/1405]\tBatch Time 0.209 (0.236)\tData Load Time 0.005 (0.005)\tCE Loss 16.6170 (16.6017)\tVB Loss 8.8689 (8.3318)\tF1 0.202 (0.196)\n",
            "Epoch: [9][500/1405]\tBatch Time 0.242 (0.233)\tData Load Time 0.005 (0.005)\tCE Loss 16.4878 (16.5972)\tVB Loss 7.0125 (8.3181)\tF1 0.185 (0.196)\n",
            "Epoch: [9][600/1405]\tBatch Time 0.211 (0.230)\tData Load Time 0.004 (0.005)\tCE Loss 16.5830 (16.5929)\tVB Loss 8.1342 (8.2789)\tF1 0.177 (0.196)\n",
            "Epoch: [9][700/1405]\tBatch Time 0.162 (0.229)\tData Load Time 0.004 (0.005)\tCE Loss 16.5073 (16.5894)\tVB Loss 6.6516 (8.2534)\tF1 0.241 (0.197)\n",
            "Epoch: [9][800/1405]\tBatch Time 0.196 (0.228)\tData Load Time 0.005 (0.005)\tCE Loss 16.6059 (16.5862)\tVB Loss 5.2787 (8.2440)\tF1 0.318 (0.197)\n",
            "Epoch: [9][900/1405]\tBatch Time 0.192 (0.229)\tData Load Time 0.004 (0.005)\tCE Loss 16.6286 (16.5819)\tVB Loss 7.9844 (8.2616)\tF1 0.176 (0.197)\n",
            "Epoch: [9][1000/1405]\tBatch Time 0.159 (0.228)\tData Load Time 0.004 (0.005)\tCE Loss 16.4622 (16.5774)\tVB Loss 3.5805 (8.2487)\tF1 0.239 (0.197)\n",
            "Epoch: [9][1100/1405]\tBatch Time 0.238 (0.228)\tData Load Time 0.005 (0.005)\tCE Loss 16.5717 (16.5735)\tVB Loss 13.0215 (8.2491)\tF1 0.168 (0.197)\n",
            "Epoch: [9][1200/1405]\tBatch Time 0.211 (0.227)\tData Load Time 0.007 (0.005)\tCE Loss 16.5062 (16.5700)\tVB Loss 8.1615 (8.2283)\tF1 0.254 (0.198)\n",
            "Epoch: [9][1300/1405]\tBatch Time 0.244 (0.227)\tData Load Time 0.004 (0.005)\tCE Loss 16.4154 (16.5666)\tVB Loss 10.1139 (8.2548)\tF1 0.176 (0.198)\n",
            "Epoch: [9][1400/1405]\tBatch Time 0.192 (0.226)\tData Load Time 0.004 (0.005)\tCE Loss 16.4948 (16.5627)\tVB Loss 7.8196 (8.2409)\tF1 0.222 (0.198)\n",
            "Validation: [0/325]\tBatch Time 0.333 (0.333)\tVB Loss 11.3202 (11.3202)\tF1 Score 0.235 (0.235)\t\n",
            "Validation: [100/325]\tBatch Time 0.114 (0.124)\tVB Loss 8.1862 (8.4713)\tF1 Score 0.184 (0.202)\t\n",
            "Validation: [200/325]\tBatch Time 0.095 (0.123)\tVB Loss 4.1444 (8.4275)\tF1 Score 0.185 (0.202)\t\n",
            "Validation: [300/325]\tBatch Time 0.112 (0.123)\tVB Loss 11.4634 (8.4804)\tF1 Score 0.224 (0.203)\t\n",
            "\n",
            " * LOSS - 8.588, F1 SCORE - 0.203\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.001000\n",
            "\n",
            "Epoch: [10][0/1405]\tBatch Time 0.509 (0.509)\tData Load Time 0.204 (0.204)\tCE Loss 16.4993 (16.4993)\tVB Loss 7.1215 (7.1215)\tF1 0.185 (0.185)\n",
            "Epoch: [10][100/1405]\tBatch Time 0.228 (0.240)\tData Load Time 0.004 (0.007)\tCE Loss 16.4400 (16.4963)\tVB Loss 8.8355 (7.8066)\tF1 0.225 (0.203)\n",
            "Epoch: [10][200/1405]\tBatch Time 0.182 (0.231)\tData Load Time 0.005 (0.006)\tCE Loss 16.3919 (16.4902)\tVB Loss 6.1598 (8.1663)\tF1 0.276 (0.202)\n",
            "Epoch: [10][300/1405]\tBatch Time 0.190 (0.225)\tData Load Time 0.004 (0.005)\tCE Loss 16.3884 (16.4909)\tVB Loss 8.9134 (8.0054)\tF1 0.169 (0.199)\n",
            "Epoch: [10][400/1405]\tBatch Time 0.243 (0.227)\tData Load Time 0.005 (0.005)\tCE Loss 16.4307 (16.4862)\tVB Loss 6.9935 (8.0780)\tF1 0.184 (0.199)\n",
            "Epoch: [10][500/1405]\tBatch Time 0.203 (0.225)\tData Load Time 0.004 (0.005)\tCE Loss 16.4645 (16.4832)\tVB Loss 5.6610 (8.0293)\tF1 0.187 (0.199)\n",
            "Epoch: [10][600/1405]\tBatch Time 0.179 (0.225)\tData Load Time 0.005 (0.005)\tCE Loss 16.3041 (16.4806)\tVB Loss 5.2809 (8.0390)\tF1 0.182 (0.201)\n",
            "Epoch: [10][700/1405]\tBatch Time 0.182 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 16.4887 (16.4744)\tVB Loss 10.0322 (8.0622)\tF1 0.179 (0.200)\n",
            "Epoch: [10][800/1405]\tBatch Time 0.249 (0.224)\tData Load Time 0.004 (0.005)\tCE Loss 16.5022 (16.4685)\tVB Loss 7.0475 (8.0592)\tF1 0.189 (0.200)\n",
            "Epoch: [10][900/1405]\tBatch Time 0.210 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 16.4342 (16.4652)\tVB Loss 7.8179 (8.0597)\tF1 0.171 (0.201)\n",
            "Epoch: [10][1000/1405]\tBatch Time 0.212 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 16.4940 (16.4603)\tVB Loss 8.9891 (8.0165)\tF1 0.181 (0.200)\n",
            "Epoch: [10][1100/1405]\tBatch Time 0.158 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 16.3178 (16.4545)\tVB Loss 6.2666 (8.0186)\tF1 0.183 (0.201)\n",
            "Epoch: [10][1200/1405]\tBatch Time 0.170 (0.221)\tData Load Time 0.004 (0.005)\tCE Loss 16.2658 (16.4478)\tVB Loss 8.7771 (8.0230)\tF1 0.193 (0.201)\n",
            "Epoch: [10][1300/1405]\tBatch Time 0.219 (0.221)\tData Load Time 0.005 (0.005)\tCE Loss 16.5173 (16.4425)\tVB Loss 7.5950 (8.0259)\tF1 0.186 (0.202)\n",
            "Epoch: [10][1400/1405]\tBatch Time 0.280 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 16.4285 (16.4375)\tVB Loss 9.4603 (8.0295)\tF1 0.181 (0.202)\n",
            "Validation: [0/325]\tBatch Time 0.300 (0.300)\tVB Loss 6.7286 (6.7286)\tF1 Score 0.290 (0.290)\t\n",
            "Validation: [100/325]\tBatch Time 0.086 (0.114)\tVB Loss 3.7378 (8.8248)\tF1 Score 0.484 (0.214)\t\n",
            "Validation: [200/325]\tBatch Time 0.136 (0.113)\tVB Loss 9.5283 (8.5561)\tF1 Score 0.182 (0.215)\t\n",
            "Validation: [300/325]\tBatch Time 0.092 (0.113)\tVB Loss 7.8045 (8.3557)\tF1 Score 0.185 (0.216)\t\n",
            "\n",
            " * LOSS - 8.325, F1 SCORE - 0.216\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000968\n",
            "\n",
            "Epoch: [11][0/1405]\tBatch Time 0.406 (0.406)\tData Load Time 0.189 (0.189)\tCE Loss 16.2957 (16.2957)\tVB Loss 7.3280 (7.3280)\tF1 0.230 (0.230)\n",
            "Epoch: [11][100/1405]\tBatch Time 0.228 (0.244)\tData Load Time 0.005 (0.007)\tCE Loss 16.2519 (16.3346)\tVB Loss 6.1944 (7.8343)\tF1 0.235 (0.211)\n",
            "Epoch: [11][200/1405]\tBatch Time 0.111 (0.233)\tData Load Time 0.004 (0.006)\tCE Loss 16.4391 (16.3398)\tVB Loss 6.5212 (8.0261)\tF1 0.213 (0.209)\n",
            "Epoch: [11][300/1405]\tBatch Time 0.147 (0.231)\tData Load Time 0.005 (0.005)\tCE Loss 16.3556 (16.3275)\tVB Loss 5.8114 (7.9692)\tF1 0.213 (0.210)\n",
            "Epoch: [11][400/1405]\tBatch Time 0.216 (0.229)\tData Load Time 0.005 (0.005)\tCE Loss 16.4177 (16.3203)\tVB Loss 5.9221 (7.8394)\tF1 0.190 (0.210)\n",
            "Epoch: [11][500/1405]\tBatch Time 0.173 (0.229)\tData Load Time 0.005 (0.005)\tCE Loss 16.1545 (16.3116)\tVB Loss 7.1588 (7.8377)\tF1 0.176 (0.209)\n",
            "Epoch: [11][600/1405]\tBatch Time 0.136 (0.227)\tData Load Time 0.005 (0.005)\tCE Loss 16.1459 (16.3041)\tVB Loss 6.8872 (7.8325)\tF1 0.146 (0.209)\n",
            "Epoch: [11][700/1405]\tBatch Time 0.177 (0.227)\tData Load Time 0.004 (0.005)\tCE Loss 16.2963 (16.2947)\tVB Loss 8.2448 (7.8451)\tF1 0.219 (0.209)\n",
            "Epoch: [11][800/1405]\tBatch Time 0.266 (0.226)\tData Load Time 0.005 (0.005)\tCE Loss 16.3406 (16.2868)\tVB Loss 9.6625 (7.8430)\tF1 0.169 (0.209)\n",
            "Epoch: [11][900/1405]\tBatch Time 0.253 (0.226)\tData Load Time 0.005 (0.005)\tCE Loss 16.3551 (16.2764)\tVB Loss 7.4352 (7.8225)\tF1 0.182 (0.210)\n",
            "Epoch: [11][1000/1405]\tBatch Time 0.230 (0.225)\tData Load Time 0.006 (0.005)\tCE Loss 16.0962 (16.2665)\tVB Loss 9.9678 (7.8121)\tF1 0.265 (0.210)\n",
            "Epoch: [11][1100/1405]\tBatch Time 0.236 (0.224)\tData Load Time 0.004 (0.005)\tCE Loss 16.1514 (16.2560)\tVB Loss 10.2590 (7.8178)\tF1 0.182 (0.211)\n",
            "Epoch: [11][1200/1405]\tBatch Time 0.258 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 15.9653 (16.2467)\tVB Loss 10.1424 (7.8361)\tF1 0.246 (0.212)\n",
            "Epoch: [11][1300/1405]\tBatch Time 0.226 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 15.9488 (16.2352)\tVB Loss 6.9211 (7.8117)\tF1 0.185 (0.212)\n",
            "Epoch: [11][1400/1405]\tBatch Time 0.255 (0.223)\tData Load Time 0.006 (0.005)\tCE Loss 16.2021 (16.2229)\tVB Loss 5.7824 (7.8140)\tF1 0.193 (0.212)\n",
            "Validation: [0/325]\tBatch Time 0.305 (0.305)\tVB Loss 8.9233 (8.9233)\tF1 Score 0.171 (0.171)\t\n",
            "Validation: [100/325]\tBatch Time 0.108 (0.111)\tVB Loss 7.9475 (8.0689)\tF1 Score 0.283 (0.231)\t\n",
            "Validation: [200/325]\tBatch Time 0.103 (0.113)\tVB Loss 5.3718 (7.9910)\tF1 Score 0.233 (0.228)\t\n",
            "Validation: [300/325]\tBatch Time 0.110 (0.116)\tVB Loss 7.7307 (7.9379)\tF1 Score 0.347 (0.231)\t\n",
            "\n",
            " * LOSS - 8.049, F1 SCORE - 0.229\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000937\n",
            "\n",
            "Epoch: [12][0/1405]\tBatch Time 0.446 (0.446)\tData Load Time 0.194 (0.194)\tCE Loss 15.9631 (15.9631)\tVB Loss 8.3259 (8.3259)\tF1 0.279 (0.279)\n",
            "Epoch: [12][100/1405]\tBatch Time 0.253 (0.237)\tData Load Time 0.005 (0.007)\tCE Loss 15.9812 (16.0060)\tVB Loss 8.8514 (7.4080)\tF1 0.177 (0.229)\n",
            "Epoch: [12][200/1405]\tBatch Time 0.178 (0.230)\tData Load Time 0.005 (0.006)\tCE Loss 15.9842 (15.9954)\tVB Loss 8.5025 (7.7431)\tF1 0.168 (0.227)\n",
            "Epoch: [12][300/1405]\tBatch Time 0.181 (0.229)\tData Load Time 0.004 (0.006)\tCE Loss 15.8847 (15.9818)\tVB Loss 6.0910 (7.7216)\tF1 0.327 (0.226)\n",
            "Epoch: [12][400/1405]\tBatch Time 0.228 (0.227)\tData Load Time 0.005 (0.005)\tCE Loss 15.7046 (15.9683)\tVB Loss 5.0116 (7.6870)\tF1 0.236 (0.224)\n",
            "Epoch: [12][500/1405]\tBatch Time 0.265 (0.227)\tData Load Time 0.006 (0.005)\tCE Loss 15.8016 (15.9522)\tVB Loss 7.3102 (7.7118)\tF1 0.180 (0.223)\n",
            "Epoch: [12][600/1405]\tBatch Time 0.294 (0.224)\tData Load Time 0.004 (0.005)\tCE Loss 15.8317 (15.9305)\tVB Loss 12.3373 (7.6471)\tF1 0.180 (0.221)\n",
            "Epoch: [12][700/1405]\tBatch Time 0.309 (0.225)\tData Load Time 0.007 (0.005)\tCE Loss 15.8673 (15.9082)\tVB Loss 13.9374 (7.6762)\tF1 0.228 (0.221)\n",
            "Epoch: [12][800/1405]\tBatch Time 0.192 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 15.3771 (15.8810)\tVB Loss 7.6416 (7.6507)\tF1 0.250 (0.221)\n",
            "Epoch: [12][900/1405]\tBatch Time 0.139 (0.222)\tData Load Time 0.007 (0.005)\tCE Loss 15.3764 (15.8515)\tVB Loss 3.7283 (7.6634)\tF1 0.413 (0.222)\n",
            "Epoch: [12][1000/1405]\tBatch Time 0.247 (0.221)\tData Load Time 0.004 (0.005)\tCE Loss 15.5825 (15.8273)\tVB Loss 10.8654 (7.6291)\tF1 0.180 (0.222)\n",
            "Epoch: [12][1100/1405]\tBatch Time 0.234 (0.221)\tData Load Time 0.004 (0.005)\tCE Loss 15.5995 (15.7993)\tVB Loss 8.9858 (7.6378)\tF1 0.227 (0.223)\n",
            "Epoch: [12][1200/1405]\tBatch Time 0.147 (0.221)\tData Load Time 0.004 (0.005)\tCE Loss 14.9132 (15.7652)\tVB Loss 5.9117 (7.6303)\tF1 0.349 (0.224)\n",
            "Epoch: [12][1300/1405]\tBatch Time 0.285 (0.220)\tData Load Time 0.004 (0.005)\tCE Loss 15.2548 (15.7295)\tVB Loss 5.8701 (7.6272)\tF1 0.309 (0.224)\n",
            "Epoch: [12][1400/1405]\tBatch Time 0.234 (0.220)\tData Load Time 0.005 (0.005)\tCE Loss 15.4443 (15.6962)\tVB Loss 9.7299 (7.6398)\tF1 0.182 (0.225)\n",
            "Validation: [0/325]\tBatch Time 0.316 (0.316)\tVB Loss 7.1318 (7.1318)\tF1 Score 0.308 (0.308)\t\n",
            "Validation: [100/325]\tBatch Time 0.111 (0.115)\tVB Loss 9.7817 (7.9603)\tF1 Score 0.224 (0.227)\t\n",
            "Validation: [200/325]\tBatch Time 0.134 (0.112)\tVB Loss 7.0908 (7.8030)\tF1 Score 0.400 (0.239)\t\n",
            "Validation: [300/325]\tBatch Time 0.112 (0.112)\tVB Loss 5.1902 (7.7760)\tF1 Score 0.284 (0.238)\t\n",
            "\n",
            " * LOSS - 7.819, F1 SCORE - 0.237\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000909\n",
            "\n",
            "Epoch: [13][0/1405]\tBatch Time 0.524 (0.524)\tData Load Time 0.211 (0.211)\tCE Loss 15.8251 (15.8251)\tVB Loss 5.7493 (5.7493)\tF1 0.234 (0.234)\n",
            "Epoch: [13][100/1405]\tBatch Time 0.242 (0.254)\tData Load Time 0.004 (0.007)\tCE Loss 15.2660 (15.1593)\tVB Loss 3.2665 (7.7149)\tF1 0.238 (0.222)\n",
            "Epoch: [13][200/1405]\tBatch Time 0.260 (0.242)\tData Load Time 0.004 (0.006)\tCE Loss 14.9239 (15.1216)\tVB Loss 7.0379 (7.8230)\tF1 0.303 (0.224)\n",
            "Epoch: [13][300/1405]\tBatch Time 0.268 (0.239)\tData Load Time 0.005 (0.006)\tCE Loss 14.6808 (15.0799)\tVB Loss 4.7099 (7.6048)\tF1 0.187 (0.228)\n",
            "Epoch: [13][400/1405]\tBatch Time 0.211 (0.232)\tData Load Time 0.005 (0.005)\tCE Loss 14.6362 (15.0424)\tVB Loss 7.3313 (7.4684)\tF1 0.305 (0.228)\n",
            "Epoch: [13][500/1405]\tBatch Time 0.216 (0.232)\tData Load Time 0.004 (0.005)\tCE Loss 14.6852 (15.0090)\tVB Loss 4.0784 (7.4970)\tF1 0.326 (0.228)\n",
            "Epoch: [13][600/1405]\tBatch Time 0.225 (0.229)\tData Load Time 0.005 (0.005)\tCE Loss 15.2337 (14.9798)\tVB Loss 5.2713 (7.4792)\tF1 0.238 (0.229)\n",
            "Epoch: [13][700/1405]\tBatch Time 0.161 (0.227)\tData Load Time 0.004 (0.005)\tCE Loss 15.0648 (14.9470)\tVB Loss 4.3572 (7.4750)\tF1 0.345 (0.231)\n",
            "Epoch: [13][800/1405]\tBatch Time 0.250 (0.226)\tData Load Time 0.005 (0.005)\tCE Loss 14.8876 (14.9102)\tVB Loss 6.5499 (7.4581)\tF1 0.437 (0.233)\n",
            "Epoch: [13][900/1405]\tBatch Time 0.219 (0.225)\tData Load Time 0.004 (0.005)\tCE Loss 14.7884 (14.8729)\tVB Loss 9.4698 (7.4821)\tF1 0.174 (0.233)\n",
            "Epoch: [13][1000/1405]\tBatch Time 0.199 (0.225)\tData Load Time 0.004 (0.005)\tCE Loss 14.9918 (14.8442)\tVB Loss 7.3060 (7.4545)\tF1 0.301 (0.234)\n",
            "Epoch: [13][1100/1405]\tBatch Time 0.210 (0.224)\tData Load Time 0.004 (0.005)\tCE Loss 14.6251 (14.8207)\tVB Loss 7.6946 (7.4493)\tF1 0.151 (0.235)\n",
            "Epoch: [13][1200/1405]\tBatch Time 0.221 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 14.2511 (14.7874)\tVB Loss 6.6078 (7.4421)\tF1 0.296 (0.236)\n",
            "Epoch: [13][1300/1405]\tBatch Time 0.257 (0.222)\tData Load Time 0.006 (0.005)\tCE Loss 14.3739 (14.7587)\tVB Loss 8.5972 (7.4092)\tF1 0.244 (0.237)\n",
            "Epoch: [13][1400/1405]\tBatch Time 0.341 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 14.7616 (14.7319)\tVB Loss 8.0375 (7.4338)\tF1 0.316 (0.238)\n",
            "Validation: [0/325]\tBatch Time 0.285 (0.285)\tVB Loss 5.6260 (5.6260)\tF1 Score 0.389 (0.389)\t\n",
            "Validation: [100/325]\tBatch Time 0.104 (0.110)\tVB Loss 9.3584 (7.8699)\tF1 Score 0.184 (0.249)\t\n",
            "Validation: [200/325]\tBatch Time 0.091 (0.109)\tVB Loss 4.4242 (7.7547)\tF1 Score 0.256 (0.243)\t\n",
            "Validation: [300/325]\tBatch Time 0.120 (0.109)\tVB Loss 7.2269 (7.5878)\tF1 Score 0.230 (0.250)\t\n",
            "\n",
            " * LOSS - 7.589, F1 SCORE - 0.250\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000882\n",
            "\n",
            "Epoch: [14][0/1405]\tBatch Time 0.543 (0.543)\tData Load Time 0.188 (0.188)\tCE Loss 13.8625 (13.8625)\tVB Loss 8.5821 (8.5821)\tF1 0.271 (0.271)\n",
            "Epoch: [14][100/1405]\tBatch Time 0.257 (0.249)\tData Load Time 0.006 (0.007)\tCE Loss 14.3071 (14.3314)\tVB Loss 4.2989 (7.7399)\tF1 0.324 (0.243)\n",
            "Epoch: [14][200/1405]\tBatch Time 0.197 (0.236)\tData Load Time 0.004 (0.006)\tCE Loss 14.2209 (14.3095)\tVB Loss 7.0020 (7.5298)\tF1 0.261 (0.249)\n",
            "Epoch: [14][300/1405]\tBatch Time 0.259 (0.228)\tData Load Time 0.005 (0.006)\tCE Loss 14.1391 (14.2913)\tVB Loss 11.2664 (7.3423)\tF1 0.354 (0.246)\n",
            "Epoch: [14][400/1405]\tBatch Time 0.221 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 13.9758 (14.2647)\tVB Loss 8.3579 (7.2546)\tF1 0.226 (0.249)\n",
            "Epoch: [14][500/1405]\tBatch Time 0.202 (0.221)\tData Load Time 0.004 (0.005)\tCE Loss 12.9928 (14.2391)\tVB Loss 8.3336 (7.2138)\tF1 0.219 (0.248)\n",
            "Epoch: [14][600/1405]\tBatch Time 0.294 (0.221)\tData Load Time 0.005 (0.005)\tCE Loss 13.9923 (14.2192)\tVB Loss 10.7022 (7.2237)\tF1 0.278 (0.250)\n",
            "Epoch: [14][700/1405]\tBatch Time 0.239 (0.220)\tData Load Time 0.004 (0.005)\tCE Loss 14.2695 (14.1944)\tVB Loss 8.7327 (7.2347)\tF1 0.189 (0.250)\n",
            "Epoch: [14][800/1405]\tBatch Time 0.167 (0.221)\tData Load Time 0.005 (0.005)\tCE Loss 13.5160 (14.1725)\tVB Loss 4.6672 (7.2685)\tF1 0.178 (0.250)\n",
            "Epoch: [14][900/1405]\tBatch Time 0.262 (0.220)\tData Load Time 0.005 (0.005)\tCE Loss 13.7129 (14.1485)\tVB Loss 7.7512 (7.2559)\tF1 0.372 (0.252)\n",
            "Epoch: [14][1000/1405]\tBatch Time 0.228 (0.221)\tData Load Time 0.004 (0.005)\tCE Loss 14.9356 (14.1253)\tVB Loss 6.0207 (7.2773)\tF1 0.289 (0.253)\n",
            "Epoch: [14][1100/1405]\tBatch Time 0.201 (0.221)\tData Load Time 0.004 (0.005)\tCE Loss 12.7311 (14.1036)\tVB Loss 6.5020 (7.2852)\tF1 0.383 (0.253)\n",
            "Epoch: [14][1200/1405]\tBatch Time 0.215 (0.221)\tData Load Time 0.005 (0.005)\tCE Loss 13.4576 (14.0806)\tVB Loss 5.6876 (7.2799)\tF1 0.336 (0.254)\n",
            "Epoch: [14][1300/1405]\tBatch Time 0.102 (0.221)\tData Load Time 0.004 (0.005)\tCE Loss 12.1660 (14.0553)\tVB Loss 3.2164 (7.2774)\tF1 0.343 (0.254)\n",
            "Epoch: [14][1400/1405]\tBatch Time 0.202 (0.221)\tData Load Time 0.005 (0.005)\tCE Loss 14.4627 (14.0360)\tVB Loss 8.0216 (7.2466)\tF1 0.182 (0.255)\n",
            "Validation: [0/325]\tBatch Time 0.305 (0.305)\tVB Loss 13.1744 (13.1744)\tF1 Score 0.221 (0.221)\t\n",
            "Validation: [100/325]\tBatch Time 0.085 (0.108)\tVB Loss 5.8749 (7.3308)\tF1 Score 0.230 (0.267)\t\n",
            "Validation: [200/325]\tBatch Time 0.161 (0.108)\tVB Loss 9.6916 (7.3019)\tF1 Score 0.293 (0.272)\t\n",
            "Validation: [300/325]\tBatch Time 0.118 (0.108)\tVB Loss 6.6297 (7.3364)\tF1 Score 0.277 (0.274)\t\n",
            "\n",
            " * LOSS - 7.339, F1 SCORE - 0.276\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000857\n",
            "\n",
            "Epoch: [15][0/1405]\tBatch Time 0.342 (0.342)\tData Load Time 0.206 (0.206)\tCE Loss 13.3659 (13.3659)\tVB Loss 4.0249 (4.0249)\tF1 0.396 (0.396)\n",
            "Epoch: [15][100/1405]\tBatch Time 0.263 (0.249)\tData Load Time 0.005 (0.007)\tCE Loss 14.0559 (13.7505)\tVB Loss 10.4915 (6.9679)\tF1 0.269 (0.258)\n",
            "Epoch: [15][200/1405]\tBatch Time 0.103 (0.229)\tData Load Time 0.005 (0.006)\tCE Loss 12.5730 (13.7356)\tVB Loss 5.3900 (6.8817)\tF1 0.355 (0.264)\n",
            "Epoch: [15][300/1405]\tBatch Time 0.253 (0.229)\tData Load Time 0.004 (0.006)\tCE Loss 13.3014 (13.7083)\tVB Loss 8.2934 (7.0499)\tF1 0.181 (0.265)\n",
            "Epoch: [15][400/1405]\tBatch Time 0.210 (0.225)\tData Load Time 0.005 (0.005)\tCE Loss 13.8281 (13.7075)\tVB Loss 6.6984 (7.0677)\tF1 0.245 (0.263)\n",
            "Epoch: [15][500/1405]\tBatch Time 0.217 (0.225)\tData Load Time 0.004 (0.005)\tCE Loss 12.6652 (13.6868)\tVB Loss 10.1716 (7.0718)\tF1 0.288 (0.262)\n",
            "Epoch: [15][600/1405]\tBatch Time 0.202 (0.224)\tData Load Time 0.004 (0.005)\tCE Loss 13.5712 (13.6767)\tVB Loss 8.3844 (7.0652)\tF1 0.228 (0.263)\n",
            "Epoch: [15][700/1405]\tBatch Time 0.196 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 12.9136 (13.6520)\tVB Loss 5.4716 (7.0767)\tF1 0.346 (0.267)\n",
            "Epoch: [15][800/1405]\tBatch Time 0.264 (0.221)\tData Load Time 0.004 (0.005)\tCE Loss 13.1371 (13.6317)\tVB Loss 7.2172 (7.0578)\tF1 0.223 (0.268)\n",
            "Epoch: [15][900/1405]\tBatch Time 0.234 (0.221)\tData Load Time 0.004 (0.005)\tCE Loss 13.6976 (13.6157)\tVB Loss 5.3791 (7.0764)\tF1 0.338 (0.270)\n",
            "Epoch: [15][1000/1405]\tBatch Time 0.261 (0.221)\tData Load Time 0.004 (0.005)\tCE Loss 13.1043 (13.5992)\tVB Loss 8.7568 (7.0645)\tF1 0.340 (0.271)\n",
            "Epoch: [15][1100/1405]\tBatch Time 0.271 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 12.8115 (13.5837)\tVB Loss 7.5197 (7.0745)\tF1 0.283 (0.272)\n",
            "Epoch: [15][1200/1405]\tBatch Time 0.176 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 12.9090 (13.5704)\tVB Loss 7.2667 (7.0686)\tF1 0.170 (0.273)\n",
            "Epoch: [15][1300/1405]\tBatch Time 0.239 (0.221)\tData Load Time 0.005 (0.005)\tCE Loss 12.4262 (13.5513)\tVB Loss 4.1865 (7.0596)\tF1 0.323 (0.273)\n",
            "Epoch: [15][1400/1405]\tBatch Time 0.172 (0.221)\tData Load Time 0.004 (0.005)\tCE Loss 12.9796 (13.5446)\tVB Loss 4.8005 (7.0520)\tF1 0.400 (0.274)\n",
            "Validation: [0/325]\tBatch Time 0.281 (0.281)\tVB Loss 3.1405 (3.1405)\tF1 Score 0.405 (0.405)\t\n",
            "Validation: [100/325]\tBatch Time 0.143 (0.123)\tVB Loss 8.2289 (7.2245)\tF1 Score 0.294 (0.290)\t\n",
            "Validation: [200/325]\tBatch Time 0.132 (0.123)\tVB Loss 8.1810 (7.2132)\tF1 Score 0.210 (0.289)\t\n",
            "Validation: [300/325]\tBatch Time 0.121 (0.123)\tVB Loss 7.4385 (7.0901)\tF1 Score 0.292 (0.295)\t\n",
            "\n",
            " * LOSS - 7.112, F1 SCORE - 0.296\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000833\n",
            "\n",
            "Epoch: [16][0/1405]\tBatch Time 0.420 (0.420)\tData Load Time 0.190 (0.190)\tCE Loss 14.0073 (14.0073)\tVB Loss 5.0441 (5.0441)\tF1 0.474 (0.474)\n",
            "Epoch: [16][100/1405]\tBatch Time 0.198 (0.249)\tData Load Time 0.006 (0.007)\tCE Loss 12.9805 (13.3119)\tVB Loss 5.3139 (6.8447)\tF1 0.235 (0.300)\n",
            "Epoch: [16][200/1405]\tBatch Time 0.242 (0.233)\tData Load Time 0.004 (0.006)\tCE Loss 12.2579 (13.2922)\tVB Loss 6.7133 (6.7803)\tF1 0.180 (0.297)\n",
            "Epoch: [16][300/1405]\tBatch Time 0.216 (0.227)\tData Load Time 0.004 (0.006)\tCE Loss 14.2905 (13.3143)\tVB Loss 4.9889 (6.8258)\tF1 0.322 (0.294)\n",
            "Epoch: [16][400/1405]\tBatch Time 0.226 (0.224)\tData Load Time 0.004 (0.005)\tCE Loss 11.8590 (13.3148)\tVB Loss 5.1782 (6.8291)\tF1 0.384 (0.293)\n",
            "Epoch: [16][500/1405]\tBatch Time 0.241 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 14.2382 (13.2955)\tVB Loss 8.1803 (6.8076)\tF1 0.303 (0.296)\n",
            "Epoch: [16][600/1405]\tBatch Time 0.309 (0.221)\tData Load Time 0.006 (0.005)\tCE Loss 13.9187 (13.2666)\tVB Loss 9.9618 (6.8028)\tF1 0.182 (0.296)\n",
            "Epoch: [16][700/1405]\tBatch Time 0.203 (0.220)\tData Load Time 0.005 (0.005)\tCE Loss 13.4463 (13.2612)\tVB Loss 6.0732 (6.8358)\tF1 0.183 (0.295)\n",
            "Epoch: [16][800/1405]\tBatch Time 0.258 (0.220)\tData Load Time 0.005 (0.005)\tCE Loss 13.4645 (13.2594)\tVB Loss 10.2509 (6.8355)\tF1 0.312 (0.295)\n",
            "Epoch: [16][900/1405]\tBatch Time 0.195 (0.220)\tData Load Time 0.004 (0.005)\tCE Loss 12.9094 (13.2467)\tVB Loss 4.5509 (6.8295)\tF1 0.423 (0.296)\n",
            "Epoch: [16][1000/1405]\tBatch Time 0.257 (0.220)\tData Load Time 0.004 (0.005)\tCE Loss 12.6022 (13.2398)\tVB Loss 9.1730 (6.8472)\tF1 0.257 (0.296)\n",
            "Epoch: [16][1100/1405]\tBatch Time 0.208 (0.220)\tData Load Time 0.005 (0.005)\tCE Loss 12.4102 (13.2363)\tVB Loss 5.3554 (6.8263)\tF1 0.362 (0.296)\n",
            "Epoch: [16][1200/1405]\tBatch Time 0.253 (0.220)\tData Load Time 0.005 (0.005)\tCE Loss 13.9381 (13.2275)\tVB Loss 7.0402 (6.8337)\tF1 0.242 (0.297)\n",
            "Epoch: [16][1300/1405]\tBatch Time 0.172 (0.219)\tData Load Time 0.005 (0.005)\tCE Loss 13.1508 (13.2249)\tVB Loss 8.4332 (6.8344)\tF1 0.291 (0.297)\n",
            "Epoch: [16][1400/1405]\tBatch Time 0.205 (0.219)\tData Load Time 0.004 (0.005)\tCE Loss 12.6322 (13.2157)\tVB Loss 4.2855 (6.8189)\tF1 0.582 (0.299)\n",
            "Validation: [0/325]\tBatch Time 0.302 (0.302)\tVB Loss 7.7328 (7.7328)\tF1 Score 0.347 (0.347)\t\n",
            "Validation: [100/325]\tBatch Time 0.256 (0.111)\tVB Loss 9.7571 (6.7370)\tF1 Score 0.314 (0.312)\t\n",
            "Validation: [200/325]\tBatch Time 0.064 (0.110)\tVB Loss 4.5561 (6.7826)\tF1 Score 0.471 (0.319)\t\n",
            "Validation: [300/325]\tBatch Time 0.110 (0.112)\tVB Loss 5.7293 (6.8049)\tF1 Score 0.329 (0.312)\t\n",
            "\n",
            " * LOSS - 6.810, F1 SCORE - 0.311\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000811\n",
            "\n",
            "Epoch: [17][0/1405]\tBatch Time 0.519 (0.519)\tData Load Time 0.204 (0.204)\tCE Loss 12.7280 (12.7280)\tVB Loss 8.8641 (8.8641)\tF1 0.185 (0.185)\n",
            "Epoch: [17][100/1405]\tBatch Time 0.250 (0.243)\tData Load Time 0.004 (0.007)\tCE Loss 12.8051 (13.1175)\tVB Loss 11.1206 (6.6439)\tF1 0.171 (0.290)\n",
            "Epoch: [17][200/1405]\tBatch Time 0.240 (0.233)\tData Load Time 0.004 (0.006)\tCE Loss 13.0079 (13.1228)\tVB Loss 7.0768 (6.8485)\tF1 0.238 (0.298)\n",
            "Epoch: [17][300/1405]\tBatch Time 0.255 (0.229)\tData Load Time 0.005 (0.006)\tCE Loss 13.4664 (13.1003)\tVB Loss 7.5346 (6.8342)\tF1 0.354 (0.304)\n",
            "Epoch: [17][400/1405]\tBatch Time 0.185 (0.229)\tData Load Time 0.005 (0.006)\tCE Loss 13.3361 (13.0878)\tVB Loss 8.1960 (6.7953)\tF1 0.264 (0.305)\n",
            "Epoch: [17][500/1405]\tBatch Time 0.208 (0.226)\tData Load Time 0.004 (0.005)\tCE Loss 13.2400 (13.0673)\tVB Loss 9.7444 (6.7367)\tF1 0.354 (0.305)\n",
            "Epoch: [17][600/1405]\tBatch Time 0.182 (0.224)\tData Load Time 0.004 (0.005)\tCE Loss 13.1392 (13.0591)\tVB Loss 5.2505 (6.7511)\tF1 0.398 (0.309)\n",
            "Epoch: [17][700/1405]\tBatch Time 0.223 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 12.7655 (13.0459)\tVB Loss 5.5562 (6.7113)\tF1 0.389 (0.311)\n",
            "Epoch: [17][800/1405]\tBatch Time 0.265 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 13.6078 (13.0484)\tVB Loss 7.1914 (6.7110)\tF1 0.372 (0.312)\n",
            "Epoch: [17][900/1405]\tBatch Time 0.260 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 13.1336 (13.0447)\tVB Loss 6.5857 (6.7089)\tF1 0.492 (0.313)\n",
            "Epoch: [17][1000/1405]\tBatch Time 0.189 (0.221)\tData Load Time 0.004 (0.005)\tCE Loss 12.8461 (13.0312)\tVB Loss 5.9435 (6.6851)\tF1 0.249 (0.314)\n",
            "Epoch: [17][1100/1405]\tBatch Time 0.181 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 13.3561 (13.0222)\tVB Loss 8.9782 (6.6911)\tF1 0.210 (0.315)\n",
            "Epoch: [17][1200/1405]\tBatch Time 0.310 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 13.1482 (13.0173)\tVB Loss 13.7335 (6.6613)\tF1 0.182 (0.316)\n",
            "Epoch: [17][1300/1405]\tBatch Time 0.194 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 12.4615 (13.0104)\tVB Loss 5.8633 (6.6420)\tF1 0.355 (0.318)\n",
            "Epoch: [17][1400/1405]\tBatch Time 0.185 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 12.7878 (13.0059)\tVB Loss 6.3918 (6.5901)\tF1 0.342 (0.319)\n",
            "Validation: [0/325]\tBatch Time 0.307 (0.307)\tVB Loss 5.0518 (5.0518)\tF1 Score 0.391 (0.391)\t\n",
            "Validation: [100/325]\tBatch Time 0.073 (0.115)\tVB Loss 3.4383 (6.2781)\tF1 Score 0.186 (0.327)\t\n",
            "Validation: [200/325]\tBatch Time 0.085 (0.116)\tVB Loss 5.6179 (6.4830)\tF1 Score 0.348 (0.329)\t\n",
            "Validation: [300/325]\tBatch Time 0.130 (0.119)\tVB Loss 6.2624 (6.4227)\tF1 Score 0.330 (0.335)\t\n",
            "\n",
            " * LOSS - 6.415, F1 SCORE - 0.337\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000789\n",
            "\n",
            "Epoch: [18][0/1405]\tBatch Time 0.894 (0.894)\tData Load Time 0.213 (0.213)\tCE Loss 13.7331 (13.7331)\tVB Loss 8.9737 (8.9737)\tF1 0.231 (0.231)\n",
            "Epoch: [18][100/1405]\tBatch Time 0.255 (0.242)\tData Load Time 0.006 (0.007)\tCE Loss 12.7139 (13.0009)\tVB Loss 2.9579 (6.4078)\tF1 0.410 (0.338)\n",
            "Epoch: [18][200/1405]\tBatch Time 0.226 (0.232)\tData Load Time 0.005 (0.006)\tCE Loss 12.4716 (12.9233)\tVB Loss 6.1713 (6.2081)\tF1 0.267 (0.344)\n",
            "Epoch: [18][300/1405]\tBatch Time 0.130 (0.229)\tData Load Time 0.006 (0.006)\tCE Loss 12.3083 (12.9040)\tVB Loss 4.7650 (6.3090)\tF1 0.363 (0.342)\n",
            "Epoch: [18][400/1405]\tBatch Time 0.262 (0.229)\tData Load Time 0.005 (0.006)\tCE Loss 13.4695 (12.9107)\tVB Loss 7.5864 (6.4217)\tF1 0.273 (0.339)\n",
            "Epoch: [18][500/1405]\tBatch Time 0.168 (0.227)\tData Load Time 0.004 (0.005)\tCE Loss 12.8301 (12.8899)\tVB Loss 4.2798 (6.3948)\tF1 0.476 (0.340)\n",
            "Epoch: [18][600/1405]\tBatch Time 0.318 (0.226)\tData Load Time 0.005 (0.005)\tCE Loss 12.9419 (12.8848)\tVB Loss 5.1946 (6.3450)\tF1 0.239 (0.341)\n",
            "Epoch: [18][700/1405]\tBatch Time 0.175 (0.226)\tData Load Time 0.004 (0.005)\tCE Loss 12.0934 (12.8789)\tVB Loss 3.4267 (6.3608)\tF1 0.248 (0.340)\n",
            "Epoch: [18][800/1405]\tBatch Time 0.195 (0.226)\tData Load Time 0.005 (0.005)\tCE Loss 13.4910 (12.8814)\tVB Loss 3.3943 (6.3502)\tF1 0.350 (0.338)\n",
            "Epoch: [18][900/1405]\tBatch Time 0.271 (0.226)\tData Load Time 0.007 (0.005)\tCE Loss 12.6172 (12.8674)\tVB Loss 5.6227 (6.3207)\tF1 0.364 (0.339)\n",
            "Epoch: [18][1000/1405]\tBatch Time 0.208 (0.226)\tData Load Time 0.004 (0.005)\tCE Loss 12.8332 (12.8696)\tVB Loss 7.5074 (6.3087)\tF1 0.283 (0.342)\n",
            "Epoch: [18][1100/1405]\tBatch Time 0.203 (0.225)\tData Load Time 0.005 (0.005)\tCE Loss 13.3037 (12.8639)\tVB Loss 5.0479 (6.3019)\tF1 0.393 (0.343)\n",
            "Epoch: [18][1200/1405]\tBatch Time 0.157 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 11.6799 (12.8580)\tVB Loss 5.9139 (6.3119)\tF1 0.277 (0.345)\n",
            "Epoch: [18][1300/1405]\tBatch Time 0.215 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 12.5017 (12.8549)\tVB Loss 4.9865 (6.3259)\tF1 0.352 (0.346)\n",
            "Epoch: [18][1400/1405]\tBatch Time 0.214 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 13.1001 (12.8493)\tVB Loss 5.5851 (6.3132)\tF1 0.232 (0.348)\n",
            "Validation: [0/325]\tBatch Time 0.308 (0.308)\tVB Loss 4.4621 (4.4621)\tF1 Score 0.317 (0.317)\t\n",
            "Validation: [100/325]\tBatch Time 0.102 (0.113)\tVB Loss 7.1230 (5.8732)\tF1 Score 0.338 (0.402)\t\n",
            "Validation: [200/325]\tBatch Time 0.080 (0.108)\tVB Loss 5.1790 (5.9984)\tF1 Score 0.240 (0.388)\t\n",
            "Validation: [300/325]\tBatch Time 0.104 (0.111)\tVB Loss 4.9922 (5.9498)\tF1 Score 0.368 (0.389)\t\n",
            "\n",
            " * LOSS - 6.003, F1 SCORE - 0.389\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000769\n",
            "\n",
            "Epoch: [19][0/1405]\tBatch Time 0.449 (0.449)\tData Load Time 0.215 (0.215)\tCE Loss 12.6203 (12.6203)\tVB Loss 4.2661 (4.2661)\tF1 0.286 (0.286)\n",
            "Epoch: [19][100/1405]\tBatch Time 0.292 (0.249)\tData Load Time 0.004 (0.007)\tCE Loss 11.7765 (12.7463)\tVB Loss 4.6989 (5.8322)\tF1 0.447 (0.361)\n",
            "Epoch: [19][200/1405]\tBatch Time 0.167 (0.234)\tData Load Time 0.006 (0.006)\tCE Loss 12.2202 (12.7809)\tVB Loss 3.4965 (6.0342)\tF1 0.516 (0.357)\n",
            "Epoch: [19][300/1405]\tBatch Time 0.242 (0.232)\tData Load Time 0.005 (0.006)\tCE Loss 12.5621 (12.7844)\tVB Loss 4.5201 (6.1091)\tF1 0.325 (0.363)\n",
            "Epoch: [19][400/1405]\tBatch Time 0.084 (0.228)\tData Load Time 0.006 (0.005)\tCE Loss 12.3381 (12.7801)\tVB Loss 2.6402 (6.1112)\tF1 0.468 (0.363)\n",
            "Epoch: [19][500/1405]\tBatch Time 0.227 (0.228)\tData Load Time 0.005 (0.005)\tCE Loss 12.8100 (12.7748)\tVB Loss 6.4047 (6.1016)\tF1 0.272 (0.361)\n",
            "Epoch: [19][600/1405]\tBatch Time 0.196 (0.227)\tData Load Time 0.004 (0.005)\tCE Loss 12.2166 (12.7690)\tVB Loss 3.7177 (6.1194)\tF1 0.402 (0.361)\n",
            "Epoch: [19][700/1405]\tBatch Time 0.263 (0.226)\tData Load Time 0.004 (0.005)\tCE Loss 12.9405 (12.7690)\tVB Loss 10.0752 (6.1302)\tF1 0.440 (0.361)\n",
            "Epoch: [19][800/1405]\tBatch Time 0.196 (0.227)\tData Load Time 0.005 (0.005)\tCE Loss 13.3230 (12.7716)\tVB Loss 3.9006 (6.1472)\tF1 0.629 (0.363)\n",
            "Epoch: [19][900/1405]\tBatch Time 0.233 (0.225)\tData Load Time 0.005 (0.005)\tCE Loss 13.0408 (12.7616)\tVB Loss 5.0755 (6.1289)\tF1 0.440 (0.367)\n",
            "Epoch: [19][1000/1405]\tBatch Time 0.245 (0.225)\tData Load Time 0.004 (0.005)\tCE Loss 12.4698 (12.7512)\tVB Loss 6.3214 (6.0872)\tF1 0.393 (0.368)\n",
            "Epoch: [19][1100/1405]\tBatch Time 0.186 (0.225)\tData Load Time 0.005 (0.005)\tCE Loss 13.5627 (12.7522)\tVB Loss 3.6028 (6.0710)\tF1 0.430 (0.370)\n",
            "Epoch: [19][1200/1405]\tBatch Time 0.219 (0.224)\tData Load Time 0.006 (0.005)\tCE Loss 12.5152 (12.7419)\tVB Loss 5.3923 (6.0557)\tF1 0.307 (0.369)\n",
            "Epoch: [19][1300/1405]\tBatch Time 0.268 (0.223)\tData Load Time 0.006 (0.005)\tCE Loss 12.5565 (12.7363)\tVB Loss 9.6483 (6.0777)\tF1 0.383 (0.370)\n",
            "Epoch: [19][1400/1405]\tBatch Time 0.214 (0.223)\tData Load Time 0.006 (0.005)\tCE Loss 12.7769 (12.7280)\tVB Loss 6.4599 (6.0777)\tF1 0.374 (0.372)\n",
            "Validation: [0/325]\tBatch Time 0.288 (0.288)\tVB Loss 4.3740 (4.3740)\tF1 Score 0.484 (0.484)\t\n",
            "Validation: [100/325]\tBatch Time 0.099 (0.106)\tVB Loss 5.3026 (5.6172)\tF1 Score 0.185 (0.405)\t\n",
            "Validation: [200/325]\tBatch Time 0.107 (0.111)\tVB Loss 5.1237 (5.8018)\tF1 Score 0.355 (0.401)\t\n",
            "Validation: [300/325]\tBatch Time 0.078 (0.113)\tVB Loss 3.7731 (5.8105)\tF1 Score 0.538 (0.400)\t\n",
            "\n",
            " * LOSS - 5.780, F1 SCORE - 0.401\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000750\n",
            "\n",
            "Epoch: [20][0/1405]\tBatch Time 0.427 (0.427)\tData Load Time 0.185 (0.185)\tCE Loss 12.8967 (12.8967)\tVB Loss 4.7963 (4.7963)\tF1 0.291 (0.291)\n",
            "Epoch: [20][100/1405]\tBatch Time 0.251 (0.253)\tData Load Time 0.004 (0.007)\tCE Loss 12.8310 (12.6975)\tVB Loss 6.7575 (5.9948)\tF1 0.382 (0.390)\n",
            "Epoch: [20][200/1405]\tBatch Time 0.248 (0.239)\tData Load Time 0.006 (0.006)\tCE Loss 12.9421 (12.6937)\tVB Loss 6.4464 (5.9653)\tF1 0.275 (0.391)\n",
            "Epoch: [20][300/1405]\tBatch Time 0.191 (0.234)\tData Load Time 0.004 (0.006)\tCE Loss 13.1629 (12.7153)\tVB Loss 4.5530 (5.9680)\tF1 0.237 (0.391)\n",
            "Epoch: [20][400/1405]\tBatch Time 0.273 (0.230)\tData Load Time 0.005 (0.005)\tCE Loss 13.0231 (12.6824)\tVB Loss 4.0122 (5.9127)\tF1 0.518 (0.391)\n",
            "Epoch: [20][500/1405]\tBatch Time 0.161 (0.228)\tData Load Time 0.004 (0.005)\tCE Loss 12.5599 (12.6862)\tVB Loss 3.8169 (5.8917)\tF1 0.405 (0.390)\n",
            "Epoch: [20][600/1405]\tBatch Time 0.252 (0.227)\tData Load Time 0.004 (0.005)\tCE Loss 13.2864 (12.6796)\tVB Loss 9.4607 (5.8965)\tF1 0.398 (0.392)\n",
            "Epoch: [20][700/1405]\tBatch Time 0.246 (0.226)\tData Load Time 0.006 (0.005)\tCE Loss 11.7866 (12.6727)\tVB Loss 4.7505 (5.8857)\tF1 0.480 (0.394)\n",
            "Epoch: [20][800/1405]\tBatch Time 0.294 (0.226)\tData Load Time 0.005 (0.005)\tCE Loss 12.9685 (12.6749)\tVB Loss 8.9347 (5.9093)\tF1 0.268 (0.392)\n",
            "Epoch: [20][900/1405]\tBatch Time 0.139 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 12.5892 (12.6552)\tVB Loss 2.5591 (5.8920)\tF1 0.366 (0.392)\n",
            "Epoch: [20][1000/1405]\tBatch Time 0.194 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 12.8423 (12.6514)\tVB Loss 3.5470 (5.8888)\tF1 0.611 (0.391)\n",
            "Epoch: [20][1100/1405]\tBatch Time 0.197 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 12.8122 (12.6458)\tVB Loss 4.3447 (5.8930)\tF1 0.521 (0.391)\n",
            "Epoch: [20][1200/1405]\tBatch Time 0.185 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 12.4250 (12.6405)\tVB Loss 3.7218 (5.8954)\tF1 0.547 (0.391)\n",
            "Epoch: [20][1300/1405]\tBatch Time 0.315 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 12.1620 (12.6326)\tVB Loss 7.2863 (5.9072)\tF1 0.506 (0.392)\n",
            "Epoch: [20][1400/1405]\tBatch Time 0.204 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 12.8745 (12.6216)\tVB Loss 7.7325 (5.8878)\tF1 0.430 (0.394)\n",
            "Validation: [0/325]\tBatch Time 0.312 (0.312)\tVB Loss 5.7068 (5.7068)\tF1 Score 0.388 (0.388)\t\n",
            "Validation: [100/325]\tBatch Time 0.119 (0.115)\tVB Loss 5.0635 (5.4520)\tF1 Score 0.376 (0.453)\t\n",
            "Validation: [200/325]\tBatch Time 0.155 (0.118)\tVB Loss 8.6064 (5.4927)\tF1 Score 0.337 (0.439)\t\n",
            "Validation: [300/325]\tBatch Time 0.147 (0.120)\tVB Loss 8.1451 (5.4759)\tF1 Score 0.420 (0.439)\t\n",
            "\n",
            " * LOSS - 5.456, F1 SCORE - 0.439\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000732\n",
            "\n",
            "Epoch: [21][0/1405]\tBatch Time 0.494 (0.494)\tData Load Time 0.199 (0.199)\tCE Loss 13.1715 (13.1715)\tVB Loss 6.6405 (6.6405)\tF1 0.335 (0.335)\n",
            "Epoch: [21][100/1405]\tBatch Time 0.187 (0.259)\tData Load Time 0.005 (0.008)\tCE Loss 12.7883 (12.5431)\tVB Loss 5.6750 (5.8457)\tF1 0.401 (0.403)\n",
            "Epoch: [21][200/1405]\tBatch Time 0.224 (0.246)\tData Load Time 0.005 (0.006)\tCE Loss 13.1902 (12.5957)\tVB Loss 5.8877 (5.7925)\tF1 0.339 (0.399)\n",
            "Epoch: [21][300/1405]\tBatch Time 0.221 (0.234)\tData Load Time 0.005 (0.006)\tCE Loss 12.8108 (12.5748)\tVB Loss 7.3963 (5.7898)\tF1 0.378 (0.397)\n",
            "Epoch: [21][400/1405]\tBatch Time 0.227 (0.230)\tData Load Time 0.005 (0.006)\tCE Loss 13.2866 (12.5652)\tVB Loss 5.0505 (5.7125)\tF1 0.505 (0.399)\n",
            "Epoch: [21][500/1405]\tBatch Time 0.243 (0.228)\tData Load Time 0.008 (0.005)\tCE Loss 12.4138 (12.5679)\tVB Loss 6.6344 (5.7573)\tF1 0.251 (0.401)\n",
            "Epoch: [21][600/1405]\tBatch Time 0.192 (0.227)\tData Load Time 0.006 (0.005)\tCE Loss 11.8360 (12.5611)\tVB Loss 5.7181 (5.7641)\tF1 0.424 (0.403)\n",
            "Epoch: [21][700/1405]\tBatch Time 0.213 (0.226)\tData Load Time 0.004 (0.005)\tCE Loss 12.7391 (12.5630)\tVB Loss 4.9244 (5.7091)\tF1 0.572 (0.406)\n",
            "Epoch: [21][800/1405]\tBatch Time 0.155 (0.226)\tData Load Time 0.005 (0.005)\tCE Loss 12.2010 (12.5517)\tVB Loss 2.6813 (5.6650)\tF1 0.571 (0.407)\n",
            "Epoch: [21][900/1405]\tBatch Time 0.143 (0.225)\tData Load Time 0.005 (0.005)\tCE Loss 12.6370 (12.5562)\tVB Loss 3.4519 (5.6993)\tF1 0.392 (0.408)\n",
            "Epoch: [21][1000/1405]\tBatch Time 0.261 (0.225)\tData Load Time 0.004 (0.005)\tCE Loss 12.6045 (12.5546)\tVB Loss 6.1714 (5.6964)\tF1 0.487 (0.410)\n",
            "Epoch: [21][1100/1405]\tBatch Time 0.144 (0.225)\tData Load Time 0.004 (0.005)\tCE Loss 12.3089 (12.5450)\tVB Loss 3.2668 (5.6762)\tF1 0.184 (0.408)\n",
            "Epoch: [21][1200/1405]\tBatch Time 0.248 (0.225)\tData Load Time 0.005 (0.005)\tCE Loss 12.5080 (12.5411)\tVB Loss 10.0531 (5.6957)\tF1 0.306 (0.407)\n",
            "Epoch: [21][1300/1405]\tBatch Time 0.258 (0.225)\tData Load Time 0.005 (0.005)\tCE Loss 12.9244 (12.5406)\tVB Loss 5.4546 (5.6962)\tF1 0.408 (0.407)\n",
            "Epoch: [21][1400/1405]\tBatch Time 0.183 (0.225)\tData Load Time 0.004 (0.005)\tCE Loss 12.7683 (12.5380)\tVB Loss 6.3552 (5.6746)\tF1 0.378 (0.407)\n",
            "Validation: [0/325]\tBatch Time 0.290 (0.290)\tVB Loss 7.0148 (7.0148)\tF1 Score 0.413 (0.413)\t\n",
            "Validation: [100/325]\tBatch Time 0.094 (0.116)\tVB Loss 4.5197 (5.4409)\tF1 Score 0.516 (0.456)\t\n",
            "Validation: [200/325]\tBatch Time 0.099 (0.113)\tVB Loss 3.2773 (5.2265)\tF1 Score 0.558 (0.459)\t\n",
            "Validation: [300/325]\tBatch Time 0.123 (0.116)\tVB Loss 5.3513 (5.2651)\tF1 Score 0.552 (0.455)\t\n",
            "\n",
            " * LOSS - 5.239, F1 SCORE - 0.455\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000714\n",
            "\n",
            "Epoch: [22][0/1405]\tBatch Time 0.526 (0.526)\tData Load Time 0.192 (0.192)\tCE Loss 12.3355 (12.3355)\tVB Loss 6.3695 (6.3695)\tF1 0.524 (0.524)\n",
            "Epoch: [22][100/1405]\tBatch Time 0.269 (0.242)\tData Load Time 0.004 (0.007)\tCE Loss 13.0320 (12.5191)\tVB Loss 7.7406 (5.7425)\tF1 0.393 (0.423)\n",
            "Epoch: [22][200/1405]\tBatch Time 0.212 (0.230)\tData Load Time 0.004 (0.006)\tCE Loss 13.1397 (12.5689)\tVB Loss 4.9840 (5.5597)\tF1 0.490 (0.421)\n",
            "Epoch: [22][300/1405]\tBatch Time 0.154 (0.224)\tData Load Time 0.004 (0.006)\tCE Loss 11.4100 (12.5390)\tVB Loss 2.6867 (5.4912)\tF1 0.336 (0.420)\n",
            "Epoch: [22][400/1405]\tBatch Time 0.213 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 13.3935 (12.5154)\tVB Loss 5.6676 (5.4967)\tF1 0.428 (0.418)\n",
            "Epoch: [22][500/1405]\tBatch Time 0.221 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 12.7772 (12.4999)\tVB Loss 5.3854 (5.4794)\tF1 0.411 (0.420)\n",
            "Epoch: [22][600/1405]\tBatch Time 0.225 (0.224)\tData Load Time 0.006 (0.005)\tCE Loss 11.0434 (12.4963)\tVB Loss 5.1210 (5.4912)\tF1 0.371 (0.424)\n",
            "Epoch: [22][700/1405]\tBatch Time 0.190 (0.224)\tData Load Time 0.004 (0.005)\tCE Loss 12.4052 (12.4858)\tVB Loss 4.9929 (5.5241)\tF1 0.308 (0.424)\n",
            "Epoch: [22][800/1405]\tBatch Time 0.180 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 12.9839 (12.4768)\tVB Loss 5.2609 (5.5312)\tF1 0.458 (0.423)\n",
            "Epoch: [22][900/1405]\tBatch Time 0.237 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 12.2445 (12.4694)\tVB Loss 5.1918 (5.5269)\tF1 0.429 (0.423)\n",
            "Epoch: [22][1000/1405]\tBatch Time 0.241 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 12.6980 (12.4711)\tVB Loss 8.9826 (5.5037)\tF1 0.470 (0.422)\n",
            "Epoch: [22][1100/1405]\tBatch Time 0.186 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 13.0004 (12.4757)\tVB Loss 6.1845 (5.5322)\tF1 0.429 (0.421)\n",
            "Epoch: [22][1200/1405]\tBatch Time 0.257 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 13.2276 (12.4711)\tVB Loss 3.2644 (5.4919)\tF1 0.422 (0.424)\n",
            "Epoch: [22][1300/1405]\tBatch Time 0.132 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 12.8118 (12.4711)\tVB Loss 2.8277 (5.4916)\tF1 0.599 (0.424)\n",
            "Epoch: [22][1400/1405]\tBatch Time 0.184 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 13.1986 (12.4660)\tVB Loss 4.2804 (5.4983)\tF1 0.410 (0.425)\n",
            "Validation: [0/325]\tBatch Time 0.320 (0.320)\tVB Loss 2.9286 (2.9286)\tF1 Score 0.562 (0.562)\t\n",
            "Validation: [100/325]\tBatch Time 0.152 (0.119)\tVB Loss 5.9917 (5.1238)\tF1 Score 0.578 (0.485)\t\n",
            "Validation: [200/325]\tBatch Time 0.149 (0.122)\tVB Loss 4.3090 (5.1511)\tF1 Score 0.655 (0.472)\t\n",
            "Validation: [300/325]\tBatch Time 0.127 (0.123)\tVB Loss 7.4311 (5.0328)\tF1 Score 0.453 (0.485)\t\n",
            "\n",
            " * LOSS - 4.990, F1 SCORE - 0.485\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000698\n",
            "\n",
            "Epoch: [23][0/1405]\tBatch Time 0.439 (0.439)\tData Load Time 0.207 (0.207)\tCE Loss 12.1531 (12.1531)\tVB Loss 3.8685 (3.8685)\tF1 0.622 (0.622)\n",
            "Epoch: [23][100/1405]\tBatch Time 0.238 (0.255)\tData Load Time 0.005 (0.007)\tCE Loss 12.4198 (12.4721)\tVB Loss 5.8711 (5.5968)\tF1 0.575 (0.436)\n",
            "Epoch: [23][200/1405]\tBatch Time 0.172 (0.238)\tData Load Time 0.004 (0.006)\tCE Loss 11.9834 (12.4655)\tVB Loss 4.7382 (5.5219)\tF1 0.608 (0.430)\n",
            "Epoch: [23][300/1405]\tBatch Time 0.186 (0.230)\tData Load Time 0.005 (0.006)\tCE Loss 11.6705 (12.4251)\tVB Loss 4.2179 (5.4245)\tF1 0.480 (0.432)\n",
            "Epoch: [23][400/1405]\tBatch Time 0.196 (0.228)\tData Load Time 0.004 (0.005)\tCE Loss 12.8855 (12.4326)\tVB Loss 4.1001 (5.4199)\tF1 0.420 (0.435)\n",
            "Epoch: [23][500/1405]\tBatch Time 0.271 (0.226)\tData Load Time 0.005 (0.005)\tCE Loss 12.3306 (12.4179)\tVB Loss 3.4612 (5.3490)\tF1 0.426 (0.437)\n",
            "Epoch: [23][600/1405]\tBatch Time 0.180 (0.224)\tData Load Time 0.004 (0.005)\tCE Loss 12.2444 (12.4113)\tVB Loss 3.2951 (5.3248)\tF1 0.599 (0.440)\n",
            "Epoch: [23][700/1405]\tBatch Time 0.232 (0.225)\tData Load Time 0.004 (0.005)\tCE Loss 12.1840 (12.4109)\tVB Loss 8.2609 (5.3490)\tF1 0.424 (0.442)\n",
            "Epoch: [23][800/1405]\tBatch Time 0.223 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 12.4700 (12.4211)\tVB Loss 5.6753 (5.3411)\tF1 0.451 (0.442)\n",
            "Epoch: [23][900/1405]\tBatch Time 0.248 (0.224)\tData Load Time 0.004 (0.005)\tCE Loss 12.3161 (12.4093)\tVB Loss 5.6970 (5.3579)\tF1 0.524 (0.440)\n",
            "Epoch: [23][1000/1405]\tBatch Time 0.211 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 12.5372 (12.4019)\tVB Loss 7.3078 (5.3277)\tF1 0.383 (0.441)\n",
            "Epoch: [23][1100/1405]\tBatch Time 0.189 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 11.9460 (12.3982)\tVB Loss 5.0071 (5.3468)\tF1 0.419 (0.442)\n",
            "Epoch: [23][1200/1405]\tBatch Time 0.284 (0.221)\tData Load Time 0.005 (0.005)\tCE Loss 12.4885 (12.4034)\tVB Loss 7.5172 (5.3133)\tF1 0.499 (0.443)\n",
            "Epoch: [23][1300/1405]\tBatch Time 0.201 (0.221)\tData Load Time 0.005 (0.005)\tCE Loss 12.3257 (12.4027)\tVB Loss 4.0540 (5.3185)\tF1 0.414 (0.444)\n",
            "Epoch: [23][1400/1405]\tBatch Time 0.221 (0.221)\tData Load Time 0.004 (0.005)\tCE Loss 12.6100 (12.4033)\tVB Loss 5.7371 (5.3166)\tF1 0.626 (0.443)\n",
            "Validation: [0/325]\tBatch Time 0.304 (0.304)\tVB Loss 6.2633 (6.2633)\tF1 Score 0.520 (0.520)\t\n",
            "Validation: [100/325]\tBatch Time 0.120 (0.110)\tVB Loss 4.1111 (4.7381)\tF1 Score 0.540 (0.499)\t\n",
            "Validation: [200/325]\tBatch Time 0.126 (0.118)\tVB Loss 4.6108 (4.7132)\tF1 Score 0.798 (0.506)\t\n",
            "Validation: [300/325]\tBatch Time 0.153 (0.118)\tVB Loss 7.8032 (4.7667)\tF1 Score 0.353 (0.505)\t\n",
            "\n",
            " * LOSS - 4.813, F1 SCORE - 0.503\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000682\n",
            "\n",
            "Epoch: [24][0/1405]\tBatch Time 0.427 (0.427)\tData Load Time 0.212 (0.212)\tCE Loss 11.4359 (11.4359)\tVB Loss 5.2153 (5.2153)\tF1 0.431 (0.431)\n",
            "Epoch: [24][100/1405]\tBatch Time 0.207 (0.247)\tData Load Time 0.004 (0.007)\tCE Loss 12.9797 (12.3795)\tVB Loss 5.3746 (5.2984)\tF1 0.426 (0.450)\n",
            "Epoch: [24][200/1405]\tBatch Time 0.266 (0.234)\tData Load Time 0.004 (0.006)\tCE Loss 12.1720 (12.3619)\tVB Loss 7.8414 (5.2807)\tF1 0.488 (0.460)\n",
            "Epoch: [24][300/1405]\tBatch Time 0.173 (0.227)\tData Load Time 0.004 (0.006)\tCE Loss 13.4912 (12.3601)\tVB Loss 4.7307 (5.2256)\tF1 0.535 (0.464)\n",
            "Epoch: [24][400/1405]\tBatch Time 0.187 (0.226)\tData Load Time 0.005 (0.005)\tCE Loss 11.5273 (12.3649)\tVB Loss 3.5129 (5.2371)\tF1 0.444 (0.461)\n",
            "Epoch: [24][500/1405]\tBatch Time 0.245 (0.225)\tData Load Time 0.004 (0.005)\tCE Loss 12.2269 (12.3798)\tVB Loss 4.5872 (5.2003)\tF1 0.367 (0.458)\n",
            "Epoch: [24][600/1405]\tBatch Time 0.259 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 11.7836 (12.3900)\tVB Loss 4.8908 (5.1948)\tF1 0.444 (0.457)\n",
            "Epoch: [24][700/1405]\tBatch Time 0.185 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 11.3645 (12.3810)\tVB Loss 4.0911 (5.1647)\tF1 0.519 (0.461)\n",
            "Epoch: [24][800/1405]\tBatch Time 0.212 (0.222)\tData Load Time 0.006 (0.005)\tCE Loss 12.0850 (12.3765)\tVB Loss 4.0997 (5.1514)\tF1 0.510 (0.458)\n",
            "Epoch: [24][900/1405]\tBatch Time 0.229 (0.221)\tData Load Time 0.005 (0.005)\tCE Loss 10.9234 (12.3682)\tVB Loss 6.2271 (5.1280)\tF1 0.355 (0.459)\n",
            "Epoch: [24][1000/1405]\tBatch Time 0.156 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 12.7055 (12.3664)\tVB Loss 4.5228 (5.1450)\tF1 0.477 (0.460)\n",
            "Epoch: [24][1100/1405]\tBatch Time 0.174 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 13.0320 (12.3638)\tVB Loss 3.0241 (5.1304)\tF1 0.323 (0.461)\n",
            "Epoch: [24][1200/1405]\tBatch Time 0.242 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 12.6224 (12.3620)\tVB Loss 6.3503 (5.1354)\tF1 0.457 (0.461)\n",
            "Epoch: [24][1300/1405]\tBatch Time 0.228 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 11.7371 (12.3574)\tVB Loss 5.9562 (5.1261)\tF1 0.344 (0.462)\n",
            "Epoch: [24][1400/1405]\tBatch Time 0.208 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 12.6905 (12.3531)\tVB Loss 9.8361 (5.1429)\tF1 0.357 (0.463)\n",
            "Validation: [0/325]\tBatch Time 0.298 (0.298)\tVB Loss 6.3265 (6.3265)\tF1 Score 0.397 (0.397)\t\n",
            "Validation: [100/325]\tBatch Time 0.130 (0.111)\tVB Loss 4.2852 (4.5209)\tF1 Score 0.303 (0.507)\t\n",
            "Validation: [200/325]\tBatch Time 0.141 (0.111)\tVB Loss 5.0981 (4.7079)\tF1 Score 0.474 (0.502)\t\n",
            "Validation: [300/325]\tBatch Time 0.129 (0.114)\tVB Loss 11.0719 (4.6468)\tF1 Score 0.208 (0.509)\t\n",
            "\n",
            " * LOSS - 4.689, F1 SCORE - 0.506\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000667\n",
            "\n",
            "Epoch: [25][0/1405]\tBatch Time 0.528 (0.528)\tData Load Time 0.263 (0.263)\tCE Loss 12.3811 (12.3811)\tVB Loss 2.6420 (2.6420)\tF1 0.445 (0.445)\n",
            "Epoch: [25][100/1405]\tBatch Time 0.246 (0.253)\tData Load Time 0.006 (0.008)\tCE Loss 11.9288 (12.3083)\tVB Loss 5.5660 (5.2534)\tF1 0.448 (0.471)\n",
            "Epoch: [25][200/1405]\tBatch Time 0.180 (0.233)\tData Load Time 0.005 (0.006)\tCE Loss 13.2743 (12.3234)\tVB Loss 5.3573 (5.2090)\tF1 0.241 (0.473)\n",
            "Epoch: [25][300/1405]\tBatch Time 0.207 (0.229)\tData Load Time 0.004 (0.006)\tCE Loss 12.6612 (12.3070)\tVB Loss 4.0192 (5.1469)\tF1 0.558 (0.472)\n",
            "Epoch: [25][400/1405]\tBatch Time 0.308 (0.228)\tData Load Time 0.006 (0.006)\tCE Loss 12.7066 (12.3144)\tVB Loss 4.1328 (5.0995)\tF1 0.453 (0.475)\n",
            "Epoch: [25][500/1405]\tBatch Time 0.245 (0.226)\tData Load Time 0.006 (0.005)\tCE Loss 11.4937 (12.3153)\tVB Loss 5.0319 (5.0513)\tF1 0.280 (0.472)\n",
            "Epoch: [25][600/1405]\tBatch Time 0.212 (0.226)\tData Load Time 0.004 (0.005)\tCE Loss 12.9427 (12.2977)\tVB Loss 6.9940 (5.0380)\tF1 0.256 (0.472)\n",
            "Epoch: [25][700/1405]\tBatch Time 0.170 (0.226)\tData Load Time 0.004 (0.005)\tCE Loss 12.0003 (12.2960)\tVB Loss 5.3819 (5.0326)\tF1 0.354 (0.473)\n",
            "Epoch: [25][800/1405]\tBatch Time 0.271 (0.226)\tData Load Time 0.006 (0.005)\tCE Loss 12.3476 (12.2981)\tVB Loss 2.9510 (5.0205)\tF1 0.633 (0.475)\n",
            "Epoch: [25][900/1405]\tBatch Time 0.184 (0.226)\tData Load Time 0.005 (0.005)\tCE Loss 12.1571 (12.3083)\tVB Loss 2.2194 (5.0147)\tF1 0.582 (0.474)\n",
            "Epoch: [25][1000/1405]\tBatch Time 0.266 (0.225)\tData Load Time 0.004 (0.005)\tCE Loss 12.4221 (12.3111)\tVB Loss 4.4600 (5.0213)\tF1 0.346 (0.472)\n",
            "Epoch: [25][1100/1405]\tBatch Time 0.208 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 11.5426 (12.3162)\tVB Loss 3.9780 (4.9786)\tF1 0.319 (0.473)\n",
            "Epoch: [25][1200/1405]\tBatch Time 0.193 (0.225)\tData Load Time 0.004 (0.005)\tCE Loss 12.2661 (12.3119)\tVB Loss 4.7348 (4.9733)\tF1 0.377 (0.475)\n",
            "Epoch: [25][1300/1405]\tBatch Time 0.293 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 12.2933 (12.3044)\tVB Loss 5.3018 (4.9544)\tF1 0.319 (0.476)\n",
            "Epoch: [25][1400/1405]\tBatch Time 0.089 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 12.4093 (12.3038)\tVB Loss 3.5367 (4.9597)\tF1 0.409 (0.476)\n",
            "Validation: [0/325]\tBatch Time 0.279 (0.279)\tVB Loss 4.9305 (4.9305)\tF1 Score 0.474 (0.474)\t\n",
            "Validation: [100/325]\tBatch Time 0.153 (0.114)\tVB Loss 3.6166 (4.5838)\tF1 Score 0.567 (0.523)\t\n",
            "Validation: [200/325]\tBatch Time 0.120 (0.114)\tVB Loss 3.9063 (4.4350)\tF1 Score 0.516 (0.525)\t\n",
            "Validation: [300/325]\tBatch Time 0.121 (0.117)\tVB Loss 2.1463 (4.4806)\tF1 Score 0.725 (0.533)\t\n",
            "\n",
            " * LOSS - 4.452, F1 SCORE - 0.534\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000652\n",
            "\n",
            "Epoch: [26][0/1405]\tBatch Time 0.471 (0.471)\tData Load Time 0.194 (0.194)\tCE Loss 11.5952 (11.5952)\tVB Loss 6.1142 (6.1142)\tF1 0.598 (0.598)\n",
            "Epoch: [26][100/1405]\tBatch Time 0.160 (0.247)\tData Load Time 0.004 (0.007)\tCE Loss 11.5839 (12.2687)\tVB Loss 6.1938 (4.9352)\tF1 0.473 (0.489)\n",
            "Epoch: [26][200/1405]\tBatch Time 0.182 (0.233)\tData Load Time 0.004 (0.006)\tCE Loss 12.1329 (12.3204)\tVB Loss 5.7411 (4.8331)\tF1 0.351 (0.483)\n",
            "Epoch: [26][300/1405]\tBatch Time 0.227 (0.227)\tData Load Time 0.006 (0.006)\tCE Loss 12.1645 (12.2800)\tVB Loss 5.2966 (4.8894)\tF1 0.505 (0.481)\n",
            "Epoch: [26][400/1405]\tBatch Time 0.257 (0.226)\tData Load Time 0.005 (0.005)\tCE Loss 12.4959 (12.2824)\tVB Loss 6.3245 (4.8836)\tF1 0.472 (0.483)\n",
            "Epoch: [26][500/1405]\tBatch Time 0.189 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 12.8712 (12.2813)\tVB Loss 3.9626 (4.8805)\tF1 0.598 (0.483)\n",
            "Epoch: [26][600/1405]\tBatch Time 0.170 (0.220)\tData Load Time 0.004 (0.005)\tCE Loss 12.6846 (12.2769)\tVB Loss 2.3403 (4.8455)\tF1 0.581 (0.482)\n",
            "Epoch: [26][700/1405]\tBatch Time 0.240 (0.219)\tData Load Time 0.005 (0.005)\tCE Loss 12.5066 (12.2720)\tVB Loss 5.9575 (4.8623)\tF1 0.513 (0.482)\n",
            "Epoch: [26][800/1405]\tBatch Time 0.278 (0.218)\tData Load Time 0.005 (0.005)\tCE Loss 12.8634 (12.2769)\tVB Loss 5.1747 (4.8405)\tF1 0.507 (0.482)\n",
            "Epoch: [26][900/1405]\tBatch Time 0.243 (0.217)\tData Load Time 0.004 (0.005)\tCE Loss 12.1877 (12.2776)\tVB Loss 7.6286 (4.8591)\tF1 0.253 (0.481)\n",
            "Epoch: [26][1000/1405]\tBatch Time 0.241 (0.218)\tData Load Time 0.005 (0.005)\tCE Loss 11.4478 (12.2791)\tVB Loss 3.9741 (4.8626)\tF1 0.466 (0.483)\n",
            "Epoch: [26][1100/1405]\tBatch Time 0.237 (0.219)\tData Load Time 0.005 (0.005)\tCE Loss 11.6920 (12.2756)\tVB Loss 6.6113 (4.8642)\tF1 0.401 (0.482)\n",
            "Epoch: [26][1200/1405]\tBatch Time 0.205 (0.219)\tData Load Time 0.004 (0.005)\tCE Loss 11.9731 (12.2770)\tVB Loss 6.0770 (4.8444)\tF1 0.275 (0.484)\n",
            "Epoch: [26][1300/1405]\tBatch Time 0.181 (0.218)\tData Load Time 0.004 (0.005)\tCE Loss 12.8283 (12.2678)\tVB Loss 4.6847 (4.8300)\tF1 0.343 (0.484)\n",
            "Epoch: [26][1400/1405]\tBatch Time 0.235 (0.219)\tData Load Time 0.005 (0.005)\tCE Loss 12.6121 (12.2665)\tVB Loss 5.2466 (4.8148)\tF1 0.513 (0.485)\n",
            "Validation: [0/325]\tBatch Time 0.276 (0.276)\tVB Loss 6.2494 (6.2494)\tF1 Score 0.470 (0.470)\t\n",
            "Validation: [100/325]\tBatch Time 0.088 (0.111)\tVB Loss 3.9235 (4.2605)\tF1 Score 0.273 (0.537)\t\n",
            "Validation: [200/325]\tBatch Time 0.114 (0.110)\tVB Loss 6.4534 (4.3657)\tF1 Score 0.461 (0.529)\t\n",
            "Validation: [300/325]\tBatch Time 0.088 (0.111)\tVB Loss 2.2970 (4.3634)\tF1 Score 0.711 (0.539)\t\n",
            "\n",
            " * LOSS - 4.346, F1 SCORE - 0.539\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000638\n",
            "\n",
            "Epoch: [27][0/1405]\tBatch Time 0.357 (0.357)\tData Load Time 0.192 (0.192)\tCE Loss 12.5309 (12.5309)\tVB Loss 4.3416 (4.3416)\tF1 0.701 (0.701)\n",
            "Epoch: [27][100/1405]\tBatch Time 0.239 (0.244)\tData Load Time 0.005 (0.007)\tCE Loss 12.9756 (12.3272)\tVB Loss 4.5068 (4.6163)\tF1 0.466 (0.506)\n",
            "Epoch: [27][200/1405]\tBatch Time 0.172 (0.231)\tData Load Time 0.004 (0.006)\tCE Loss 12.6646 (12.2571)\tVB Loss 5.2072 (4.7085)\tF1 0.537 (0.505)\n",
            "Epoch: [27][300/1405]\tBatch Time 0.237 (0.227)\tData Load Time 0.006 (0.006)\tCE Loss 11.9580 (12.2403)\tVB Loss 4.0167 (4.7161)\tF1 0.585 (0.505)\n",
            "Epoch: [27][400/1405]\tBatch Time 0.218 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 12.5016 (12.2526)\tVB Loss 4.3973 (4.6963)\tF1 0.337 (0.501)\n",
            "Epoch: [27][500/1405]\tBatch Time 0.226 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 11.9609 (12.2556)\tVB Loss 5.6118 (4.6978)\tF1 0.443 (0.499)\n",
            "Epoch: [27][600/1405]\tBatch Time 0.217 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 11.7462 (12.2352)\tVB Loss 4.1420 (4.6834)\tF1 0.490 (0.502)\n",
            "Epoch: [27][700/1405]\tBatch Time 0.237 (0.221)\tData Load Time 0.004 (0.005)\tCE Loss 12.5336 (12.2299)\tVB Loss 3.0030 (4.6642)\tF1 0.727 (0.502)\n",
            "Epoch: [27][800/1405]\tBatch Time 0.217 (0.221)\tData Load Time 0.005 (0.005)\tCE Loss 11.9943 (12.2333)\tVB Loss 4.9531 (4.6845)\tF1 0.654 (0.501)\n",
            "Epoch: [27][900/1405]\tBatch Time 0.214 (0.219)\tData Load Time 0.004 (0.005)\tCE Loss 12.0173 (12.2335)\tVB Loss 2.8254 (4.6731)\tF1 0.771 (0.502)\n",
            "Epoch: [27][1000/1405]\tBatch Time 0.246 (0.219)\tData Load Time 0.004 (0.005)\tCE Loss 11.9676 (12.2308)\tVB Loss 3.8922 (4.6671)\tF1 0.686 (0.501)\n",
            "Epoch: [27][1100/1405]\tBatch Time 0.227 (0.220)\tData Load Time 0.005 (0.005)\tCE Loss 11.9529 (12.2301)\tVB Loss 7.7864 (4.6736)\tF1 0.390 (0.499)\n",
            "Epoch: [27][1200/1405]\tBatch Time 0.277 (0.220)\tData Load Time 0.006 (0.005)\tCE Loss 11.9464 (12.2304)\tVB Loss 6.8980 (4.6779)\tF1 0.231 (0.498)\n",
            "Epoch: [27][1300/1405]\tBatch Time 0.255 (0.221)\tData Load Time 0.006 (0.005)\tCE Loss 12.4938 (12.2314)\tVB Loss 2.5358 (4.6933)\tF1 0.599 (0.497)\n",
            "Epoch: [27][1400/1405]\tBatch Time 0.161 (0.221)\tData Load Time 0.006 (0.005)\tCE Loss 12.1156 (12.2290)\tVB Loss 1.6724 (4.6842)\tF1 0.754 (0.497)\n",
            "Validation: [0/325]\tBatch Time 0.323 (0.323)\tVB Loss 2.7700 (2.7700)\tF1 Score 0.616 (0.616)\t\n",
            "Validation: [100/325]\tBatch Time 0.084 (0.108)\tVB Loss 3.7451 (4.1511)\tF1 Score 0.496 (0.559)\t\n",
            "Validation: [200/325]\tBatch Time 0.162 (0.113)\tVB Loss 6.3630 (4.2205)\tF1 Score 0.509 (0.548)\t\n",
            "Validation: [300/325]\tBatch Time 0.119 (0.118)\tVB Loss 4.6048 (4.1967)\tF1 Score 0.323 (0.546)\t\n",
            "\n",
            " * LOSS - 4.178, F1 SCORE - 0.547\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000625\n",
            "\n",
            "Epoch: [28][0/1405]\tBatch Time 0.495 (0.495)\tData Load Time 0.213 (0.213)\tCE Loss 12.6893 (12.6893)\tVB Loss 3.3306 (3.3306)\tF1 0.682 (0.682)\n",
            "Epoch: [28][100/1405]\tBatch Time 0.207 (0.252)\tData Load Time 0.004 (0.007)\tCE Loss 12.6814 (12.2697)\tVB Loss 2.7316 (4.6376)\tF1 0.492 (0.515)\n",
            "Epoch: [28][200/1405]\tBatch Time 0.155 (0.236)\tData Load Time 0.004 (0.006)\tCE Loss 11.6489 (12.2003)\tVB Loss 1.4789 (4.5494)\tF1 0.633 (0.517)\n",
            "Epoch: [28][300/1405]\tBatch Time 0.215 (0.229)\tData Load Time 0.005 (0.006)\tCE Loss 11.9706 (12.2114)\tVB Loss 3.7320 (4.4718)\tF1 0.722 (0.525)\n",
            "Epoch: [28][400/1405]\tBatch Time 0.226 (0.226)\tData Load Time 0.004 (0.005)\tCE Loss 12.1904 (12.2105)\tVB Loss 8.7149 (4.5411)\tF1 0.544 (0.516)\n",
            "Epoch: [28][500/1405]\tBatch Time 0.123 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 12.3210 (12.2079)\tVB Loss 2.0656 (4.5800)\tF1 0.873 (0.513)\n",
            "Epoch: [28][600/1405]\tBatch Time 0.180 (0.224)\tData Load Time 0.004 (0.005)\tCE Loss 12.4248 (12.2155)\tVB Loss 2.4359 (4.6080)\tF1 0.662 (0.507)\n",
            "Epoch: [28][700/1405]\tBatch Time 0.088 (0.221)\tData Load Time 0.006 (0.005)\tCE Loss 13.2077 (12.1938)\tVB Loss 1.5332 (4.5414)\tF1 0.547 (0.512)\n",
            "Epoch: [28][800/1405]\tBatch Time 0.268 (0.220)\tData Load Time 0.005 (0.005)\tCE Loss 12.1969 (12.1894)\tVB Loss 6.2733 (4.5308)\tF1 0.467 (0.512)\n",
            "Epoch: [28][900/1405]\tBatch Time 0.129 (0.220)\tData Load Time 0.004 (0.005)\tCE Loss 11.6173 (12.1914)\tVB Loss 1.3581 (4.5369)\tF1 0.711 (0.511)\n",
            "Epoch: [28][1000/1405]\tBatch Time 0.274 (0.220)\tData Load Time 0.004 (0.005)\tCE Loss 12.6277 (12.1967)\tVB Loss 6.4203 (4.5530)\tF1 0.489 (0.512)\n",
            "Epoch: [28][1100/1405]\tBatch Time 0.202 (0.220)\tData Load Time 0.005 (0.005)\tCE Loss 11.6293 (12.1936)\tVB Loss 2.2064 (4.5286)\tF1 0.658 (0.514)\n",
            "Epoch: [28][1200/1405]\tBatch Time 0.250 (0.221)\tData Load Time 0.005 (0.005)\tCE Loss 11.8968 (12.1935)\tVB Loss 5.4760 (4.5312)\tF1 0.591 (0.512)\n",
            "Epoch: [28][1300/1405]\tBatch Time 0.262 (0.221)\tData Load Time 0.005 (0.005)\tCE Loss 13.0846 (12.1936)\tVB Loss 3.6251 (4.5385)\tF1 0.530 (0.511)\n",
            "Epoch: [28][1400/1405]\tBatch Time 0.209 (0.221)\tData Load Time 0.004 (0.005)\tCE Loss 11.8454 (12.1940)\tVB Loss 2.1431 (4.5416)\tF1 0.470 (0.510)\n",
            "Validation: [0/325]\tBatch Time 0.310 (0.310)\tVB Loss 2.7817 (2.7817)\tF1 Score 0.613 (0.613)\t\n",
            "Validation: [100/325]\tBatch Time 0.114 (0.111)\tVB Loss 6.8971 (3.7456)\tF1 Score 0.341 (0.596)\t\n",
            "Validation: [200/325]\tBatch Time 0.099 (0.112)\tVB Loss 3.5197 (3.9677)\tF1 Score 0.640 (0.587)\t\n",
            "Validation: [300/325]\tBatch Time 0.128 (0.114)\tVB Loss 4.4456 (4.0232)\tF1 Score 0.488 (0.576)\t\n",
            "\n",
            " * LOSS - 4.032, F1 SCORE - 0.576\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000612\n",
            "\n",
            "Epoch: [29][0/1405]\tBatch Time 0.527 (0.527)\tData Load Time 0.201 (0.201)\tCE Loss 12.7434 (12.7434)\tVB Loss 6.0849 (6.0849)\tF1 0.542 (0.542)\n",
            "Epoch: [29][100/1405]\tBatch Time 0.207 (0.244)\tData Load Time 0.005 (0.007)\tCE Loss 11.8274 (12.1104)\tVB Loss 3.0774 (4.3466)\tF1 0.633 (0.516)\n",
            "Epoch: [29][200/1405]\tBatch Time 0.269 (0.236)\tData Load Time 0.006 (0.006)\tCE Loss 11.6435 (12.1894)\tVB Loss 4.0570 (4.4722)\tF1 0.614 (0.527)\n",
            "Epoch: [29][300/1405]\tBatch Time 0.242 (0.229)\tData Load Time 0.004 (0.006)\tCE Loss 11.5571 (12.1824)\tVB Loss 5.1475 (4.3735)\tF1 0.456 (0.529)\n",
            "Epoch: [29][400/1405]\tBatch Time 0.247 (0.227)\tData Load Time 0.005 (0.005)\tCE Loss 12.1581 (12.1735)\tVB Loss 5.7632 (4.3525)\tF1 0.491 (0.528)\n",
            "Epoch: [29][500/1405]\tBatch Time 0.191 (0.226)\tData Load Time 0.005 (0.005)\tCE Loss 11.8509 (12.1871)\tVB Loss 7.0421 (4.3698)\tF1 0.370 (0.531)\n",
            "Epoch: [29][600/1405]\tBatch Time 0.242 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 12.4660 (12.1845)\tVB Loss 5.2849 (4.3663)\tF1 0.552 (0.529)\n",
            "Epoch: [29][700/1405]\tBatch Time 0.214 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 12.0152 (12.1871)\tVB Loss 6.1236 (4.3876)\tF1 0.491 (0.529)\n",
            "Epoch: [29][800/1405]\tBatch Time 0.234 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 12.0249 (12.1772)\tVB Loss 3.9125 (4.3805)\tF1 0.602 (0.527)\n",
            "Epoch: [29][900/1405]\tBatch Time 0.209 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 12.0631 (12.1814)\tVB Loss 3.3686 (4.3976)\tF1 0.636 (0.527)\n",
            "Epoch: [29][1000/1405]\tBatch Time 0.247 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 12.4082 (12.1754)\tVB Loss 6.6647 (4.4134)\tF1 0.437 (0.525)\n",
            "Epoch: [29][1100/1405]\tBatch Time 0.221 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 12.0095 (12.1683)\tVB Loss 4.5033 (4.4061)\tF1 0.542 (0.525)\n",
            "Epoch: [29][1200/1405]\tBatch Time 0.200 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 12.3920 (12.1687)\tVB Loss 5.2589 (4.4056)\tF1 0.385 (0.525)\n",
            "Epoch: [29][1300/1405]\tBatch Time 0.246 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 11.8464 (12.1657)\tVB Loss 7.4892 (4.3994)\tF1 0.600 (0.526)\n",
            "Epoch: [29][1400/1405]\tBatch Time 0.288 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 11.8840 (12.1629)\tVB Loss 7.5955 (4.4115)\tF1 0.534 (0.528)\n",
            "Validation: [0/325]\tBatch Time 0.358 (0.358)\tVB Loss 3.0680 (3.0680)\tF1 Score 0.733 (0.733)\t\n",
            "Validation: [100/325]\tBatch Time 0.115 (0.107)\tVB Loss 3.6490 (3.7500)\tF1 Score 0.451 (0.598)\t\n",
            "Validation: [200/325]\tBatch Time 0.104 (0.113)\tVB Loss 2.8856 (3.7881)\tF1 Score 0.624 (0.596)\t\n",
            "Validation: [300/325]\tBatch Time 0.113 (0.116)\tVB Loss 3.3563 (3.8362)\tF1 Score 0.649 (0.598)\t\n",
            "\n",
            " * LOSS - 3.855, F1 SCORE - 0.600\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000600\n",
            "\n",
            "Epoch: [30][0/1405]\tBatch Time 0.541 (0.541)\tData Load Time 0.230 (0.230)\tCE Loss 12.6884 (12.6884)\tVB Loss 2.9973 (2.9973)\tF1 0.662 (0.662)\n",
            "Epoch: [30][100/1405]\tBatch Time 0.245 (0.245)\tData Load Time 0.006 (0.007)\tCE Loss 11.1620 (12.0896)\tVB Loss 3.2305 (4.0791)\tF1 0.504 (0.544)\n",
            "Epoch: [30][200/1405]\tBatch Time 0.188 (0.234)\tData Load Time 0.005 (0.006)\tCE Loss 11.2665 (12.1395)\tVB Loss 1.9743 (4.1920)\tF1 0.936 (0.536)\n",
            "Epoch: [30][300/1405]\tBatch Time 0.277 (0.231)\tData Load Time 0.004 (0.006)\tCE Loss 13.0324 (12.1366)\tVB Loss 4.7609 (4.2536)\tF1 0.304 (0.536)\n",
            "Epoch: [30][400/1405]\tBatch Time 0.230 (0.228)\tData Load Time 0.004 (0.005)\tCE Loss 11.8975 (12.1443)\tVB Loss 6.3477 (4.2746)\tF1 0.524 (0.537)\n",
            "Epoch: [30][500/1405]\tBatch Time 0.224 (0.225)\tData Load Time 0.005 (0.005)\tCE Loss 11.8334 (12.1353)\tVB Loss 3.7537 (4.2649)\tF1 0.500 (0.536)\n",
            "Epoch: [30][600/1405]\tBatch Time 0.203 (0.224)\tData Load Time 0.004 (0.005)\tCE Loss 12.1360 (12.1288)\tVB Loss 4.2658 (4.2859)\tF1 0.510 (0.536)\n",
            "Epoch: [30][700/1405]\tBatch Time 0.153 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 11.8977 (12.1299)\tVB Loss 2.1367 (4.2948)\tF1 0.574 (0.534)\n",
            "Epoch: [30][800/1405]\tBatch Time 0.235 (0.221)\tData Load Time 0.004 (0.005)\tCE Loss 12.7920 (12.1396)\tVB Loss 5.9434 (4.3053)\tF1 0.486 (0.536)\n",
            "Epoch: [30][900/1405]\tBatch Time 0.194 (0.221)\tData Load Time 0.004 (0.005)\tCE Loss 13.4896 (12.1398)\tVB Loss 2.6907 (4.2993)\tF1 0.804 (0.537)\n",
            "Epoch: [30][1000/1405]\tBatch Time 0.218 (0.221)\tData Load Time 0.004 (0.005)\tCE Loss 12.0254 (12.1411)\tVB Loss 4.3785 (4.2852)\tF1 0.694 (0.536)\n",
            "Epoch: [30][1100/1405]\tBatch Time 0.209 (0.220)\tData Load Time 0.005 (0.005)\tCE Loss 12.1162 (12.1334)\tVB Loss 2.8398 (4.2913)\tF1 0.689 (0.538)\n",
            "Epoch: [30][1200/1405]\tBatch Time 0.199 (0.219)\tData Load Time 0.004 (0.005)\tCE Loss 11.9618 (12.1303)\tVB Loss 3.7166 (4.3010)\tF1 0.648 (0.535)\n",
            "Epoch: [30][1300/1405]\tBatch Time 0.214 (0.219)\tData Load Time 0.005 (0.005)\tCE Loss 11.9235 (12.1278)\tVB Loss 4.7469 (4.3048)\tF1 0.320 (0.535)\n",
            "Epoch: [30][1400/1405]\tBatch Time 0.240 (0.219)\tData Load Time 0.005 (0.005)\tCE Loss 11.5199 (12.1277)\tVB Loss 3.7775 (4.3053)\tF1 0.631 (0.535)\n",
            "Validation: [0/325]\tBatch Time 0.320 (0.320)\tVB Loss 3.7377 (3.7377)\tF1 Score 0.583 (0.583)\t\n",
            "Validation: [100/325]\tBatch Time 0.107 (0.123)\tVB Loss 2.3053 (3.7055)\tF1 Score 0.769 (0.594)\t\n",
            "Validation: [200/325]\tBatch Time 0.147 (0.124)\tVB Loss 3.1333 (3.8302)\tF1 Score 0.639 (0.600)\t\n",
            "Validation: [300/325]\tBatch Time 0.136 (0.122)\tVB Loss 3.3546 (3.8463)\tF1 Score 0.671 (0.601)\t\n",
            "\n",
            " * LOSS - 3.780, F1 SCORE - 0.604\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000588\n",
            "\n",
            "Epoch: [31][0/1405]\tBatch Time 0.394 (0.394)\tData Load Time 0.187 (0.187)\tCE Loss 11.8202 (11.8202)\tVB Loss 4.0994 (4.0994)\tF1 0.569 (0.569)\n",
            "Epoch: [31][100/1405]\tBatch Time 0.240 (0.253)\tData Load Time 0.005 (0.007)\tCE Loss 12.8937 (12.0942)\tVB Loss 5.5229 (4.1841)\tF1 0.394 (0.546)\n",
            "Epoch: [31][200/1405]\tBatch Time 0.228 (0.238)\tData Load Time 0.005 (0.006)\tCE Loss 12.3570 (12.1221)\tVB Loss 4.4495 (4.1491)\tF1 0.470 (0.546)\n",
            "Epoch: [31][300/1405]\tBatch Time 0.238 (0.236)\tData Load Time 0.004 (0.006)\tCE Loss 12.1998 (12.1106)\tVB Loss 5.3822 (4.2211)\tF1 0.467 (0.538)\n",
            "Epoch: [31][400/1405]\tBatch Time 0.297 (0.232)\tData Load Time 0.006 (0.005)\tCE Loss 12.6385 (12.1089)\tVB Loss 3.1576 (4.1159)\tF1 0.506 (0.544)\n",
            "Epoch: [31][500/1405]\tBatch Time 0.203 (0.229)\tData Load Time 0.006 (0.005)\tCE Loss 11.7505 (12.1002)\tVB Loss 3.1409 (4.1405)\tF1 0.654 (0.544)\n",
            "Epoch: [31][600/1405]\tBatch Time 0.279 (0.226)\tData Load Time 0.005 (0.005)\tCE Loss 12.0386 (12.0873)\tVB Loss 6.1444 (4.1319)\tF1 0.391 (0.545)\n",
            "Epoch: [31][700/1405]\tBatch Time 0.168 (0.226)\tData Load Time 0.006 (0.005)\tCE Loss 10.7929 (12.0963)\tVB Loss 3.2343 (4.1450)\tF1 0.725 (0.544)\n",
            "Epoch: [31][800/1405]\tBatch Time 0.192 (0.224)\tData Load Time 0.006 (0.005)\tCE Loss 11.0909 (12.0988)\tVB Loss 2.3295 (4.1449)\tF1 0.588 (0.545)\n",
            "Epoch: [31][900/1405]\tBatch Time 0.239 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 12.1037 (12.1083)\tVB Loss 7.4304 (4.1488)\tF1 0.466 (0.545)\n",
            "Epoch: [31][1000/1405]\tBatch Time 0.288 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 10.9094 (12.1078)\tVB Loss 5.2495 (4.1466)\tF1 0.552 (0.547)\n",
            "Epoch: [31][1100/1405]\tBatch Time 0.120 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 10.8701 (12.1041)\tVB Loss 2.2261 (4.1538)\tF1 0.402 (0.546)\n",
            "Epoch: [31][1200/1405]\tBatch Time 0.209 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 12.2249 (12.1007)\tVB Loss 4.1407 (4.1742)\tF1 0.442 (0.547)\n",
            "Epoch: [31][1300/1405]\tBatch Time 0.196 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 11.3749 (12.0992)\tVB Loss 3.8889 (4.1830)\tF1 0.669 (0.549)\n",
            "Epoch: [31][1400/1405]\tBatch Time 0.262 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 11.3758 (12.0982)\tVB Loss 4.4148 (4.1755)\tF1 0.747 (0.549)\n",
            "Validation: [0/325]\tBatch Time 0.260 (0.260)\tVB Loss 2.1018 (2.1018)\tF1 Score 0.495 (0.495)\t\n",
            "Validation: [100/325]\tBatch Time 0.152 (0.124)\tVB Loss 3.7208 (3.7937)\tF1 Score 0.819 (0.617)\t\n",
            "Validation: [200/325]\tBatch Time 0.137 (0.121)\tVB Loss 5.5211 (3.5877)\tF1 Score 0.495 (0.630)\t\n",
            "Validation: [300/325]\tBatch Time 0.126 (0.121)\tVB Loss 4.9252 (3.6410)\tF1 Score 0.614 (0.630)\t\n",
            "\n",
            " * LOSS - 3.629, F1 SCORE - 0.631\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000577\n",
            "\n",
            "Epoch: [32][0/1405]\tBatch Time 0.545 (0.545)\tData Load Time 0.238 (0.238)\tCE Loss 11.6422 (11.6422)\tVB Loss 7.5895 (7.5895)\tF1 0.440 (0.440)\n",
            "Epoch: [32][100/1405]\tBatch Time 0.177 (0.250)\tData Load Time 0.004 (0.008)\tCE Loss 11.9069 (12.1019)\tVB Loss 5.8688 (4.1469)\tF1 0.186 (0.555)\n",
            "Epoch: [32][200/1405]\tBatch Time 0.140 (0.239)\tData Load Time 0.004 (0.007)\tCE Loss 11.7527 (12.0808)\tVB Loss 1.3676 (4.1071)\tF1 0.609 (0.558)\n",
            "Epoch: [32][300/1405]\tBatch Time 0.313 (0.233)\tData Load Time 0.004 (0.006)\tCE Loss 12.3734 (12.0896)\tVB Loss 5.2551 (4.1397)\tF1 0.493 (0.558)\n",
            "Epoch: [32][400/1405]\tBatch Time 0.207 (0.231)\tData Load Time 0.004 (0.006)\tCE Loss 12.4972 (12.0708)\tVB Loss 2.5606 (4.1108)\tF1 0.678 (0.559)\n",
            "Epoch: [32][500/1405]\tBatch Time 0.143 (0.230)\tData Load Time 0.005 (0.006)\tCE Loss 11.7648 (12.0656)\tVB Loss 3.4375 (4.1283)\tF1 0.583 (0.557)\n",
            "Epoch: [32][600/1405]\tBatch Time 0.268 (0.228)\tData Load Time 0.004 (0.005)\tCE Loss 12.4202 (12.0616)\tVB Loss 4.0220 (4.1431)\tF1 0.624 (0.558)\n",
            "Epoch: [32][700/1405]\tBatch Time 0.259 (0.228)\tData Load Time 0.005 (0.005)\tCE Loss 11.5621 (12.0604)\tVB Loss 4.6457 (4.1483)\tF1 0.697 (0.558)\n",
            "Epoch: [32][800/1405]\tBatch Time 0.260 (0.228)\tData Load Time 0.004 (0.005)\tCE Loss 12.3752 (12.0558)\tVB Loss 6.3260 (4.1526)\tF1 0.562 (0.559)\n",
            "Epoch: [32][900/1405]\tBatch Time 0.175 (0.226)\tData Load Time 0.009 (0.005)\tCE Loss 12.0211 (12.0607)\tVB Loss 2.3405 (4.1485)\tF1 0.277 (0.559)\n",
            "Epoch: [32][1000/1405]\tBatch Time 0.220 (0.225)\tData Load Time 0.005 (0.005)\tCE Loss 12.2241 (12.0617)\tVB Loss 3.6605 (4.1250)\tF1 0.451 (0.560)\n",
            "Epoch: [32][1100/1405]\tBatch Time 0.251 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 11.9614 (12.0630)\tVB Loss 5.6107 (4.1089)\tF1 0.472 (0.561)\n",
            "Epoch: [32][1200/1405]\tBatch Time 0.162 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 11.9317 (12.0605)\tVB Loss 5.2838 (4.0956)\tF1 0.364 (0.560)\n",
            "Epoch: [32][1300/1405]\tBatch Time 0.206 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 11.1414 (12.0673)\tVB Loss 4.4773 (4.0676)\tF1 0.751 (0.561)\n",
            "Epoch: [32][1400/1405]\tBatch Time 0.248 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 12.3918 (12.0656)\tVB Loss 3.6391 (4.0701)\tF1 0.555 (0.561)\n",
            "Validation: [0/325]\tBatch Time 0.333 (0.333)\tVB Loss 4.2766 (4.2766)\tF1 Score 0.555 (0.555)\t\n",
            "Validation: [100/325]\tBatch Time 0.132 (0.118)\tVB Loss 6.7746 (3.6481)\tF1 Score 0.601 (0.604)\t\n",
            "Validation: [200/325]\tBatch Time 0.137 (0.119)\tVB Loss 6.1089 (3.5345)\tF1 Score 0.614 (0.612)\t\n",
            "Validation: [300/325]\tBatch Time 0.112 (0.122)\tVB Loss 2.4633 (3.6313)\tF1 Score 0.772 (0.611)\t\n",
            "\n",
            " * LOSS - 3.660, F1 SCORE - 0.611\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000566\n",
            "\n",
            "Epoch: [33][0/1405]\tBatch Time 0.446 (0.446)\tData Load Time 0.199 (0.199)\tCE Loss 12.4669 (12.4669)\tVB Loss 2.3727 (2.3727)\tF1 0.830 (0.830)\n",
            "Epoch: [33][100/1405]\tBatch Time 0.244 (0.255)\tData Load Time 0.005 (0.007)\tCE Loss 12.0930 (12.0827)\tVB Loss 3.0687 (3.9361)\tF1 0.504 (0.544)\n",
            "Epoch: [33][200/1405]\tBatch Time 0.233 (0.243)\tData Load Time 0.004 (0.006)\tCE Loss 12.7029 (12.0706)\tVB Loss 3.0302 (4.0251)\tF1 0.716 (0.550)\n",
            "Epoch: [33][300/1405]\tBatch Time 0.268 (0.236)\tData Load Time 0.005 (0.006)\tCE Loss 12.5746 (12.0420)\tVB Loss 3.0088 (4.1274)\tF1 0.476 (0.549)\n",
            "Epoch: [33][400/1405]\tBatch Time 0.238 (0.233)\tData Load Time 0.004 (0.005)\tCE Loss 12.2342 (12.0570)\tVB Loss 4.9180 (4.1282)\tF1 0.567 (0.557)\n",
            "Epoch: [33][500/1405]\tBatch Time 0.241 (0.232)\tData Load Time 0.005 (0.005)\tCE Loss 10.9725 (12.0547)\tVB Loss 4.6455 (4.0668)\tF1 0.653 (0.559)\n",
            "Epoch: [33][600/1405]\tBatch Time 0.185 (0.229)\tData Load Time 0.005 (0.005)\tCE Loss 11.2960 (12.0610)\tVB Loss 4.3714 (4.0341)\tF1 0.488 (0.559)\n",
            "Epoch: [33][700/1405]\tBatch Time 0.218 (0.227)\tData Load Time 0.005 (0.005)\tCE Loss 12.7882 (12.0626)\tVB Loss 4.6349 (4.0101)\tF1 0.463 (0.562)\n",
            "Epoch: [33][800/1405]\tBatch Time 0.112 (0.227)\tData Load Time 0.004 (0.005)\tCE Loss 11.0154 (12.0621)\tVB Loss 1.0848 (3.9987)\tF1 0.877 (0.564)\n",
            "Epoch: [33][900/1405]\tBatch Time 0.195 (0.226)\tData Load Time 0.004 (0.005)\tCE Loss 10.9124 (12.0431)\tVB Loss 3.0914 (3.9586)\tF1 0.401 (0.566)\n",
            "Epoch: [33][1000/1405]\tBatch Time 0.252 (0.227)\tData Load Time 0.004 (0.005)\tCE Loss 12.0069 (12.0414)\tVB Loss 4.2574 (3.9547)\tF1 0.558 (0.566)\n",
            "Epoch: [33][1100/1405]\tBatch Time 0.205 (0.228)\tData Load Time 0.005 (0.005)\tCE Loss 11.7490 (12.0435)\tVB Loss 7.2699 (3.9772)\tF1 0.465 (0.564)\n",
            "Epoch: [33][1200/1405]\tBatch Time 0.292 (0.227)\tData Load Time 0.005 (0.005)\tCE Loss 12.9704 (12.0464)\tVB Loss 5.8276 (3.9883)\tF1 0.651 (0.564)\n",
            "Epoch: [33][1300/1405]\tBatch Time 0.164 (0.227)\tData Load Time 0.004 (0.005)\tCE Loss 10.6653 (12.0351)\tVB Loss 2.7550 (3.9889)\tF1 0.646 (0.564)\n",
            "Epoch: [33][1400/1405]\tBatch Time 0.142 (0.227)\tData Load Time 0.005 (0.005)\tCE Loss 11.7040 (12.0382)\tVB Loss 0.8148 (3.9945)\tF1 0.906 (0.563)\n",
            "Validation: [0/325]\tBatch Time 0.293 (0.293)\tVB Loss 5.8693 (5.8693)\tF1 Score 0.488 (0.488)\t\n",
            "Validation: [100/325]\tBatch Time 0.128 (0.124)\tVB Loss 2.3808 (3.6259)\tF1 Score 0.726 (0.631)\t\n",
            "Validation: [200/325]\tBatch Time 0.088 (0.124)\tVB Loss 2.8434 (3.6393)\tF1 Score 0.784 (0.641)\t\n",
            "Validation: [300/325]\tBatch Time 0.103 (0.124)\tVB Loss 2.0727 (3.5980)\tF1 Score 0.817 (0.640)\t\n",
            "\n",
            " * LOSS - 3.548, F1 SCORE - 0.641\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000556\n",
            "\n",
            "Epoch: [34][0/1405]\tBatch Time 0.410 (0.410)\tData Load Time 0.189 (0.189)\tCE Loss 12.1232 (12.1232)\tVB Loss 2.1799 (2.1799)\tF1 0.801 (0.801)\n",
            "Epoch: [34][100/1405]\tBatch Time 0.243 (0.252)\tData Load Time 0.005 (0.007)\tCE Loss 11.6957 (12.0670)\tVB Loss 3.8826 (4.0744)\tF1 0.468 (0.563)\n",
            "Epoch: [34][200/1405]\tBatch Time 0.234 (0.243)\tData Load Time 0.005 (0.006)\tCE Loss 11.3189 (12.0834)\tVB Loss 3.9433 (3.9701)\tF1 0.482 (0.578)\n",
            "Epoch: [34][300/1405]\tBatch Time 0.256 (0.236)\tData Load Time 0.005 (0.006)\tCE Loss 11.8936 (12.0541)\tVB Loss 6.2117 (3.9643)\tF1 0.628 (0.572)\n",
            "Epoch: [34][400/1405]\tBatch Time 0.278 (0.230)\tData Load Time 0.004 (0.005)\tCE Loss 12.5955 (12.0587)\tVB Loss 3.4135 (3.9590)\tF1 0.482 (0.571)\n",
            "Epoch: [34][500/1405]\tBatch Time 0.243 (0.230)\tData Load Time 0.004 (0.005)\tCE Loss 11.8256 (12.0609)\tVB Loss 7.2836 (3.9953)\tF1 0.647 (0.572)\n",
            "Epoch: [34][600/1405]\tBatch Time 0.144 (0.227)\tData Load Time 0.004 (0.005)\tCE Loss 12.1709 (12.0493)\tVB Loss 2.3397 (3.9358)\tF1 0.678 (0.574)\n",
            "Epoch: [34][700/1405]\tBatch Time 0.213 (0.226)\tData Load Time 0.004 (0.005)\tCE Loss 12.7713 (12.0441)\tVB Loss 2.5175 (3.8757)\tF1 0.674 (0.577)\n",
            "Epoch: [34][800/1405]\tBatch Time 0.252 (0.226)\tData Load Time 0.005 (0.005)\tCE Loss 12.1904 (12.0326)\tVB Loss 2.6747 (3.8726)\tF1 0.556 (0.578)\n",
            "Epoch: [34][900/1405]\tBatch Time 0.283 (0.224)\tData Load Time 0.004 (0.005)\tCE Loss 11.3844 (12.0273)\tVB Loss 8.1708 (3.8757)\tF1 0.488 (0.578)\n",
            "Epoch: [34][1000/1405]\tBatch Time 0.183 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 12.3687 (12.0209)\tVB Loss 4.3832 (3.8706)\tF1 0.559 (0.577)\n",
            "Epoch: [34][1100/1405]\tBatch Time 0.270 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 12.2372 (12.0200)\tVB Loss 3.2417 (3.8773)\tF1 0.410 (0.577)\n",
            "Epoch: [34][1200/1405]\tBatch Time 0.306 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 12.2467 (12.0177)\tVB Loss 6.1702 (3.8791)\tF1 0.428 (0.577)\n",
            "Epoch: [34][1300/1405]\tBatch Time 0.234 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 11.8307 (12.0151)\tVB Loss 6.4641 (3.8910)\tF1 0.388 (0.578)\n",
            "Epoch: [34][1400/1405]\tBatch Time 0.245 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 11.8533 (12.0106)\tVB Loss 2.8053 (3.8752)\tF1 0.569 (0.578)\n",
            "Validation: [0/325]\tBatch Time 0.285 (0.285)\tVB Loss 1.6985 (1.6985)\tF1 Score 0.679 (0.679)\t\n",
            "Validation: [100/325]\tBatch Time 0.130 (0.126)\tVB Loss 4.0670 (3.4615)\tF1 Score 0.592 (0.666)\t\n",
            "Validation: [200/325]\tBatch Time 0.087 (0.121)\tVB Loss 1.1728 (3.3388)\tF1 Score 0.926 (0.665)\t\n",
            "Validation: [300/325]\tBatch Time 0.139 (0.124)\tVB Loss 2.3894 (3.3687)\tF1 Score 0.876 (0.668)\t\n",
            "\n",
            " * LOSS - 3.365, F1 SCORE - 0.668\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000545\n",
            "\n",
            "Epoch: [35][0/1405]\tBatch Time 0.469 (0.469)\tData Load Time 0.191 (0.191)\tCE Loss 12.9106 (12.9106)\tVB Loss 5.3573 (5.3573)\tF1 0.589 (0.589)\n",
            "Epoch: [35][100/1405]\tBatch Time 0.275 (0.248)\tData Load Time 0.004 (0.007)\tCE Loss 10.9979 (11.9876)\tVB Loss 5.8787 (3.7324)\tF1 0.476 (0.585)\n",
            "Epoch: [35][200/1405]\tBatch Time 0.285 (0.234)\tData Load Time 0.004 (0.006)\tCE Loss 11.6750 (11.9667)\tVB Loss 6.2685 (3.9333)\tF1 0.608 (0.587)\n",
            "Epoch: [35][300/1405]\tBatch Time 0.190 (0.225)\tData Load Time 0.004 (0.005)\tCE Loss 12.5591 (11.9706)\tVB Loss 4.2053 (3.9034)\tF1 0.484 (0.584)\n",
            "Epoch: [35][400/1405]\tBatch Time 0.235 (0.225)\tData Load Time 0.005 (0.005)\tCE Loss 12.3151 (11.9740)\tVB Loss 2.5109 (3.8984)\tF1 0.712 (0.583)\n",
            "Epoch: [35][500/1405]\tBatch Time 0.313 (0.227)\tData Load Time 0.006 (0.005)\tCE Loss 12.0028 (11.9995)\tVB Loss 4.2204 (3.9012)\tF1 0.621 (0.589)\n",
            "Epoch: [35][600/1405]\tBatch Time 0.229 (0.225)\tData Load Time 0.004 (0.005)\tCE Loss 12.2357 (11.9993)\tVB Loss 4.1498 (3.8693)\tF1 0.611 (0.593)\n",
            "Epoch: [35][700/1405]\tBatch Time 0.190 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 12.3373 (12.0037)\tVB Loss 2.7208 (3.8583)\tF1 0.691 (0.592)\n",
            "Epoch: [35][800/1405]\tBatch Time 0.272 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 11.9684 (11.9958)\tVB Loss 4.9811 (3.8243)\tF1 0.778 (0.592)\n",
            "Epoch: [35][900/1405]\tBatch Time 0.293 (0.221)\tData Load Time 0.005 (0.005)\tCE Loss 12.6319 (12.0008)\tVB Loss 6.1383 (3.8276)\tF1 0.485 (0.592)\n",
            "Epoch: [35][1000/1405]\tBatch Time 0.125 (0.220)\tData Load Time 0.005 (0.005)\tCE Loss 11.9868 (11.9926)\tVB Loss 2.0458 (3.8084)\tF1 0.648 (0.592)\n",
            "Epoch: [35][1100/1405]\tBatch Time 0.283 (0.221)\tData Load Time 0.005 (0.005)\tCE Loss 11.7083 (11.9875)\tVB Loss 3.8780 (3.7961)\tF1 0.606 (0.590)\n",
            "Epoch: [35][1200/1405]\tBatch Time 0.290 (0.221)\tData Load Time 0.005 (0.005)\tCE Loss 11.9640 (11.9897)\tVB Loss 5.1456 (3.8075)\tF1 0.540 (0.590)\n",
            "Epoch: [35][1300/1405]\tBatch Time 0.157 (0.220)\tData Load Time 0.005 (0.005)\tCE Loss 11.9973 (11.9853)\tVB Loss 2.6067 (3.8118)\tF1 0.638 (0.589)\n",
            "Epoch: [35][1400/1405]\tBatch Time 0.166 (0.220)\tData Load Time 0.004 (0.005)\tCE Loss 11.5642 (11.9814)\tVB Loss 1.9698 (3.8064)\tF1 0.513 (0.590)\n",
            "Validation: [0/325]\tBatch Time 0.301 (0.301)\tVB Loss 8.9729 (8.9729)\tF1 Score 0.601 (0.601)\t\n",
            "Validation: [100/325]\tBatch Time 0.078 (0.107)\tVB Loss 1.7790 (3.2486)\tF1 Score 0.701 (0.679)\t\n",
            "Validation: [200/325]\tBatch Time 0.128 (0.111)\tVB Loss 5.8927 (3.4105)\tF1 Score 0.616 (0.672)\t\n",
            "Validation: [300/325]\tBatch Time 0.128 (0.112)\tVB Loss 0.4344 (3.4159)\tF1 Score 0.960 (0.670)\t\n",
            "\n",
            " * LOSS - 3.362, F1 SCORE - 0.670\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000536\n",
            "\n",
            "Epoch: [36][0/1405]\tBatch Time 0.380 (0.380)\tData Load Time 0.201 (0.201)\tCE Loss 11.3685 (11.3685)\tVB Loss 2.7348 (2.7348)\tF1 0.522 (0.522)\n",
            "Epoch: [36][100/1405]\tBatch Time 0.139 (0.247)\tData Load Time 0.004 (0.007)\tCE Loss 12.2361 (11.9499)\tVB Loss 2.5540 (3.8656)\tF1 0.738 (0.602)\n",
            "Epoch: [36][200/1405]\tBatch Time 0.193 (0.235)\tData Load Time 0.004 (0.006)\tCE Loss 11.7029 (11.9853)\tVB Loss 3.4338 (3.7817)\tF1 0.385 (0.598)\n",
            "Epoch: [36][300/1405]\tBatch Time 0.167 (0.229)\tData Load Time 0.005 (0.006)\tCE Loss 11.7937 (11.9732)\tVB Loss 4.2415 (3.7716)\tF1 0.436 (0.599)\n",
            "Epoch: [36][400/1405]\tBatch Time 0.215 (0.227)\tData Load Time 0.005 (0.005)\tCE Loss 12.0056 (11.9597)\tVB Loss 8.5936 (3.7333)\tF1 0.568 (0.597)\n",
            "Epoch: [36][500/1405]\tBatch Time 0.240 (0.225)\tData Load Time 0.005 (0.005)\tCE Loss 11.7497 (11.9478)\tVB Loss 3.5997 (3.7119)\tF1 0.560 (0.591)\n",
            "Epoch: [36][600/1405]\tBatch Time 0.196 (0.225)\tData Load Time 0.005 (0.005)\tCE Loss 12.9792 (11.9457)\tVB Loss 3.9611 (3.7355)\tF1 0.456 (0.592)\n",
            "Epoch: [36][700/1405]\tBatch Time 0.214 (0.224)\tData Load Time 0.004 (0.005)\tCE Loss 11.8032 (11.9529)\tVB Loss 2.8714 (3.7574)\tF1 0.431 (0.589)\n",
            "Epoch: [36][800/1405]\tBatch Time 0.149 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 11.6344 (11.9452)\tVB Loss 1.9427 (3.7372)\tF1 0.641 (0.591)\n",
            "Epoch: [36][900/1405]\tBatch Time 0.179 (0.221)\tData Load Time 0.004 (0.005)\tCE Loss 11.3825 (11.9473)\tVB Loss 3.0905 (3.7246)\tF1 0.524 (0.593)\n",
            "Epoch: [36][1000/1405]\tBatch Time 0.192 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 11.7992 (11.9490)\tVB Loss 7.6150 (3.7277)\tF1 0.603 (0.594)\n",
            "Epoch: [36][1100/1405]\tBatch Time 0.223 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 12.4021 (11.9472)\tVB Loss 5.9428 (3.7349)\tF1 0.529 (0.596)\n",
            "Epoch: [36][1200/1405]\tBatch Time 0.163 (0.221)\tData Load Time 0.005 (0.005)\tCE Loss 11.9974 (11.9506)\tVB Loss 1.7211 (3.7213)\tF1 0.598 (0.597)\n",
            "Epoch: [36][1300/1405]\tBatch Time 0.272 (0.221)\tData Load Time 0.005 (0.005)\tCE Loss 12.2552 (11.9504)\tVB Loss 2.3132 (3.7154)\tF1 0.661 (0.596)\n",
            "Epoch: [36][1400/1405]\tBatch Time 0.261 (0.221)\tData Load Time 0.005 (0.005)\tCE Loss 12.0672 (11.9505)\tVB Loss 3.9907 (3.7321)\tF1 0.644 (0.596)\n",
            "Validation: [0/325]\tBatch Time 0.412 (0.412)\tVB Loss 6.3618 (6.3618)\tF1 Score 0.585 (0.585)\t\n",
            "Validation: [100/325]\tBatch Time 0.119 (0.124)\tVB Loss 4.0459 (3.0167)\tF1 Score 0.848 (0.707)\t\n",
            "Validation: [200/325]\tBatch Time 0.137 (0.123)\tVB Loss 4.2804 (3.1821)\tF1 Score 0.543 (0.696)\t\n",
            "Validation: [300/325]\tBatch Time 0.119 (0.123)\tVB Loss 4.0174 (3.1531)\tF1 Score 0.596 (0.697)\t\n",
            "\n",
            " * LOSS - 3.128, F1 SCORE - 0.700\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000526\n",
            "\n",
            "Epoch: [37][0/1405]\tBatch Time 0.416 (0.416)\tData Load Time 0.192 (0.192)\tCE Loss 11.7808 (11.7808)\tVB Loss 3.9783 (3.9783)\tF1 0.733 (0.733)\n",
            "Epoch: [37][100/1405]\tBatch Time 0.251 (0.246)\tData Load Time 0.004 (0.007)\tCE Loss 12.0981 (12.0419)\tVB Loss 5.4573 (3.6210)\tF1 0.499 (0.636)\n",
            "Epoch: [37][200/1405]\tBatch Time 0.228 (0.233)\tData Load Time 0.006 (0.006)\tCE Loss 11.9637 (11.9969)\tVB Loss 2.6205 (3.5664)\tF1 0.434 (0.616)\n",
            "Epoch: [37][300/1405]\tBatch Time 0.150 (0.227)\tData Load Time 0.007 (0.006)\tCE Loss 11.7301 (11.9714)\tVB Loss 1.0237 (3.5321)\tF1 0.840 (0.615)\n",
            "Epoch: [37][400/1405]\tBatch Time 0.232 (0.226)\tData Load Time 0.004 (0.005)\tCE Loss 11.7340 (11.9544)\tVB Loss 3.1360 (3.5556)\tF1 0.569 (0.611)\n",
            "Epoch: [37][500/1405]\tBatch Time 0.305 (0.226)\tData Load Time 0.005 (0.005)\tCE Loss 12.0702 (11.9397)\tVB Loss 2.6272 (3.6109)\tF1 0.575 (0.610)\n",
            "Epoch: [37][600/1405]\tBatch Time 0.228 (0.227)\tData Load Time 0.004 (0.005)\tCE Loss 11.2839 (11.9323)\tVB Loss 3.0090 (3.6100)\tF1 0.525 (0.611)\n",
            "Epoch: [37][700/1405]\tBatch Time 0.183 (0.225)\tData Load Time 0.004 (0.005)\tCE Loss 11.7981 (11.9281)\tVB Loss 2.2443 (3.6219)\tF1 0.591 (0.611)\n",
            "Epoch: [37][800/1405]\tBatch Time 0.166 (0.226)\tData Load Time 0.004 (0.005)\tCE Loss 12.3653 (11.9373)\tVB Loss 3.2720 (3.6508)\tF1 0.696 (0.609)\n",
            "Epoch: [37][900/1405]\tBatch Time 0.280 (0.226)\tData Load Time 0.005 (0.005)\tCE Loss 12.6114 (11.9419)\tVB Loss 1.9565 (3.6699)\tF1 0.743 (0.608)\n",
            "Epoch: [37][1000/1405]\tBatch Time 0.223 (0.225)\tData Load Time 0.004 (0.005)\tCE Loss 12.1647 (11.9345)\tVB Loss 4.8165 (3.6517)\tF1 0.644 (0.607)\n",
            "Epoch: [37][1100/1405]\tBatch Time 0.219 (0.226)\tData Load Time 0.005 (0.005)\tCE Loss 11.9778 (11.9376)\tVB Loss 1.7232 (3.6536)\tF1 0.735 (0.608)\n",
            "Epoch: [37][1200/1405]\tBatch Time 0.197 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 11.0869 (11.9342)\tVB Loss 1.8364 (3.6393)\tF1 0.612 (0.608)\n",
            "Epoch: [37][1300/1405]\tBatch Time 0.214 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 11.7551 (11.9291)\tVB Loss 2.9066 (3.6228)\tF1 0.425 (0.609)\n",
            "Epoch: [37][1400/1405]\tBatch Time 0.242 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 12.1791 (11.9266)\tVB Loss 4.2924 (3.6257)\tF1 0.591 (0.608)\n",
            "Validation: [0/325]\tBatch Time 0.277 (0.277)\tVB Loss 2.1658 (2.1658)\tF1 Score 0.749 (0.749)\t\n",
            "Validation: [100/325]\tBatch Time 0.166 (0.114)\tVB Loss 6.0060 (3.2222)\tF1 Score 0.678 (0.692)\t\n",
            "Validation: [200/325]\tBatch Time 0.085 (0.111)\tVB Loss 2.1919 (3.2295)\tF1 Score 0.712 (0.681)\t\n",
            "Validation: [300/325]\tBatch Time 0.176 (0.115)\tVB Loss 3.3938 (3.1877)\tF1 Score 0.527 (0.687)\t\n",
            "\n",
            " * LOSS - 3.205, F1 SCORE - 0.688\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000517\n",
            "\n",
            "Epoch: [38][0/1405]\tBatch Time 0.504 (0.504)\tData Load Time 0.194 (0.194)\tCE Loss 11.8698 (11.8698)\tVB Loss 4.0190 (4.0190)\tF1 0.687 (0.687)\n",
            "Epoch: [38][100/1405]\tBatch Time 0.177 (0.258)\tData Load Time 0.005 (0.007)\tCE Loss 11.1922 (12.0109)\tVB Loss 3.0569 (3.7595)\tF1 0.817 (0.607)\n",
            "Epoch: [38][200/1405]\tBatch Time 0.247 (0.238)\tData Load Time 0.008 (0.006)\tCE Loss 12.4985 (11.9649)\tVB Loss 5.7941 (3.7506)\tF1 0.598 (0.585)\n",
            "Epoch: [38][300/1405]\tBatch Time 0.213 (0.235)\tData Load Time 0.004 (0.006)\tCE Loss 12.0104 (11.9391)\tVB Loss 3.5855 (3.6828)\tF1 0.515 (0.593)\n",
            "Epoch: [38][400/1405]\tBatch Time 0.236 (0.230)\tData Load Time 0.004 (0.006)\tCE Loss 12.3700 (11.9434)\tVB Loss 4.2995 (3.6201)\tF1 0.647 (0.605)\n",
            "Epoch: [38][500/1405]\tBatch Time 0.294 (0.230)\tData Load Time 0.005 (0.005)\tCE Loss 11.6968 (11.9411)\tVB Loss 5.6368 (3.5915)\tF1 0.519 (0.606)\n",
            "Epoch: [38][600/1405]\tBatch Time 0.241 (0.228)\tData Load Time 0.004 (0.005)\tCE Loss 12.2342 (11.9348)\tVB Loss 2.4417 (3.5649)\tF1 0.684 (0.610)\n",
            "Epoch: [38][700/1405]\tBatch Time 0.265 (0.228)\tData Load Time 0.004 (0.005)\tCE Loss 11.5180 (11.9248)\tVB Loss 4.7789 (3.5547)\tF1 0.613 (0.610)\n",
            "Epoch: [38][800/1405]\tBatch Time 0.239 (0.225)\tData Load Time 0.005 (0.005)\tCE Loss 11.6154 (11.9182)\tVB Loss 2.8120 (3.5366)\tF1 0.471 (0.611)\n",
            "Epoch: [38][900/1405]\tBatch Time 0.114 (0.224)\tData Load Time 0.004 (0.005)\tCE Loss 11.1070 (11.9172)\tVB Loss 1.0649 (3.5373)\tF1 0.962 (0.612)\n",
            "Epoch: [38][1000/1405]\tBatch Time 0.156 (0.224)\tData Load Time 0.006 (0.005)\tCE Loss 12.2735 (11.9194)\tVB Loss 1.0803 (3.5352)\tF1 0.698 (0.612)\n",
            "Epoch: [38][1100/1405]\tBatch Time 0.241 (0.222)\tData Load Time 0.008 (0.005)\tCE Loss 12.3008 (11.9141)\tVB Loss 4.5339 (3.5146)\tF1 0.559 (0.614)\n",
            "Epoch: [38][1200/1405]\tBatch Time 0.216 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 11.3819 (11.9108)\tVB Loss 2.4659 (3.5583)\tF1 0.754 (0.612)\n",
            "Epoch: [38][1300/1405]\tBatch Time 0.194 (0.221)\tData Load Time 0.004 (0.005)\tCE Loss 11.6179 (11.9057)\tVB Loss 3.1716 (3.5628)\tF1 0.586 (0.612)\n",
            "Epoch: [38][1400/1405]\tBatch Time 0.221 (0.221)\tData Load Time 0.004 (0.005)\tCE Loss 11.4752 (11.9032)\tVB Loss 1.9073 (3.5579)\tF1 0.578 (0.613)\n",
            "Validation: [0/325]\tBatch Time 0.289 (0.289)\tVB Loss 1.5411 (1.5411)\tF1 Score 0.691 (0.691)\t\n",
            "Validation: [100/325]\tBatch Time 0.110 (0.111)\tVB Loss 2.3230 (2.9127)\tF1 Score 0.705 (0.715)\t\n",
            "Validation: [200/325]\tBatch Time 0.131 (0.109)\tVB Loss 3.3638 (2.8913)\tF1 Score 0.603 (0.706)\t\n",
            "Validation: [300/325]\tBatch Time 0.264 (0.111)\tVB Loss 2.5901 (3.0133)\tF1 Score 0.920 (0.701)\t\n",
            "\n",
            " * LOSS - 3.011, F1 SCORE - 0.699\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000508\n",
            "\n",
            "Epoch: [39][0/1405]\tBatch Time 0.447 (0.447)\tData Load Time 0.185 (0.185)\tCE Loss 11.8640 (11.8640)\tVB Loss 2.5727 (2.5727)\tF1 0.657 (0.657)\n",
            "Epoch: [39][100/1405]\tBatch Time 0.208 (0.249)\tData Load Time 0.004 (0.007)\tCE Loss 11.1636 (11.8449)\tVB Loss 1.9596 (3.3482)\tF1 0.721 (0.630)\n",
            "Epoch: [39][200/1405]\tBatch Time 0.232 (0.238)\tData Load Time 0.006 (0.006)\tCE Loss 12.4351 (11.9087)\tVB Loss 5.8842 (3.3853)\tF1 0.602 (0.621)\n",
            "Epoch: [39][300/1405]\tBatch Time 0.292 (0.233)\tData Load Time 0.005 (0.006)\tCE Loss 12.1060 (11.9109)\tVB Loss 5.1027 (3.4764)\tF1 0.557 (0.615)\n",
            "Epoch: [39][400/1405]\tBatch Time 0.176 (0.230)\tData Load Time 0.005 (0.005)\tCE Loss 11.5159 (11.8961)\tVB Loss 3.9289 (3.4663)\tF1 0.634 (0.616)\n",
            "Epoch: [39][500/1405]\tBatch Time 0.190 (0.229)\tData Load Time 0.005 (0.005)\tCE Loss 11.7195 (11.9067)\tVB Loss 2.9528 (3.5416)\tF1 0.555 (0.611)\n",
            "Epoch: [39][600/1405]\tBatch Time 0.194 (0.226)\tData Load Time 0.006 (0.005)\tCE Loss 10.8737 (11.9107)\tVB Loss 2.6548 (3.5268)\tF1 0.744 (0.614)\n",
            "Epoch: [39][700/1405]\tBatch Time 0.230 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 11.9695 (11.8999)\tVB Loss 4.9906 (3.5079)\tF1 0.613 (0.614)\n",
            "Epoch: [39][800/1405]\tBatch Time 0.216 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 12.3896 (11.8936)\tVB Loss 3.8922 (3.5103)\tF1 0.675 (0.617)\n",
            "Epoch: [39][900/1405]\tBatch Time 0.214 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 10.8384 (11.8899)\tVB Loss 3.9728 (3.5019)\tF1 0.795 (0.617)\n",
            "Epoch: [39][1000/1405]\tBatch Time 0.233 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 11.5764 (11.8871)\tVB Loss 2.9401 (3.5042)\tF1 0.678 (0.616)\n",
            "Epoch: [39][1100/1405]\tBatch Time 0.240 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 11.5172 (11.8862)\tVB Loss 4.6453 (3.5063)\tF1 0.536 (0.618)\n",
            "Epoch: [39][1200/1405]\tBatch Time 0.280 (0.223)\tData Load Time 0.006 (0.005)\tCE Loss 12.4941 (11.8819)\tVB Loss 5.9021 (3.4961)\tF1 0.648 (0.618)\n",
            "Epoch: [39][1300/1405]\tBatch Time 0.164 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 11.9171 (11.8819)\tVB Loss 2.9611 (3.5045)\tF1 0.653 (0.618)\n",
            "Epoch: [39][1400/1405]\tBatch Time 0.204 (0.223)\tData Load Time 0.006 (0.005)\tCE Loss 12.4279 (11.8793)\tVB Loss 1.8907 (3.4919)\tF1 0.816 (0.620)\n",
            "Validation: [0/325]\tBatch Time 0.301 (0.301)\tVB Loss 8.0574 (8.0574)\tF1 Score 0.678 (0.678)\t\n",
            "Validation: [100/325]\tBatch Time 0.117 (0.107)\tVB Loss 3.5003 (3.0569)\tF1 Score 0.583 (0.707)\t\n",
            "Validation: [200/325]\tBatch Time 0.093 (0.108)\tVB Loss 2.1112 (3.1073)\tF1 Score 0.666 (0.690)\t\n",
            "Validation: [300/325]\tBatch Time 0.091 (0.112)\tVB Loss 2.6368 (3.0267)\tF1 Score 0.715 (0.703)\t\n",
            "\n",
            " * LOSS - 3.028, F1 SCORE - 0.703\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000500\n",
            "\n",
            "Epoch: [40][0/1405]\tBatch Time 0.481 (0.481)\tData Load Time 0.203 (0.203)\tCE Loss 11.4661 (11.4661)\tVB Loss 3.8360 (3.8360)\tF1 0.683 (0.683)\n",
            "Epoch: [40][100/1405]\tBatch Time 0.220 (0.254)\tData Load Time 0.005 (0.007)\tCE Loss 11.7642 (11.8502)\tVB Loss 1.6845 (3.2762)\tF1 0.714 (0.627)\n",
            "Epoch: [40][200/1405]\tBatch Time 0.127 (0.234)\tData Load Time 0.005 (0.006)\tCE Loss 12.6260 (11.8704)\tVB Loss 2.4922 (3.2489)\tF1 0.815 (0.633)\n",
            "Epoch: [40][300/1405]\tBatch Time 0.214 (0.231)\tData Load Time 0.004 (0.006)\tCE Loss 11.5899 (11.8780)\tVB Loss 4.7407 (3.3124)\tF1 0.470 (0.630)\n",
            "Epoch: [40][400/1405]\tBatch Time 0.222 (0.231)\tData Load Time 0.004 (0.005)\tCE Loss 10.7230 (11.8979)\tVB Loss 1.7165 (3.3385)\tF1 0.681 (0.629)\n",
            "Epoch: [40][500/1405]\tBatch Time 0.227 (0.229)\tData Load Time 0.004 (0.005)\tCE Loss 11.8405 (11.8879)\tVB Loss 2.1916 (3.3704)\tF1 0.813 (0.630)\n",
            "Epoch: [40][600/1405]\tBatch Time 0.209 (0.228)\tData Load Time 0.004 (0.005)\tCE Loss 11.2412 (11.8779)\tVB Loss 3.0288 (3.3725)\tF1 0.720 (0.629)\n",
            "Epoch: [40][700/1405]\tBatch Time 0.248 (0.226)\tData Load Time 0.006 (0.005)\tCE Loss 11.7831 (11.8706)\tVB Loss 3.0338 (3.3888)\tF1 0.580 (0.629)\n",
            "Epoch: [40][800/1405]\tBatch Time 0.243 (0.226)\tData Load Time 0.004 (0.005)\tCE Loss 11.9033 (11.8751)\tVB Loss 3.7986 (3.4069)\tF1 0.511 (0.627)\n",
            "Epoch: [40][900/1405]\tBatch Time 0.208 (0.226)\tData Load Time 0.008 (0.005)\tCE Loss 11.4597 (11.8745)\tVB Loss 4.1316 (3.4054)\tF1 0.567 (0.627)\n",
            "Epoch: [40][1000/1405]\tBatch Time 0.248 (0.225)\tData Load Time 0.006 (0.005)\tCE Loss 11.2591 (11.8632)\tVB Loss 6.0467 (3.3965)\tF1 0.657 (0.628)\n",
            "Epoch: [40][1100/1405]\tBatch Time 0.201 (0.224)\tData Load Time 0.004 (0.005)\tCE Loss 11.2655 (11.8634)\tVB Loss 4.4983 (3.3740)\tF1 0.531 (0.629)\n",
            "Epoch: [40][1200/1405]\tBatch Time 0.233 (0.224)\tData Load Time 0.004 (0.005)\tCE Loss 11.8063 (11.8541)\tVB Loss 4.9988 (3.3844)\tF1 0.580 (0.630)\n",
            "Epoch: [40][1300/1405]\tBatch Time 0.258 (0.224)\tData Load Time 0.004 (0.005)\tCE Loss 12.6095 (11.8508)\tVB Loss 5.0924 (3.3847)\tF1 0.553 (0.631)\n",
            "Epoch: [40][1400/1405]\tBatch Time 0.306 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 12.3886 (11.8532)\tVB Loss 5.6905 (3.3926)\tF1 0.420 (0.630)\n",
            "Validation: [0/325]\tBatch Time 0.291 (0.291)\tVB Loss 4.1028 (4.1028)\tF1 Score 0.796 (0.796)\t\n",
            "Validation: [100/325]\tBatch Time 0.103 (0.122)\tVB Loss 1.7293 (2.9740)\tF1 Score 0.600 (0.717)\t\n",
            "Validation: [200/325]\tBatch Time 0.147 (0.122)\tVB Loss 1.5464 (2.8769)\tF1 Score 0.920 (0.714)\t\n",
            "Validation: [300/325]\tBatch Time 0.143 (0.123)\tVB Loss 3.5498 (2.8757)\tF1 Score 0.731 (0.719)\t\n",
            "\n",
            " * LOSS - 2.863, F1 SCORE - 0.720\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000492\n",
            "\n",
            "Epoch: [41][0/1405]\tBatch Time 0.502 (0.502)\tData Load Time 0.204 (0.204)\tCE Loss 12.2317 (12.2317)\tVB Loss 2.0251 (2.0251)\tF1 0.715 (0.715)\n",
            "Epoch: [41][100/1405]\tBatch Time 0.140 (0.243)\tData Load Time 0.005 (0.007)\tCE Loss 12.0371 (11.8361)\tVB Loss 1.0745 (3.3023)\tF1 0.672 (0.650)\n",
            "Epoch: [41][200/1405]\tBatch Time 0.266 (0.235)\tData Load Time 0.005 (0.006)\tCE Loss 12.1268 (11.7983)\tVB Loss 4.3766 (3.2363)\tF1 0.419 (0.653)\n",
            "Epoch: [41][300/1405]\tBatch Time 0.214 (0.231)\tData Load Time 0.004 (0.006)\tCE Loss 12.1366 (11.8042)\tVB Loss 3.0577 (3.3591)\tF1 0.564 (0.646)\n",
            "Epoch: [41][400/1405]\tBatch Time 0.183 (0.230)\tData Load Time 0.006 (0.005)\tCE Loss 11.4605 (11.8264)\tVB Loss 2.6436 (3.3329)\tF1 0.699 (0.646)\n",
            "Epoch: [41][500/1405]\tBatch Time 0.218 (0.227)\tData Load Time 0.005 (0.005)\tCE Loss 10.7355 (11.8506)\tVB Loss 3.6554 (3.3323)\tF1 0.626 (0.645)\n",
            "Epoch: [41][600/1405]\tBatch Time 0.125 (0.226)\tData Load Time 0.004 (0.005)\tCE Loss 10.5729 (11.8359)\tVB Loss 2.2448 (3.3239)\tF1 0.651 (0.642)\n",
            "Epoch: [41][700/1405]\tBatch Time 0.276 (0.225)\tData Load Time 0.004 (0.005)\tCE Loss 11.5540 (11.8414)\tVB Loss 5.0331 (3.3418)\tF1 0.525 (0.638)\n",
            "Epoch: [41][800/1405]\tBatch Time 0.185 (0.225)\tData Load Time 0.004 (0.005)\tCE Loss 12.1949 (11.8370)\tVB Loss 2.7667 (3.3513)\tF1 0.582 (0.638)\n",
            "Epoch: [41][900/1405]\tBatch Time 0.276 (0.225)\tData Load Time 0.004 (0.005)\tCE Loss 12.1061 (11.8316)\tVB Loss 4.2501 (3.3376)\tF1 0.679 (0.639)\n",
            "Epoch: [41][1000/1405]\tBatch Time 0.289 (0.225)\tData Load Time 0.006 (0.005)\tCE Loss 11.8782 (11.8299)\tVB Loss 3.8175 (3.3022)\tF1 0.715 (0.640)\n",
            "Epoch: [41][1100/1405]\tBatch Time 0.217 (0.224)\tData Load Time 0.004 (0.005)\tCE Loss 12.4306 (11.8249)\tVB Loss 2.8382 (3.3072)\tF1 0.825 (0.641)\n",
            "Epoch: [41][1200/1405]\tBatch Time 0.204 (0.224)\tData Load Time 0.004 (0.005)\tCE Loss 11.6732 (11.8238)\tVB Loss 3.2733 (3.3300)\tF1 0.676 (0.641)\n",
            "Epoch: [41][1300/1405]\tBatch Time 0.255 (0.224)\tData Load Time 0.004 (0.005)\tCE Loss 11.6816 (11.8298)\tVB Loss 2.6986 (3.3272)\tF1 0.752 (0.640)\n",
            "Epoch: [41][1400/1405]\tBatch Time 0.269 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 12.5936 (11.8285)\tVB Loss 4.4540 (3.3231)\tF1 0.672 (0.640)\n",
            "Validation: [0/325]\tBatch Time 0.314 (0.314)\tVB Loss 3.6020 (3.6020)\tF1 Score 0.672 (0.672)\t\n",
            "Validation: [100/325]\tBatch Time 0.099 (0.121)\tVB Loss 1.0293 (2.6399)\tF1 Score 0.902 (0.735)\t\n",
            "Validation: [200/325]\tBatch Time 0.130 (0.125)\tVB Loss 2.7310 (2.7055)\tF1 Score 0.747 (0.736)\t\n",
            "Validation: [300/325]\tBatch Time 0.130 (0.122)\tVB Loss 2.2968 (2.7740)\tF1 Score 0.651 (0.735)\t\n",
            "\n",
            " * LOSS - 2.776, F1 SCORE - 0.734\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000484\n",
            "\n",
            "Epoch: [42][0/1405]\tBatch Time 0.558 (0.558)\tData Load Time 0.220 (0.220)\tCE Loss 12.2408 (12.2408)\tVB Loss 4.0169 (4.0169)\tF1 0.583 (0.583)\n",
            "Epoch: [42][100/1405]\tBatch Time 0.210 (0.247)\tData Load Time 0.005 (0.007)\tCE Loss 10.8482 (11.7804)\tVB Loss 1.6652 (3.2694)\tF1 0.917 (0.638)\n",
            "Epoch: [42][200/1405]\tBatch Time 0.116 (0.234)\tData Load Time 0.006 (0.006)\tCE Loss 9.7103 (11.8157)\tVB Loss 1.6292 (3.2074)\tF1 0.833 (0.643)\n",
            "Epoch: [42][300/1405]\tBatch Time 0.232 (0.227)\tData Load Time 0.005 (0.006)\tCE Loss 11.5367 (11.7824)\tVB Loss 3.7701 (3.2171)\tF1 0.436 (0.634)\n",
            "Epoch: [42][400/1405]\tBatch Time 0.253 (0.226)\tData Load Time 0.005 (0.005)\tCE Loss 12.9915 (11.8036)\tVB Loss 3.6367 (3.1773)\tF1 0.604 (0.640)\n",
            "Epoch: [42][500/1405]\tBatch Time 0.203 (0.227)\tData Load Time 0.006 (0.005)\tCE Loss 11.8132 (11.8273)\tVB Loss 2.4656 (3.2645)\tF1 0.685 (0.638)\n",
            "Epoch: [42][600/1405]\tBatch Time 0.210 (0.226)\tData Load Time 0.005 (0.005)\tCE Loss 11.8004 (11.8235)\tVB Loss 7.3340 (3.3050)\tF1 0.551 (0.640)\n",
            "Epoch: [42][700/1405]\tBatch Time 0.220 (0.226)\tData Load Time 0.004 (0.005)\tCE Loss 12.4669 (11.8292)\tVB Loss 1.5480 (3.3131)\tF1 0.640 (0.640)\n",
            "Epoch: [42][800/1405]\tBatch Time 0.195 (0.225)\tData Load Time 0.005 (0.005)\tCE Loss 11.7613 (11.8233)\tVB Loss 3.2412 (3.3197)\tF1 0.748 (0.641)\n",
            "Epoch: [42][900/1405]\tBatch Time 0.254 (0.225)\tData Load Time 0.004 (0.005)\tCE Loss 10.9356 (11.8223)\tVB Loss 4.8850 (3.3035)\tF1 0.576 (0.642)\n",
            "Epoch: [42][1000/1405]\tBatch Time 0.265 (0.225)\tData Load Time 0.005 (0.005)\tCE Loss 11.6965 (11.8181)\tVB Loss 3.3983 (3.3104)\tF1 0.723 (0.641)\n",
            "Epoch: [42][1100/1405]\tBatch Time 0.201 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 10.5700 (11.8149)\tVB Loss 1.6761 (3.2787)\tF1 0.667 (0.643)\n",
            "Epoch: [42][1200/1405]\tBatch Time 0.180 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 12.3229 (11.8104)\tVB Loss 3.7438 (3.2809)\tF1 0.644 (0.644)\n",
            "Epoch: [42][1300/1405]\tBatch Time 0.218 (0.224)\tData Load Time 0.004 (0.005)\tCE Loss 11.6154 (11.8109)\tVB Loss 2.9917 (3.2789)\tF1 0.568 (0.644)\n",
            "Epoch: [42][1400/1405]\tBatch Time 0.094 (0.224)\tData Load Time 0.004 (0.005)\tCE Loss 12.0229 (11.8110)\tVB Loss 1.1405 (3.2826)\tF1 0.742 (0.642)\n",
            "Validation: [0/325]\tBatch Time 0.291 (0.291)\tVB Loss 2.1172 (2.1172)\tF1 Score 0.608 (0.608)\t\n",
            "Validation: [100/325]\tBatch Time 0.081 (0.110)\tVB Loss 2.2351 (2.8171)\tF1 Score 0.785 (0.723)\t\n",
            "Validation: [200/325]\tBatch Time 0.123 (0.114)\tVB Loss 1.8167 (2.7271)\tF1 Score 0.752 (0.726)\t\n",
            "Validation: [300/325]\tBatch Time 0.140 (0.117)\tVB Loss 2.2700 (2.7639)\tF1 Score 0.753 (0.726)\t\n",
            "\n",
            " * LOSS - 2.743, F1 SCORE - 0.727\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000476\n",
            "\n",
            "Epoch: [43][0/1405]\tBatch Time 0.401 (0.401)\tData Load Time 0.206 (0.206)\tCE Loss 11.5743 (11.5743)\tVB Loss 1.7082 (1.7082)\tF1 0.847 (0.847)\n",
            "Epoch: [43][100/1405]\tBatch Time 0.224 (0.233)\tData Load Time 0.004 (0.007)\tCE Loss 11.7370 (11.8012)\tVB Loss 4.3013 (2.8782)\tF1 0.483 (0.678)\n",
            "Epoch: [43][200/1405]\tBatch Time 0.193 (0.226)\tData Load Time 0.004 (0.006)\tCE Loss 12.1199 (11.8017)\tVB Loss 2.0858 (3.1470)\tF1 0.720 (0.660)\n",
            "Epoch: [43][300/1405]\tBatch Time 0.244 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 11.5154 (11.7977)\tVB Loss 3.7087 (3.1746)\tF1 0.730 (0.651)\n",
            "Epoch: [43][400/1405]\tBatch Time 0.251 (0.225)\tData Load Time 0.006 (0.005)\tCE Loss 11.8527 (11.7717)\tVB Loss 3.0240 (3.2290)\tF1 0.612 (0.645)\n",
            "Epoch: [43][500/1405]\tBatch Time 0.096 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 11.4758 (11.7896)\tVB Loss 1.8058 (3.2753)\tF1 0.740 (0.641)\n",
            "Epoch: [43][600/1405]\tBatch Time 0.175 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 11.4082 (11.8057)\tVB Loss 2.0058 (3.2863)\tF1 0.712 (0.640)\n",
            "Epoch: [43][700/1405]\tBatch Time 0.197 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 11.9060 (11.8018)\tVB Loss 3.1721 (3.2420)\tF1 0.700 (0.643)\n",
            "Epoch: [43][800/1405]\tBatch Time 0.183 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 11.9383 (11.8134)\tVB Loss 4.4066 (3.2495)\tF1 0.588 (0.645)\n",
            "Epoch: [43][900/1405]\tBatch Time 0.199 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 12.0984 (11.8116)\tVB Loss 2.5957 (3.2386)\tF1 0.752 (0.647)\n",
            "Epoch: [43][1000/1405]\tBatch Time 0.193 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 11.6752 (11.8081)\tVB Loss 1.7556 (3.2276)\tF1 0.586 (0.647)\n",
            "Epoch: [43][1100/1405]\tBatch Time 0.093 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 10.8795 (11.8034)\tVB Loss 0.7399 (3.2470)\tF1 0.900 (0.646)\n",
            "Epoch: [43][1200/1405]\tBatch Time 0.245 (0.222)\tData Load Time 0.006 (0.005)\tCE Loss 12.4056 (11.7982)\tVB Loss 2.8750 (3.2468)\tF1 0.648 (0.648)\n",
            "Epoch: [43][1300/1405]\tBatch Time 0.199 (0.221)\tData Load Time 0.004 (0.005)\tCE Loss 10.3836 (11.7918)\tVB Loss 2.3075 (3.2263)\tF1 0.814 (0.647)\n",
            "Epoch: [43][1400/1405]\tBatch Time 0.202 (0.220)\tData Load Time 0.005 (0.005)\tCE Loss 11.3905 (11.7917)\tVB Loss 1.5893 (3.2109)\tF1 0.566 (0.647)\n",
            "Validation: [0/325]\tBatch Time 0.311 (0.311)\tVB Loss 2.6438 (2.6438)\tF1 Score 0.740 (0.740)\t\n",
            "Validation: [100/325]\tBatch Time 0.082 (0.111)\tVB Loss 3.2075 (2.5232)\tF1 Score 0.708 (0.743)\t\n",
            "Validation: [200/325]\tBatch Time 0.099 (0.111)\tVB Loss 2.3139 (2.5258)\tF1 Score 0.459 (0.745)\t\n",
            "Validation: [300/325]\tBatch Time 0.097 (0.111)\tVB Loss 5.3563 (2.6597)\tF1 Score 0.677 (0.741)\t\n",
            "\n",
            " * LOSS - 2.676, F1 SCORE - 0.741\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000469\n",
            "\n",
            "Epoch: [44][0/1405]\tBatch Time 0.446 (0.446)\tData Load Time 0.197 (0.197)\tCE Loss 11.0588 (11.0588)\tVB Loss 3.3344 (3.3344)\tF1 0.766 (0.766)\n",
            "Epoch: [44][100/1405]\tBatch Time 0.186 (0.257)\tData Load Time 0.004 (0.007)\tCE Loss 12.1509 (11.7742)\tVB Loss 2.5951 (3.3191)\tF1 0.700 (0.642)\n",
            "Epoch: [44][200/1405]\tBatch Time 0.288 (0.246)\tData Load Time 0.005 (0.006)\tCE Loss 12.3122 (11.7947)\tVB Loss 2.2211 (3.3000)\tF1 0.609 (0.637)\n",
            "Epoch: [44][300/1405]\tBatch Time 0.288 (0.238)\tData Load Time 0.004 (0.006)\tCE Loss 11.6247 (11.7812)\tVB Loss 3.3154 (3.2962)\tF1 0.672 (0.636)\n",
            "Epoch: [44][400/1405]\tBatch Time 0.158 (0.233)\tData Load Time 0.004 (0.005)\tCE Loss 11.8250 (11.7894)\tVB Loss 1.2426 (3.2184)\tF1 0.917 (0.636)\n",
            "Epoch: [44][500/1405]\tBatch Time 0.201 (0.230)\tData Load Time 0.004 (0.005)\tCE Loss 11.8593 (11.7958)\tVB Loss 2.8116 (3.1660)\tF1 0.764 (0.644)\n",
            "Epoch: [44][600/1405]\tBatch Time 0.183 (0.229)\tData Load Time 0.006 (0.005)\tCE Loss 11.6596 (11.7943)\tVB Loss 4.0147 (3.1477)\tF1 0.658 (0.645)\n",
            "Epoch: [44][700/1405]\tBatch Time 0.241 (0.227)\tData Load Time 0.005 (0.005)\tCE Loss 11.9414 (11.7852)\tVB Loss 2.5847 (3.1469)\tF1 0.743 (0.646)\n",
            "Epoch: [44][800/1405]\tBatch Time 0.293 (0.226)\tData Load Time 0.005 (0.005)\tCE Loss 12.2332 (11.7878)\tVB Loss 3.3218 (3.1447)\tF1 0.662 (0.647)\n",
            "Epoch: [44][900/1405]\tBatch Time 0.240 (0.225)\tData Load Time 0.004 (0.005)\tCE Loss 11.6465 (11.7817)\tVB Loss 4.7647 (3.1741)\tF1 0.580 (0.647)\n",
            "Epoch: [44][1000/1405]\tBatch Time 0.281 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 12.1294 (11.7771)\tVB Loss 3.6152 (3.1558)\tF1 0.423 (0.647)\n",
            "Epoch: [44][1100/1405]\tBatch Time 0.267 (0.224)\tData Load Time 0.004 (0.005)\tCE Loss 12.4276 (11.7786)\tVB Loss 3.3051 (3.1590)\tF1 0.565 (0.648)\n",
            "Epoch: [44][1200/1405]\tBatch Time 0.183 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 11.2884 (11.7820)\tVB Loss 1.6966 (3.1714)\tF1 0.800 (0.648)\n",
            "Epoch: [44][1300/1405]\tBatch Time 0.197 (0.223)\tData Load Time 0.006 (0.005)\tCE Loss 12.0157 (11.7784)\tVB Loss 1.9053 (3.1643)\tF1 0.778 (0.650)\n",
            "Epoch: [44][1400/1405]\tBatch Time 0.256 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 11.2167 (11.7741)\tVB Loss 3.3669 (3.1615)\tF1 0.711 (0.650)\n",
            "Validation: [0/325]\tBatch Time 0.307 (0.307)\tVB Loss 2.1852 (2.1852)\tF1 Score 0.855 (0.855)\t\n",
            "Validation: [100/325]\tBatch Time 0.114 (0.112)\tVB Loss 4.8299 (2.9804)\tF1 Score 0.669 (0.721)\t\n",
            "Validation: [200/325]\tBatch Time 0.112 (0.116)\tVB Loss 1.2546 (2.8105)\tF1 Score 0.976 (0.728)\t\n",
            "Validation: [300/325]\tBatch Time 0.235 (0.119)\tVB Loss 2.2469 (2.8677)\tF1 Score 0.667 (0.730)\t\n",
            "\n",
            " * LOSS - 2.846, F1 SCORE - 0.731\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000462\n",
            "\n",
            "Epoch: [45][0/1405]\tBatch Time 0.498 (0.498)\tData Load Time 0.210 (0.210)\tCE Loss 11.6208 (11.6208)\tVB Loss 1.8272 (1.8272)\tF1 0.797 (0.797)\n",
            "Epoch: [45][100/1405]\tBatch Time 0.208 (0.238)\tData Load Time 0.006 (0.008)\tCE Loss 11.2466 (11.7750)\tVB Loss 2.5333 (3.0321)\tF1 0.866 (0.664)\n",
            "Epoch: [45][200/1405]\tBatch Time 0.266 (0.235)\tData Load Time 0.005 (0.006)\tCE Loss 12.1648 (11.7924)\tVB Loss 2.6459 (3.0519)\tF1 0.453 (0.652)\n",
            "Epoch: [45][300/1405]\tBatch Time 0.192 (0.227)\tData Load Time 0.004 (0.006)\tCE Loss 12.4944 (11.7621)\tVB Loss 2.5183 (3.0344)\tF1 0.586 (0.662)\n",
            "Epoch: [45][400/1405]\tBatch Time 0.223 (0.224)\tData Load Time 0.004 (0.005)\tCE Loss 12.1734 (11.7414)\tVB Loss 2.9806 (3.0750)\tF1 0.453 (0.661)\n",
            "Epoch: [45][500/1405]\tBatch Time 0.216 (0.225)\tData Load Time 0.005 (0.005)\tCE Loss 11.9481 (11.7392)\tVB Loss 2.2115 (3.1135)\tF1 0.533 (0.657)\n",
            "Epoch: [45][600/1405]\tBatch Time 0.103 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 9.3844 (11.7338)\tVB Loss 0.8697 (3.1041)\tF1 0.776 (0.660)\n",
            "Epoch: [45][700/1405]\tBatch Time 0.242 (0.224)\tData Load Time 0.004 (0.005)\tCE Loss 12.0876 (11.7383)\tVB Loss 2.3659 (3.1333)\tF1 0.857 (0.658)\n",
            "Epoch: [45][800/1405]\tBatch Time 0.182 (0.224)\tData Load Time 0.004 (0.005)\tCE Loss 12.0524 (11.7423)\tVB Loss 1.8652 (3.1541)\tF1 0.702 (0.655)\n",
            "Epoch: [45][900/1405]\tBatch Time 0.231 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 11.5769 (11.7427)\tVB Loss 4.0550 (3.1414)\tF1 0.703 (0.654)\n",
            "Epoch: [45][1000/1405]\tBatch Time 0.185 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 11.4493 (11.7436)\tVB Loss 0.9543 (3.1211)\tF1 0.904 (0.655)\n",
            "Epoch: [45][1100/1405]\tBatch Time 0.134 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 10.8154 (11.7465)\tVB Loss 2.6860 (3.1187)\tF1 0.607 (0.656)\n",
            "Epoch: [45][1200/1405]\tBatch Time 0.222 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 11.5089 (11.7525)\tVB Loss 4.2968 (3.1307)\tF1 0.553 (0.656)\n",
            "Epoch: [45][1300/1405]\tBatch Time 0.268 (0.221)\tData Load Time 0.004 (0.005)\tCE Loss 11.6957 (11.7493)\tVB Loss 5.0672 (3.1208)\tF1 0.518 (0.656)\n",
            "Epoch: [45][1400/1405]\tBatch Time 0.248 (0.221)\tData Load Time 0.005 (0.005)\tCE Loss 11.9559 (11.7530)\tVB Loss 3.8392 (3.1292)\tF1 0.595 (0.656)\n",
            "Validation: [0/325]\tBatch Time 0.323 (0.323)\tVB Loss 1.3966 (1.3966)\tF1 Score 0.811 (0.811)\t\n",
            "Validation: [100/325]\tBatch Time 0.108 (0.111)\tVB Loss 2.2468 (2.6609)\tF1 Score 0.815 (0.746)\t\n",
            "Validation: [200/325]\tBatch Time 0.096 (0.109)\tVB Loss 2.9738 (2.5030)\tF1 Score 0.831 (0.749)\t\n",
            "Validation: [300/325]\tBatch Time 0.159 (0.114)\tVB Loss 2.6186 (2.6125)\tF1 Score 0.893 (0.750)\t\n",
            "\n",
            " * LOSS - 2.639, F1 SCORE - 0.748\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000455\n",
            "\n",
            "Epoch: [46][0/1405]\tBatch Time 0.444 (0.444)\tData Load Time 0.199 (0.199)\tCE Loss 11.3276 (11.3276)\tVB Loss 1.8903 (1.8903)\tF1 0.806 (0.806)\n",
            "Epoch: [46][100/1405]\tBatch Time 0.215 (0.245)\tData Load Time 0.005 (0.007)\tCE Loss 11.7150 (11.7408)\tVB Loss 3.6856 (3.1913)\tF1 0.668 (0.643)\n",
            "Epoch: [46][200/1405]\tBatch Time 0.231 (0.233)\tData Load Time 0.005 (0.006)\tCE Loss 12.4559 (11.7263)\tVB Loss 2.6357 (3.1993)\tF1 0.775 (0.654)\n",
            "Epoch: [46][300/1405]\tBatch Time 0.254 (0.232)\tData Load Time 0.005 (0.006)\tCE Loss 11.2760 (11.7344)\tVB Loss 1.5617 (3.1518)\tF1 0.693 (0.653)\n",
            "Epoch: [46][400/1405]\tBatch Time 0.178 (0.230)\tData Load Time 0.005 (0.005)\tCE Loss 10.7945 (11.7376)\tVB Loss 1.3996 (3.1239)\tF1 0.783 (0.653)\n",
            "Epoch: [46][500/1405]\tBatch Time 0.215 (0.227)\tData Load Time 0.004 (0.005)\tCE Loss 11.4266 (11.7301)\tVB Loss 4.2969 (3.0869)\tF1 0.548 (0.653)\n",
            "Epoch: [46][600/1405]\tBatch Time 0.227 (0.227)\tData Load Time 0.004 (0.005)\tCE Loss 11.8812 (11.7280)\tVB Loss 3.4256 (3.0847)\tF1 0.633 (0.655)\n",
            "Epoch: [46][700/1405]\tBatch Time 0.253 (0.227)\tData Load Time 0.006 (0.005)\tCE Loss 10.9021 (11.7410)\tVB Loss 3.0534 (3.1047)\tF1 0.581 (0.657)\n",
            "Epoch: [46][800/1405]\tBatch Time 0.222 (0.227)\tData Load Time 0.005 (0.005)\tCE Loss 12.1452 (11.7350)\tVB Loss 4.1073 (3.1313)\tF1 0.695 (0.657)\n",
            "Epoch: [46][900/1405]\tBatch Time 0.211 (0.225)\tData Load Time 0.006 (0.005)\tCE Loss 11.9408 (11.7411)\tVB Loss 2.3528 (3.1075)\tF1 0.833 (0.658)\n",
            "Epoch: [46][1000/1405]\tBatch Time 0.154 (0.225)\tData Load Time 0.005 (0.005)\tCE Loss 12.1602 (11.7435)\tVB Loss 3.8322 (3.0880)\tF1 0.527 (0.660)\n",
            "Epoch: [46][1100/1405]\tBatch Time 0.309 (0.224)\tData Load Time 0.004 (0.005)\tCE Loss 11.6519 (11.7468)\tVB Loss 2.7343 (3.0914)\tF1 0.554 (0.660)\n",
            "Epoch: [46][1200/1405]\tBatch Time 0.225 (0.224)\tData Load Time 0.006 (0.005)\tCE Loss 11.0962 (11.7406)\tVB Loss 4.7887 (3.0757)\tF1 0.520 (0.661)\n",
            "Epoch: [46][1300/1405]\tBatch Time 0.246 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 12.0253 (11.7382)\tVB Loss 2.5328 (3.0576)\tF1 0.780 (0.662)\n",
            "Epoch: [46][1400/1405]\tBatch Time 0.172 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 11.5260 (11.7377)\tVB Loss 3.1059 (3.0737)\tF1 0.499 (0.661)\n",
            "Validation: [0/325]\tBatch Time 0.315 (0.315)\tVB Loss 4.6581 (4.6581)\tF1 Score 0.659 (0.659)\t\n",
            "Validation: [100/325]\tBatch Time 0.095 (0.115)\tVB Loss 2.3054 (2.5062)\tF1 Score 0.817 (0.760)\t\n",
            "Validation: [200/325]\tBatch Time 0.133 (0.114)\tVB Loss 2.8334 (2.6032)\tF1 Score 0.568 (0.757)\t\n",
            "Validation: [300/325]\tBatch Time 0.096 (0.114)\tVB Loss 3.0137 (2.5849)\tF1 Score 0.774 (0.756)\t\n",
            "\n",
            " * LOSS - 2.575, F1 SCORE - 0.752\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000448\n",
            "\n",
            "Epoch: [47][0/1405]\tBatch Time 0.514 (0.514)\tData Load Time 0.226 (0.226)\tCE Loss 11.6164 (11.6164)\tVB Loss 2.9435 (2.9435)\tF1 0.682 (0.682)\n",
            "Epoch: [47][100/1405]\tBatch Time 0.098 (0.248)\tData Load Time 0.005 (0.007)\tCE Loss 11.3497 (11.6752)\tVB Loss 1.6037 (3.1903)\tF1 0.822 (0.666)\n",
            "Epoch: [47][200/1405]\tBatch Time 0.192 (0.233)\tData Load Time 0.005 (0.006)\tCE Loss 11.5914 (11.7150)\tVB Loss 2.8193 (3.1178)\tF1 0.440 (0.666)\n",
            "Epoch: [47][300/1405]\tBatch Time 0.252 (0.229)\tData Load Time 0.006 (0.006)\tCE Loss 12.1772 (11.7374)\tVB Loss 3.1397 (3.1187)\tF1 0.881 (0.666)\n",
            "Epoch: [47][400/1405]\tBatch Time 0.222 (0.225)\tData Load Time 0.005 (0.005)\tCE Loss 11.4924 (11.7195)\tVB Loss 1.4560 (3.0825)\tF1 0.903 (0.664)\n",
            "Epoch: [47][500/1405]\tBatch Time 0.241 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 11.6527 (11.7247)\tVB Loss 2.3506 (3.0942)\tF1 0.688 (0.661)\n",
            "Epoch: [47][600/1405]\tBatch Time 0.147 (0.223)\tData Load Time 0.006 (0.005)\tCE Loss 11.1080 (11.7321)\tVB Loss 1.1303 (3.0707)\tF1 0.714 (0.661)\n",
            "Epoch: [47][700/1405]\tBatch Time 0.169 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 12.1644 (11.7251)\tVB Loss 2.8793 (3.0363)\tF1 0.654 (0.661)\n",
            "Epoch: [47][800/1405]\tBatch Time 0.154 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 10.3202 (11.7283)\tVB Loss 1.5935 (3.0402)\tF1 0.797 (0.663)\n",
            "Epoch: [47][900/1405]\tBatch Time 0.243 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 12.0195 (11.7225)\tVB Loss 5.9461 (3.0272)\tF1 0.617 (0.665)\n",
            "Epoch: [47][1000/1405]\tBatch Time 0.219 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 11.2875 (11.7212)\tVB Loss 5.9518 (3.0325)\tF1 0.353 (0.664)\n",
            "Epoch: [47][1100/1405]\tBatch Time 0.148 (0.224)\tData Load Time 0.004 (0.005)\tCE Loss 10.8746 (11.7144)\tVB Loss 1.0647 (3.0350)\tF1 0.730 (0.663)\n",
            "Epoch: [47][1200/1405]\tBatch Time 0.186 (0.224)\tData Load Time 0.004 (0.005)\tCE Loss 12.0764 (11.7191)\tVB Loss 3.0743 (3.0257)\tF1 0.629 (0.664)\n",
            "Epoch: [47][1300/1405]\tBatch Time 0.244 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 11.8138 (11.7198)\tVB Loss 3.9220 (3.0146)\tF1 0.669 (0.664)\n",
            "Epoch: [47][1400/1405]\tBatch Time 0.188 (0.224)\tData Load Time 0.004 (0.005)\tCE Loss 11.4419 (11.7208)\tVB Loss 4.2508 (3.0264)\tF1 0.429 (0.664)\n",
            "Validation: [0/325]\tBatch Time 0.300 (0.300)\tVB Loss 1.0602 (1.0602)\tF1 Score 0.917 (0.917)\t\n",
            "Validation: [100/325]\tBatch Time 0.086 (0.110)\tVB Loss 3.8906 (2.4440)\tF1 Score 0.511 (0.770)\t\n",
            "Validation: [200/325]\tBatch Time 0.082 (0.112)\tVB Loss 2.1168 (2.4421)\tF1 Score 0.809 (0.763)\t\n",
            "Validation: [300/325]\tBatch Time 0.199 (0.115)\tVB Loss 0.4740 (2.4767)\tF1 Score 0.931 (0.764)\t\n",
            "\n",
            " * LOSS - 2.507, F1 SCORE - 0.763\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000441\n",
            "\n",
            "Epoch: [48][0/1405]\tBatch Time 0.478 (0.478)\tData Load Time 0.210 (0.210)\tCE Loss 11.9367 (11.9367)\tVB Loss 1.8322 (1.8322)\tF1 0.715 (0.715)\n",
            "Epoch: [48][100/1405]\tBatch Time 0.272 (0.248)\tData Load Time 0.005 (0.007)\tCE Loss 11.6947 (11.6650)\tVB Loss 2.9009 (2.9155)\tF1 0.819 (0.685)\n",
            "Epoch: [48][200/1405]\tBatch Time 0.155 (0.240)\tData Load Time 0.005 (0.006)\tCE Loss 11.7412 (11.7176)\tVB Loss 0.9880 (2.9908)\tF1 0.788 (0.675)\n",
            "Epoch: [48][300/1405]\tBatch Time 0.240 (0.234)\tData Load Time 0.004 (0.006)\tCE Loss 11.7600 (11.7103)\tVB Loss 4.5333 (2.9805)\tF1 0.694 (0.676)\n",
            "Epoch: [48][400/1405]\tBatch Time 0.197 (0.230)\tData Load Time 0.004 (0.005)\tCE Loss 12.0550 (11.7144)\tVB Loss 2.1224 (3.0183)\tF1 0.675 (0.674)\n",
            "Epoch: [48][500/1405]\tBatch Time 0.204 (0.228)\tData Load Time 0.005 (0.005)\tCE Loss 11.3832 (11.7090)\tVB Loss 0.8825 (3.0200)\tF1 0.953 (0.676)\n",
            "Epoch: [48][600/1405]\tBatch Time 0.282 (0.226)\tData Load Time 0.006 (0.005)\tCE Loss 9.9204 (11.7045)\tVB Loss 1.6581 (3.0273)\tF1 0.735 (0.671)\n",
            "Epoch: [48][700/1405]\tBatch Time 0.232 (0.225)\tData Load Time 0.004 (0.005)\tCE Loss 12.1465 (11.7080)\tVB Loss 5.6888 (3.0052)\tF1 0.663 (0.671)\n",
            "Epoch: [48][800/1405]\tBatch Time 0.222 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 11.7451 (11.7101)\tVB Loss 2.3498 (2.9921)\tF1 0.600 (0.670)\n",
            "Epoch: [48][900/1405]\tBatch Time 0.160 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 11.3024 (11.7066)\tVB Loss 1.8541 (2.9941)\tF1 0.832 (0.670)\n",
            "Epoch: [48][1000/1405]\tBatch Time 0.256 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 11.4544 (11.7023)\tVB Loss 3.5541 (2.9882)\tF1 0.563 (0.670)\n",
            "Epoch: [48][1100/1405]\tBatch Time 0.253 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 12.1560 (11.7003)\tVB Loss 3.0397 (3.0041)\tF1 0.715 (0.669)\n",
            "Epoch: [48][1200/1405]\tBatch Time 0.229 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 11.7264 (11.7086)\tVB Loss 1.3831 (3.0002)\tF1 0.938 (0.669)\n",
            "Epoch: [48][1300/1405]\tBatch Time 0.227 (0.221)\tData Load Time 0.004 (0.005)\tCE Loss 11.9627 (11.7065)\tVB Loss 4.3135 (2.9805)\tF1 0.643 (0.671)\n",
            "Epoch: [48][1400/1405]\tBatch Time 0.283 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 11.7259 (11.7015)\tVB Loss 3.7103 (2.9808)\tF1 0.742 (0.671)\n",
            "Validation: [0/325]\tBatch Time 0.290 (0.290)\tVB Loss 2.7319 (2.7319)\tF1 Score 0.693 (0.693)\t\n",
            "Validation: [100/325]\tBatch Time 0.173 (0.115)\tVB Loss 3.5901 (2.3361)\tF1 Score 0.768 (0.769)\t\n",
            "Validation: [200/325]\tBatch Time 0.132 (0.117)\tVB Loss 4.6043 (2.4150)\tF1 Score 0.680 (0.756)\t\n",
            "Validation: [300/325]\tBatch Time 0.059 (0.116)\tVB Loss 0.9253 (2.4246)\tF1 Score 0.824 (0.761)\t\n",
            "\n",
            " * LOSS - 2.450, F1 SCORE - 0.761\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000435\n",
            "\n",
            "Epoch: [49][0/1405]\tBatch Time 0.502 (0.502)\tData Load Time 0.210 (0.210)\tCE Loss 12.1206 (12.1206)\tVB Loss 4.6513 (4.6513)\tF1 0.562 (0.562)\n",
            "Epoch: [49][100/1405]\tBatch Time 0.210 (0.251)\tData Load Time 0.005 (0.007)\tCE Loss 11.6885 (11.8162)\tVB Loss 2.8010 (3.1217)\tF1 0.628 (0.660)\n",
            "Epoch: [49][200/1405]\tBatch Time 0.260 (0.236)\tData Load Time 0.004 (0.006)\tCE Loss 12.3589 (11.7290)\tVB Loss 2.9781 (3.0469)\tF1 0.679 (0.661)\n",
            "Epoch: [49][300/1405]\tBatch Time 0.295 (0.227)\tData Load Time 0.005 (0.006)\tCE Loss 12.0264 (11.7222)\tVB Loss 3.2847 (2.9913)\tF1 0.573 (0.660)\n",
            "Epoch: [49][400/1405]\tBatch Time 0.197 (0.226)\tData Load Time 0.005 (0.005)\tCE Loss 11.7679 (11.7121)\tVB Loss 3.6451 (3.0395)\tF1 0.667 (0.663)\n",
            "Epoch: [49][500/1405]\tBatch Time 0.208 (0.224)\tData Load Time 0.004 (0.005)\tCE Loss 11.9221 (11.7043)\tVB Loss 3.0476 (3.0102)\tF1 0.679 (0.666)\n",
            "Epoch: [49][600/1405]\tBatch Time 0.214 (0.224)\tData Load Time 0.006 (0.005)\tCE Loss 12.7592 (11.7049)\tVB Loss 4.6085 (2.9957)\tF1 0.649 (0.671)\n",
            "Epoch: [49][700/1405]\tBatch Time 0.284 (0.224)\tData Load Time 0.004 (0.005)\tCE Loss 12.2850 (11.6974)\tVB Loss 5.9576 (2.9720)\tF1 0.685 (0.672)\n",
            "Epoch: [49][800/1405]\tBatch Time 0.210 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 11.3167 (11.6968)\tVB Loss 2.0519 (2.9477)\tF1 0.705 (0.676)\n",
            "Epoch: [49][900/1405]\tBatch Time 0.225 (0.223)\tData Load Time 0.006 (0.005)\tCE Loss 11.7312 (11.6922)\tVB Loss 1.3750 (2.9526)\tF1 0.688 (0.677)\n",
            "Epoch: [49][1000/1405]\tBatch Time 0.209 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 11.5905 (11.6902)\tVB Loss 3.0698 (2.9417)\tF1 0.750 (0.677)\n",
            "Epoch: [49][1100/1405]\tBatch Time 0.190 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 11.4777 (11.6893)\tVB Loss 2.5524 (2.9440)\tF1 0.744 (0.677)\n",
            "Epoch: [49][1200/1405]\tBatch Time 0.276 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 11.6229 (11.6907)\tVB Loss 2.6684 (2.9555)\tF1 0.843 (0.676)\n",
            "Epoch: [49][1300/1405]\tBatch Time 0.172 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 11.9254 (11.6870)\tVB Loss 3.7390 (2.9592)\tF1 0.603 (0.676)\n",
            "Epoch: [49][1400/1405]\tBatch Time 0.249 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 11.0808 (11.6846)\tVB Loss 4.3929 (2.9489)\tF1 0.665 (0.676)\n",
            "Validation: [0/325]\tBatch Time 0.297 (0.297)\tVB Loss 0.7747 (0.7747)\tF1 Score 0.928 (0.928)\t\n",
            "Validation: [100/325]\tBatch Time 0.120 (0.106)\tVB Loss 3.2782 (2.7352)\tF1 Score 0.660 (0.750)\t\n",
            "Validation: [200/325]\tBatch Time 0.146 (0.116)\tVB Loss 5.0535 (2.5866)\tF1 Score 0.761 (0.758)\t\n",
            "Validation: [300/325]\tBatch Time 0.119 (0.118)\tVB Loss 2.8642 (2.6146)\tF1 Score 0.664 (0.753)\t\n",
            "\n",
            " * LOSS - 2.600, F1 SCORE - 0.755\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000429\n",
            "\n",
            "Epoch: [50][0/1405]\tBatch Time 0.565 (0.565)\tData Load Time 0.213 (0.213)\tCE Loss 11.6230 (11.6230)\tVB Loss 3.7145 (3.7145)\tF1 0.626 (0.626)\n",
            "Epoch: [50][100/1405]\tBatch Time 0.191 (0.252)\tData Load Time 0.004 (0.007)\tCE Loss 12.2781 (11.6981)\tVB Loss 1.2105 (2.9369)\tF1 0.773 (0.674)\n",
            "Epoch: [50][200/1405]\tBatch Time 0.230 (0.237)\tData Load Time 0.004 (0.006)\tCE Loss 11.5576 (11.6931)\tVB Loss 2.0426 (2.8703)\tF1 0.507 (0.678)\n",
            "Epoch: [50][300/1405]\tBatch Time 0.228 (0.233)\tData Load Time 0.004 (0.006)\tCE Loss 11.5247 (11.7087)\tVB Loss 2.7030 (2.8988)\tF1 0.482 (0.675)\n",
            "Epoch: [50][400/1405]\tBatch Time 0.244 (0.229)\tData Load Time 0.005 (0.005)\tCE Loss 10.9475 (11.6833)\tVB Loss 3.0781 (2.8510)\tF1 0.743 (0.677)\n",
            "Epoch: [50][500/1405]\tBatch Time 0.254 (0.226)\tData Load Time 0.005 (0.005)\tCE Loss 12.1600 (11.6850)\tVB Loss 3.3116 (2.8349)\tF1 0.687 (0.681)\n",
            "Epoch: [50][600/1405]\tBatch Time 0.232 (0.226)\tData Load Time 0.004 (0.005)\tCE Loss 12.4975 (11.6904)\tVB Loss 2.0745 (2.8602)\tF1 0.669 (0.679)\n",
            "Epoch: [50][700/1405]\tBatch Time 0.129 (0.225)\tData Load Time 0.004 (0.005)\tCE Loss 11.7229 (11.6862)\tVB Loss 0.8680 (2.8636)\tF1 0.859 (0.676)\n",
            "Epoch: [50][800/1405]\tBatch Time 0.206 (0.224)\tData Load Time 0.004 (0.005)\tCE Loss 11.8928 (11.6780)\tVB Loss 0.9572 (2.8498)\tF1 0.776 (0.681)\n",
            "Epoch: [50][900/1405]\tBatch Time 0.176 (0.224)\tData Load Time 0.007 (0.005)\tCE Loss 11.5416 (11.6787)\tVB Loss 5.3066 (2.8484)\tF1 0.448 (0.680)\n",
            "Epoch: [50][1000/1405]\tBatch Time 0.176 (0.225)\tData Load Time 0.005 (0.005)\tCE Loss 11.3233 (11.6766)\tVB Loss 0.9136 (2.8645)\tF1 0.959 (0.680)\n",
            "Epoch: [50][1100/1405]\tBatch Time 0.279 (0.225)\tData Load Time 0.005 (0.005)\tCE Loss 12.1000 (11.6760)\tVB Loss 2.5069 (2.8930)\tF1 0.578 (0.679)\n",
            "Epoch: [50][1200/1405]\tBatch Time 0.203 (0.225)\tData Load Time 0.004 (0.005)\tCE Loss 11.5975 (11.6750)\tVB Loss 3.7688 (2.9123)\tF1 0.560 (0.678)\n",
            "Epoch: [50][1300/1405]\tBatch Time 0.280 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 11.6981 (11.6718)\tVB Loss 2.0854 (2.9015)\tF1 0.806 (0.679)\n",
            "Epoch: [50][1400/1405]\tBatch Time 0.171 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 12.5874 (11.6691)\tVB Loss 1.8534 (2.9004)\tF1 0.670 (0.679)\n",
            "Validation: [0/325]\tBatch Time 0.294 (0.294)\tVB Loss 1.7948 (1.7948)\tF1 Score 0.953 (0.953)\t\n",
            "Validation: [100/325]\tBatch Time 0.084 (0.117)\tVB Loss 2.0543 (2.3690)\tF1 Score 0.748 (0.772)\t\n",
            "Validation: [200/325]\tBatch Time 0.073 (0.121)\tVB Loss 1.9464 (2.4143)\tF1 Score 0.602 (0.767)\t\n",
            "Validation: [300/325]\tBatch Time 0.098 (0.122)\tVB Loss 2.3505 (2.4905)\tF1 Score 0.652 (0.762)\t\n",
            "\n",
            " * LOSS - 2.466, F1 SCORE - 0.764\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000423\n",
            "\n",
            "Epoch: [51][0/1405]\tBatch Time 0.503 (0.503)\tData Load Time 0.199 (0.199)\tCE Loss 11.2788 (11.2788)\tVB Loss 3.4234 (3.4234)\tF1 0.784 (0.784)\n",
            "Epoch: [51][100/1405]\tBatch Time 0.214 (0.247)\tData Load Time 0.005 (0.007)\tCE Loss 12.2148 (11.6460)\tVB Loss 2.8293 (2.9438)\tF1 0.845 (0.682)\n",
            "Epoch: [51][200/1405]\tBatch Time 0.207 (0.236)\tData Load Time 0.005 (0.006)\tCE Loss 11.6680 (11.6764)\tVB Loss 3.1698 (2.9952)\tF1 0.712 (0.676)\n",
            "Epoch: [51][300/1405]\tBatch Time 0.233 (0.227)\tData Load Time 0.005 (0.006)\tCE Loss 11.3751 (11.7018)\tVB Loss 2.5371 (2.9669)\tF1 0.682 (0.677)\n",
            "Epoch: [51][400/1405]\tBatch Time 0.254 (0.226)\tData Load Time 0.005 (0.006)\tCE Loss 11.9033 (11.6881)\tVB Loss 4.0421 (3.0078)\tF1 0.727 (0.674)\n",
            "Epoch: [51][500/1405]\tBatch Time 0.204 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 12.4620 (11.6925)\tVB Loss 2.7938 (2.9967)\tF1 0.798 (0.673)\n",
            "Epoch: [51][600/1405]\tBatch Time 0.268 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 11.7204 (11.6666)\tVB Loss 4.0727 (2.9652)\tF1 0.695 (0.674)\n",
            "Epoch: [51][700/1405]\tBatch Time 0.274 (0.221)\tData Load Time 0.004 (0.005)\tCE Loss 11.5558 (11.6615)\tVB Loss 0.8780 (2.9450)\tF1 0.875 (0.675)\n",
            "Epoch: [51][800/1405]\tBatch Time 0.165 (0.221)\tData Load Time 0.005 (0.005)\tCE Loss 11.4109 (11.6560)\tVB Loss 2.2292 (2.9063)\tF1 0.788 (0.677)\n",
            "Epoch: [51][900/1405]\tBatch Time 0.203 (0.221)\tData Load Time 0.004 (0.005)\tCE Loss 11.8860 (11.6488)\tVB Loss 2.2307 (2.8870)\tF1 0.916 (0.680)\n",
            "Epoch: [51][1000/1405]\tBatch Time 0.239 (0.221)\tData Load Time 0.004 (0.005)\tCE Loss 11.5443 (11.6476)\tVB Loss 3.7355 (2.8752)\tF1 0.576 (0.680)\n",
            "Epoch: [51][1100/1405]\tBatch Time 0.170 (0.221)\tData Load Time 0.005 (0.005)\tCE Loss 11.5571 (11.6482)\tVB Loss 1.6964 (2.8685)\tF1 0.697 (0.682)\n",
            "Epoch: [51][1200/1405]\tBatch Time 0.254 (0.221)\tData Load Time 0.004 (0.005)\tCE Loss 11.4078 (11.6484)\tVB Loss 2.1720 (2.8588)\tF1 0.640 (0.683)\n",
            "Epoch: [51][1300/1405]\tBatch Time 0.240 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 12.2817 (11.6514)\tVB Loss 3.5901 (2.8628)\tF1 0.731 (0.683)\n",
            "Epoch: [51][1400/1405]\tBatch Time 0.246 (0.222)\tData Load Time 0.006 (0.005)\tCE Loss 11.9327 (11.6541)\tVB Loss 2.6588 (2.8681)\tF1 0.578 (0.684)\n",
            "Validation: [0/325]\tBatch Time 0.338 (0.338)\tVB Loss 1.1978 (1.1978)\tF1 Score 0.715 (0.715)\t\n",
            "Validation: [100/325]\tBatch Time 0.147 (0.116)\tVB Loss 4.0602 (2.3880)\tF1 Score 0.729 (0.774)\t\n",
            "Validation: [200/325]\tBatch Time 0.133 (0.121)\tVB Loss 2.5966 (2.4190)\tF1 Score 0.693 (0.771)\t\n",
            "Validation: [300/325]\tBatch Time 0.128 (0.121)\tVB Loss 1.8417 (2.4033)\tF1 Score 0.854 (0.772)\t\n",
            "\n",
            " * LOSS - 2.413, F1 SCORE - 0.771\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000417\n",
            "\n",
            "Epoch: [52][0/1405]\tBatch Time 0.435 (0.435)\tData Load Time 0.193 (0.193)\tCE Loss 11.8786 (11.8786)\tVB Loss 1.4172 (1.4172)\tF1 0.746 (0.746)\n",
            "Epoch: [52][100/1405]\tBatch Time 0.233 (0.248)\tData Load Time 0.006 (0.007)\tCE Loss 11.8150 (11.6110)\tVB Loss 3.4055 (2.6633)\tF1 0.628 (0.716)\n",
            "Epoch: [52][200/1405]\tBatch Time 0.227 (0.239)\tData Load Time 0.005 (0.006)\tCE Loss 11.9818 (11.6674)\tVB Loss 2.0040 (2.7764)\tF1 0.879 (0.710)\n",
            "Epoch: [52][300/1405]\tBatch Time 0.291 (0.235)\tData Load Time 0.005 (0.006)\tCE Loss 11.6412 (11.6713)\tVB Loss 3.6568 (2.8534)\tF1 0.794 (0.698)\n",
            "Epoch: [52][400/1405]\tBatch Time 0.160 (0.232)\tData Load Time 0.004 (0.005)\tCE Loss 12.4910 (11.6731)\tVB Loss 3.1956 (2.8397)\tF1 0.773 (0.694)\n",
            "Epoch: [52][500/1405]\tBatch Time 0.218 (0.231)\tData Load Time 0.005 (0.005)\tCE Loss 11.2425 (11.6638)\tVB Loss 3.5592 (2.8442)\tF1 0.714 (0.699)\n",
            "Epoch: [52][600/1405]\tBatch Time 0.182 (0.231)\tData Load Time 0.005 (0.005)\tCE Loss 10.8358 (11.6652)\tVB Loss 3.8190 (2.8540)\tF1 0.430 (0.696)\n",
            "Epoch: [52][700/1405]\tBatch Time 0.286 (0.230)\tData Load Time 0.005 (0.005)\tCE Loss 11.4594 (11.6551)\tVB Loss 3.5605 (2.8444)\tF1 0.667 (0.694)\n",
            "Epoch: [52][800/1405]\tBatch Time 0.181 (0.230)\tData Load Time 0.005 (0.005)\tCE Loss 10.4056 (11.6550)\tVB Loss 1.3754 (2.8529)\tF1 0.895 (0.693)\n",
            "Epoch: [52][900/1405]\tBatch Time 0.152 (0.228)\tData Load Time 0.004 (0.005)\tCE Loss 12.1145 (11.6502)\tVB Loss 2.6430 (2.8343)\tF1 0.670 (0.694)\n",
            "Epoch: [52][1000/1405]\tBatch Time 0.257 (0.228)\tData Load Time 0.006 (0.005)\tCE Loss 11.9010 (11.6510)\tVB Loss 4.5164 (2.8410)\tF1 0.596 (0.692)\n",
            "Epoch: [52][1100/1405]\tBatch Time 0.321 (0.227)\tData Load Time 0.005 (0.005)\tCE Loss 11.2647 (11.6457)\tVB Loss 4.9654 (2.8244)\tF1 0.624 (0.694)\n",
            "Epoch: [52][1200/1405]\tBatch Time 0.214 (0.227)\tData Load Time 0.005 (0.005)\tCE Loss 11.6930 (11.6460)\tVB Loss 2.5059 (2.8108)\tF1 0.677 (0.694)\n",
            "Epoch: [52][1300/1405]\tBatch Time 0.250 (0.226)\tData Load Time 0.004 (0.005)\tCE Loss 12.1195 (11.6423)\tVB Loss 5.1663 (2.8115)\tF1 0.607 (0.694)\n",
            "Epoch: [52][1400/1405]\tBatch Time 0.156 (0.224)\tData Load Time 0.004 (0.005)\tCE Loss 10.9059 (11.6387)\tVB Loss 1.1743 (2.8035)\tF1 0.895 (0.693)\n",
            "Validation: [0/325]\tBatch Time 0.365 (0.365)\tVB Loss 1.4799 (1.4799)\tF1 Score 0.700 (0.700)\t\n",
            "Validation: [100/325]\tBatch Time 0.151 (0.113)\tVB Loss 1.8783 (2.4069)\tF1 Score 0.898 (0.779)\t\n",
            "Validation: [200/325]\tBatch Time 0.135 (0.117)\tVB Loss 4.4369 (2.3636)\tF1 Score 0.636 (0.775)\t\n",
            "Validation: [300/325]\tBatch Time 0.103 (0.119)\tVB Loss 3.1200 (2.3696)\tF1 Score 0.758 (0.773)\t\n",
            "\n",
            " * LOSS - 2.372, F1 SCORE - 0.775\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000411\n",
            "\n",
            "Epoch: [53][0/1405]\tBatch Time 0.521 (0.521)\tData Load Time 0.222 (0.222)\tCE Loss 11.3803 (11.3803)\tVB Loss 3.0283 (3.0283)\tF1 0.856 (0.856)\n",
            "Epoch: [53][100/1405]\tBatch Time 0.232 (0.243)\tData Load Time 0.004 (0.008)\tCE Loss 10.5432 (11.5889)\tVB Loss 6.9474 (2.6185)\tF1 0.478 (0.691)\n",
            "Epoch: [53][200/1405]\tBatch Time 0.185 (0.241)\tData Load Time 0.006 (0.006)\tCE Loss 10.9612 (11.6034)\tVB Loss 1.2361 (2.7376)\tF1 0.727 (0.697)\n",
            "Epoch: [53][300/1405]\tBatch Time 0.203 (0.233)\tData Load Time 0.004 (0.006)\tCE Loss 11.4942 (11.6013)\tVB Loss 4.6774 (2.6536)\tF1 0.757 (0.703)\n",
            "Epoch: [53][400/1405]\tBatch Time 0.231 (0.229)\tData Load Time 0.005 (0.006)\tCE Loss 11.4248 (11.6120)\tVB Loss 3.0880 (2.7518)\tF1 0.598 (0.694)\n",
            "Epoch: [53][500/1405]\tBatch Time 0.187 (0.226)\tData Load Time 0.005 (0.005)\tCE Loss 11.0595 (11.6170)\tVB Loss 2.7534 (2.7414)\tF1 0.633 (0.693)\n",
            "Epoch: [53][600/1405]\tBatch Time 0.277 (0.224)\tData Load Time 0.004 (0.005)\tCE Loss 11.7475 (11.6267)\tVB Loss 4.9490 (2.7285)\tF1 0.584 (0.697)\n",
            "Epoch: [53][700/1405]\tBatch Time 0.217 (0.224)\tData Load Time 0.006 (0.005)\tCE Loss 11.1545 (11.6143)\tVB Loss 2.8703 (2.7449)\tF1 0.818 (0.695)\n",
            "Epoch: [53][800/1405]\tBatch Time 0.251 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 12.0632 (11.6296)\tVB Loss 3.0778 (2.7341)\tF1 0.617 (0.697)\n",
            "Epoch: [53][900/1405]\tBatch Time 0.216 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 11.7641 (11.6308)\tVB Loss 4.1216 (2.7335)\tF1 0.580 (0.696)\n",
            "Epoch: [53][1000/1405]\tBatch Time 0.269 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 11.2870 (11.6321)\tVB Loss 3.2872 (2.7415)\tF1 0.815 (0.694)\n",
            "Epoch: [53][1100/1405]\tBatch Time 0.121 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 11.7135 (11.6286)\tVB Loss 0.6180 (2.7350)\tF1 0.958 (0.697)\n",
            "Epoch: [53][1200/1405]\tBatch Time 0.274 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 11.4919 (11.6202)\tVB Loss 4.7540 (2.7372)\tF1 0.608 (0.695)\n",
            "Epoch: [53][1300/1405]\tBatch Time 0.229 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 12.4395 (11.6225)\tVB Loss 2.2695 (2.7442)\tF1 0.570 (0.695)\n",
            "Epoch: [53][1400/1405]\tBatch Time 0.233 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 11.1805 (11.6220)\tVB Loss 2.0968 (2.7612)\tF1 0.849 (0.693)\n",
            "Validation: [0/325]\tBatch Time 0.324 (0.324)\tVB Loss 2.7123 (2.7123)\tF1 Score 0.700 (0.700)\t\n",
            "Validation: [100/325]\tBatch Time 0.108 (0.109)\tVB Loss 2.3560 (2.1809)\tF1 Score 0.693 (0.781)\t\n",
            "Validation: [200/325]\tBatch Time 0.098 (0.109)\tVB Loss 1.0083 (2.3379)\tF1 Score 0.861 (0.782)\t\n",
            "Validation: [300/325]\tBatch Time 0.129 (0.113)\tVB Loss 1.0443 (2.3241)\tF1 Score 0.954 (0.781)\t\n",
            "\n",
            " * LOSS - 2.311, F1 SCORE - 0.781\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000405\n",
            "\n",
            "Epoch: [54][0/1405]\tBatch Time 0.460 (0.460)\tData Load Time 0.203 (0.203)\tCE Loss 11.2679 (11.2679)\tVB Loss 2.4751 (2.4751)\tF1 0.890 (0.890)\n",
            "Epoch: [54][100/1405]\tBatch Time 0.163 (0.251)\tData Load Time 0.005 (0.007)\tCE Loss 11.3513 (11.6770)\tVB Loss 2.4152 (2.7748)\tF1 0.806 (0.702)\n",
            "Epoch: [54][200/1405]\tBatch Time 0.222 (0.240)\tData Load Time 0.004 (0.006)\tCE Loss 11.9585 (11.6514)\tVB Loss 2.6773 (2.7619)\tF1 0.864 (0.701)\n",
            "Epoch: [54][300/1405]\tBatch Time 0.170 (0.233)\tData Load Time 0.005 (0.006)\tCE Loss 11.0746 (11.6376)\tVB Loss 3.7885 (2.7543)\tF1 0.559 (0.700)\n",
            "Epoch: [54][400/1405]\tBatch Time 0.233 (0.230)\tData Load Time 0.004 (0.005)\tCE Loss 10.8527 (11.6242)\tVB Loss 3.4229 (2.6706)\tF1 0.627 (0.704)\n",
            "Epoch: [54][500/1405]\tBatch Time 0.149 (0.228)\tData Load Time 0.004 (0.005)\tCE Loss 11.4347 (11.6135)\tVB Loss 5.3829 (2.6914)\tF1 0.527 (0.702)\n",
            "Epoch: [54][600/1405]\tBatch Time 0.186 (0.227)\tData Load Time 0.004 (0.005)\tCE Loss 12.2218 (11.6134)\tVB Loss 0.4381 (2.7041)\tF1 1.000 (0.698)\n",
            "Epoch: [54][700/1405]\tBatch Time 0.211 (0.227)\tData Load Time 0.005 (0.005)\tCE Loss 11.1548 (11.6107)\tVB Loss 3.3167 (2.7141)\tF1 0.598 (0.699)\n",
            "Epoch: [54][800/1405]\tBatch Time 0.249 (0.226)\tData Load Time 0.005 (0.005)\tCE Loss 12.0863 (11.6062)\tVB Loss 3.3684 (2.7345)\tF1 0.781 (0.699)\n",
            "Epoch: [54][900/1405]\tBatch Time 0.257 (0.225)\tData Load Time 0.004 (0.005)\tCE Loss 11.7840 (11.6034)\tVB Loss 1.6893 (2.7121)\tF1 0.828 (0.699)\n",
            "Epoch: [54][1000/1405]\tBatch Time 0.276 (0.224)\tData Load Time 0.006 (0.005)\tCE Loss 11.6661 (11.6063)\tVB Loss 3.5448 (2.7160)\tF1 0.704 (0.697)\n",
            "Epoch: [54][1100/1405]\tBatch Time 0.254 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 11.8376 (11.6055)\tVB Loss 2.8428 (2.7149)\tF1 0.620 (0.697)\n",
            "Epoch: [54][1200/1405]\tBatch Time 0.219 (0.224)\tData Load Time 0.006 (0.005)\tCE Loss 11.1049 (11.6039)\tVB Loss 1.4109 (2.7030)\tF1 0.887 (0.697)\n",
            "Epoch: [54][1300/1405]\tBatch Time 0.187 (0.223)\tData Load Time 0.006 (0.005)\tCE Loss 12.3292 (11.6059)\tVB Loss 2.0007 (2.7193)\tF1 0.533 (0.695)\n",
            "Epoch: [54][1400/1405]\tBatch Time 0.234 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 10.9847 (11.6052)\tVB Loss 4.1939 (2.7210)\tF1 0.420 (0.695)\n",
            "Validation: [0/325]\tBatch Time 0.325 (0.325)\tVB Loss 2.2097 (2.2097)\tF1 Score 0.854 (0.854)\t\n",
            "Validation: [100/325]\tBatch Time 0.142 (0.127)\tVB Loss 4.3072 (2.3835)\tF1 Score 0.560 (0.759)\t\n",
            "Validation: [200/325]\tBatch Time 0.111 (0.123)\tVB Loss 2.2106 (2.3086)\tF1 Score 0.862 (0.766)\t\n",
            "Validation: [300/325]\tBatch Time 0.139 (0.122)\tVB Loss 2.7659 (2.2533)\tF1 Score 0.822 (0.776)\t\n",
            "\n",
            " * LOSS - 2.269, F1 SCORE - 0.779\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000400\n",
            "\n",
            "Epoch: [55][0/1405]\tBatch Time 0.528 (0.528)\tData Load Time 0.183 (0.183)\tCE Loss 11.4691 (11.4691)\tVB Loss 3.9361 (3.9361)\tF1 0.516 (0.516)\n",
            "Epoch: [55][100/1405]\tBatch Time 0.174 (0.251)\tData Load Time 0.004 (0.007)\tCE Loss 10.1558 (11.6186)\tVB Loss 3.1678 (2.6696)\tF1 0.575 (0.709)\n",
            "Epoch: [55][200/1405]\tBatch Time 0.181 (0.234)\tData Load Time 0.005 (0.006)\tCE Loss 11.3577 (11.6013)\tVB Loss 2.4365 (2.7291)\tF1 0.798 (0.697)\n",
            "Epoch: [55][300/1405]\tBatch Time 0.246 (0.227)\tData Load Time 0.005 (0.005)\tCE Loss 11.4374 (11.5862)\tVB Loss 3.4736 (2.7056)\tF1 0.693 (0.699)\n",
            "Epoch: [55][400/1405]\tBatch Time 0.209 (0.226)\tData Load Time 0.005 (0.005)\tCE Loss 11.6499 (11.6102)\tVB Loss 4.7028 (2.7203)\tF1 0.601 (0.696)\n",
            "Epoch: [55][500/1405]\tBatch Time 0.258 (0.226)\tData Load Time 0.004 (0.005)\tCE Loss 12.2872 (11.6114)\tVB Loss 2.2640 (2.7148)\tF1 0.578 (0.696)\n",
            "Epoch: [55][600/1405]\tBatch Time 0.252 (0.225)\tData Load Time 0.005 (0.005)\tCE Loss 12.0178 (11.5961)\tVB Loss 2.6637 (2.7349)\tF1 0.566 (0.692)\n",
            "Epoch: [55][700/1405]\tBatch Time 0.265 (0.226)\tData Load Time 0.004 (0.005)\tCE Loss 10.7429 (11.5965)\tVB Loss 1.6856 (2.7511)\tF1 0.821 (0.689)\n",
            "Epoch: [55][800/1405]\tBatch Time 0.161 (0.226)\tData Load Time 0.005 (0.005)\tCE Loss 12.0554 (11.6014)\tVB Loss 2.3175 (2.7475)\tF1 0.792 (0.688)\n",
            "Epoch: [55][900/1405]\tBatch Time 0.103 (0.225)\tData Load Time 0.004 (0.005)\tCE Loss 10.7904 (11.5958)\tVB Loss 1.1184 (2.7134)\tF1 0.979 (0.691)\n",
            "Epoch: [55][1000/1405]\tBatch Time 0.185 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 11.6429 (11.5882)\tVB Loss 1.7304 (2.7063)\tF1 0.917 (0.693)\n",
            "Epoch: [55][1100/1405]\tBatch Time 0.243 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 11.9971 (11.5888)\tVB Loss 3.7929 (2.7003)\tF1 0.688 (0.697)\n",
            "Epoch: [55][1200/1405]\tBatch Time 0.214 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 11.0155 (11.5868)\tVB Loss 4.0474 (2.6815)\tF1 0.627 (0.698)\n",
            "Epoch: [55][1300/1405]\tBatch Time 0.164 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 11.2690 (11.5830)\tVB Loss 3.4136 (2.6872)\tF1 0.696 (0.699)\n",
            "Epoch: [55][1400/1405]\tBatch Time 0.205 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 11.7348 (11.5891)\tVB Loss 2.0875 (2.6857)\tF1 0.573 (0.701)\n",
            "Validation: [0/325]\tBatch Time 0.309 (0.309)\tVB Loss 4.2002 (4.2002)\tF1 Score 0.757 (0.757)\t\n",
            "Validation: [100/325]\tBatch Time 0.157 (0.125)\tVB Loss 2.4465 (2.2654)\tF1 Score 0.804 (0.788)\t\n",
            "Validation: [200/325]\tBatch Time 0.070 (0.123)\tVB Loss 1.8884 (2.3140)\tF1 Score 0.697 (0.793)\t\n",
            "Validation: [300/325]\tBatch Time 0.105 (0.123)\tVB Loss 1.6702 (2.2183)\tF1 Score 0.932 (0.793)\t\n",
            "\n",
            " * LOSS - 2.232, F1 SCORE - 0.791\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000395\n",
            "\n",
            "Epoch: [56][0/1405]\tBatch Time 0.501 (0.501)\tData Load Time 0.209 (0.209)\tCE Loss 12.0121 (12.0121)\tVB Loss 2.3152 (2.3152)\tF1 0.662 (0.662)\n",
            "Epoch: [56][100/1405]\tBatch Time 0.158 (0.255)\tData Load Time 0.005 (0.008)\tCE Loss 12.4307 (11.6027)\tVB Loss 2.4808 (2.7133)\tF1 0.739 (0.711)\n",
            "Epoch: [56][200/1405]\tBatch Time 0.146 (0.238)\tData Load Time 0.004 (0.006)\tCE Loss 11.5845 (11.6194)\tVB Loss 1.6299 (2.6186)\tF1 0.622 (0.718)\n",
            "Epoch: [56][300/1405]\tBatch Time 0.198 (0.232)\tData Load Time 0.004 (0.006)\tCE Loss 12.6845 (11.6016)\tVB Loss 2.8766 (2.6216)\tF1 0.693 (0.720)\n",
            "Epoch: [56][400/1405]\tBatch Time 0.212 (0.230)\tData Load Time 0.004 (0.005)\tCE Loss 10.7361 (11.5982)\tVB Loss 2.3416 (2.6572)\tF1 0.765 (0.715)\n",
            "Epoch: [56][500/1405]\tBatch Time 0.266 (0.226)\tData Load Time 0.004 (0.005)\tCE Loss 11.2070 (11.5924)\tVB Loss 1.9032 (2.6325)\tF1 0.902 (0.712)\n",
            "Epoch: [56][600/1405]\tBatch Time 0.215 (0.225)\tData Load Time 0.004 (0.005)\tCE Loss 11.1758 (11.5871)\tVB Loss 1.1614 (2.6236)\tF1 0.651 (0.707)\n",
            "Epoch: [56][700/1405]\tBatch Time 0.222 (0.225)\tData Load Time 0.005 (0.005)\tCE Loss 11.6595 (11.5885)\tVB Loss 2.8781 (2.6630)\tF1 0.687 (0.706)\n",
            "Epoch: [56][800/1405]\tBatch Time 0.240 (0.223)\tData Load Time 0.006 (0.005)\tCE Loss 11.0841 (11.5853)\tVB Loss 4.0546 (2.6713)\tF1 0.603 (0.706)\n",
            "Epoch: [56][900/1405]\tBatch Time 0.215 (0.222)\tData Load Time 0.008 (0.005)\tCE Loss 11.4713 (11.5922)\tVB Loss 2.6860 (2.6545)\tF1 0.736 (0.707)\n",
            "Epoch: [56][1000/1405]\tBatch Time 0.172 (0.221)\tData Load Time 0.004 (0.005)\tCE Loss 12.1735 (11.5905)\tVB Loss 2.4485 (2.6572)\tF1 0.705 (0.706)\n",
            "Epoch: [56][1100/1405]\tBatch Time 0.306 (0.220)\tData Load Time 0.004 (0.005)\tCE Loss 11.7504 (11.5844)\tVB Loss 3.2579 (2.6502)\tF1 0.409 (0.705)\n",
            "Epoch: [56][1200/1405]\tBatch Time 0.190 (0.220)\tData Load Time 0.004 (0.005)\tCE Loss 11.3254 (11.5858)\tVB Loss 1.2427 (2.6704)\tF1 0.745 (0.703)\n",
            "Epoch: [56][1300/1405]\tBatch Time 0.185 (0.219)\tData Load Time 0.006 (0.005)\tCE Loss 10.1398 (11.5812)\tVB Loss 1.3137 (2.6634)\tF1 0.860 (0.702)\n",
            "Epoch: [56][1400/1405]\tBatch Time 0.120 (0.219)\tData Load Time 0.004 (0.005)\tCE Loss 11.7150 (11.5742)\tVB Loss 1.5394 (2.6651)\tF1 0.758 (0.702)\n",
            "Validation: [0/325]\tBatch Time 0.340 (0.340)\tVB Loss 5.2591 (5.2591)\tF1 Score 0.639 (0.639)\t\n",
            "Validation: [100/325]\tBatch Time 0.125 (0.105)\tVB Loss 2.5038 (2.2239)\tF1 Score 0.713 (0.789)\t\n",
            "Validation: [200/325]\tBatch Time 0.177 (0.111)\tVB Loss 12.1783 (2.2364)\tF1 Score 0.603 (0.785)\t\n",
            "Validation: [300/325]\tBatch Time 0.100 (0.115)\tVB Loss 1.5621 (2.2511)\tF1 Score 0.924 (0.786)\t\n",
            "\n",
            " * LOSS - 2.230, F1 SCORE - 0.786\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000390\n",
            "\n",
            "Epoch: [57][0/1405]\tBatch Time 0.427 (0.427)\tData Load Time 0.211 (0.211)\tCE Loss 11.5156 (11.5156)\tVB Loss 1.2642 (1.2642)\tF1 0.690 (0.690)\n",
            "Epoch: [57][100/1405]\tBatch Time 0.182 (0.247)\tData Load Time 0.007 (0.007)\tCE Loss 11.8314 (11.5577)\tVB Loss 3.2173 (2.5944)\tF1 0.796 (0.694)\n",
            "Epoch: [57][200/1405]\tBatch Time 0.285 (0.237)\tData Load Time 0.006 (0.006)\tCE Loss 11.4295 (11.5843)\tVB Loss 2.7518 (2.6212)\tF1 0.613 (0.699)\n",
            "Epoch: [57][300/1405]\tBatch Time 0.215 (0.231)\tData Load Time 0.004 (0.006)\tCE Loss 11.3367 (11.5727)\tVB Loss 2.9021 (2.5917)\tF1 0.631 (0.697)\n",
            "Epoch: [57][400/1405]\tBatch Time 0.265 (0.229)\tData Load Time 0.004 (0.005)\tCE Loss 10.7740 (11.5722)\tVB Loss 4.6388 (2.6558)\tF1 0.613 (0.694)\n",
            "Epoch: [57][500/1405]\tBatch Time 0.150 (0.227)\tData Load Time 0.004 (0.005)\tCE Loss 10.7083 (11.5876)\tVB Loss 2.3346 (2.6152)\tF1 0.578 (0.695)\n",
            "Epoch: [57][600/1405]\tBatch Time 0.227 (0.227)\tData Load Time 0.005 (0.005)\tCE Loss 11.8754 (11.5837)\tVB Loss 2.6849 (2.6196)\tF1 0.715 (0.700)\n",
            "Epoch: [57][700/1405]\tBatch Time 0.249 (0.226)\tData Load Time 0.005 (0.005)\tCE Loss 11.7844 (11.5783)\tVB Loss 3.2823 (2.6023)\tF1 0.781 (0.703)\n",
            "Epoch: [57][800/1405]\tBatch Time 0.274 (0.225)\tData Load Time 0.005 (0.005)\tCE Loss 12.1964 (11.5679)\tVB Loss 1.9051 (2.5851)\tF1 0.777 (0.705)\n",
            "Epoch: [57][900/1405]\tBatch Time 0.300 (0.226)\tData Load Time 0.005 (0.005)\tCE Loss 11.7133 (11.5661)\tVB Loss 3.9807 (2.5851)\tF1 0.574 (0.705)\n",
            "Epoch: [57][1000/1405]\tBatch Time 0.239 (0.226)\tData Load Time 0.005 (0.005)\tCE Loss 11.1977 (11.5637)\tVB Loss 2.9533 (2.5971)\tF1 0.799 (0.706)\n",
            "Epoch: [57][1100/1405]\tBatch Time 0.209 (0.227)\tData Load Time 0.005 (0.005)\tCE Loss 11.2704 (11.5621)\tVB Loss 3.0687 (2.6178)\tF1 0.643 (0.706)\n",
            "Epoch: [57][1200/1405]\tBatch Time 0.267 (0.225)\tData Load Time 0.005 (0.005)\tCE Loss 11.6845 (11.5598)\tVB Loss 3.0606 (2.6154)\tF1 0.489 (0.706)\n",
            "Epoch: [57][1300/1405]\tBatch Time 0.234 (0.225)\tData Load Time 0.006 (0.005)\tCE Loss 12.2249 (11.5626)\tVB Loss 1.3934 (2.6188)\tF1 0.948 (0.706)\n",
            "Epoch: [57][1400/1405]\tBatch Time 0.256 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 12.1460 (11.5601)\tVB Loss 3.7739 (2.6256)\tF1 0.516 (0.707)\n",
            "Validation: [0/325]\tBatch Time 0.330 (0.330)\tVB Loss 1.4188 (1.4188)\tF1 Score 0.790 (0.790)\t\n",
            "Validation: [100/325]\tBatch Time 0.103 (0.107)\tVB Loss 1.7707 (2.3746)\tF1 Score 0.817 (0.781)\t\n",
            "Validation: [200/325]\tBatch Time 0.118 (0.109)\tVB Loss 4.4553 (2.2484)\tF1 Score 0.820 (0.784)\t\n",
            "Validation: [300/325]\tBatch Time 0.117 (0.107)\tVB Loss 2.1039 (2.2103)\tF1 Score 0.843 (0.787)\t\n",
            "\n",
            " * LOSS - 2.210, F1 SCORE - 0.786\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000385\n",
            "\n",
            "Epoch: [58][0/1405]\tBatch Time 0.431 (0.431)\tData Load Time 0.208 (0.208)\tCE Loss 11.1621 (11.1621)\tVB Loss 4.2647 (4.2647)\tF1 0.710 (0.710)\n",
            "Epoch: [58][100/1405]\tBatch Time 0.186 (0.238)\tData Load Time 0.004 (0.007)\tCE Loss 11.6894 (11.4544)\tVB Loss 1.7548 (2.6318)\tF1 0.818 (0.696)\n",
            "Epoch: [58][200/1405]\tBatch Time 0.210 (0.227)\tData Load Time 0.004 (0.006)\tCE Loss 11.9736 (11.4854)\tVB Loss 2.5077 (2.5456)\tF1 0.753 (0.712)\n",
            "Epoch: [58][300/1405]\tBatch Time 0.240 (0.225)\tData Load Time 0.004 (0.006)\tCE Loss 11.0026 (11.4857)\tVB Loss 2.8059 (2.5898)\tF1 0.578 (0.709)\n",
            "Epoch: [58][400/1405]\tBatch Time 0.229 (0.223)\tData Load Time 0.005 (0.006)\tCE Loss 10.7435 (11.5101)\tVB Loss 1.1525 (2.5493)\tF1 0.878 (0.710)\n",
            "Epoch: [58][500/1405]\tBatch Time 0.247 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 11.8289 (11.5217)\tVB Loss 2.9417 (2.5985)\tF1 0.754 (0.706)\n",
            "Epoch: [58][600/1405]\tBatch Time 0.206 (0.221)\tData Load Time 0.004 (0.005)\tCE Loss 11.5131 (11.5388)\tVB Loss 1.8318 (2.6108)\tF1 0.936 (0.706)\n",
            "Epoch: [58][700/1405]\tBatch Time 0.201 (0.221)\tData Load Time 0.004 (0.005)\tCE Loss 12.7115 (11.5444)\tVB Loss 1.4552 (2.6129)\tF1 0.880 (0.706)\n",
            "Epoch: [58][800/1405]\tBatch Time 0.232 (0.221)\tData Load Time 0.005 (0.005)\tCE Loss 10.8873 (11.5491)\tVB Loss 2.8879 (2.6311)\tF1 0.685 (0.706)\n",
            "Epoch: [58][900/1405]\tBatch Time 0.162 (0.220)\tData Load Time 0.005 (0.005)\tCE Loss 11.7002 (11.5498)\tVB Loss 2.6356 (2.6480)\tF1 0.750 (0.705)\n",
            "Epoch: [58][1000/1405]\tBatch Time 0.245 (0.220)\tData Load Time 0.004 (0.005)\tCE Loss 11.7827 (11.5488)\tVB Loss 1.5869 (2.6265)\tF1 0.836 (0.706)\n",
            "Epoch: [58][1100/1405]\tBatch Time 0.205 (0.221)\tData Load Time 0.005 (0.005)\tCE Loss 12.3439 (11.5534)\tVB Loss 1.9541 (2.6151)\tF1 0.634 (0.706)\n",
            "Epoch: [58][1200/1405]\tBatch Time 0.301 (0.220)\tData Load Time 0.005 (0.005)\tCE Loss 11.4143 (11.5505)\tVB Loss 3.4604 (2.6103)\tF1 0.809 (0.705)\n",
            "Epoch: [58][1300/1405]\tBatch Time 0.273 (0.220)\tData Load Time 0.005 (0.005)\tCE Loss 11.6195 (11.5470)\tVB Loss 4.2033 (2.6080)\tF1 0.709 (0.705)\n",
            "Epoch: [58][1400/1405]\tBatch Time 0.196 (0.220)\tData Load Time 0.005 (0.005)\tCE Loss 11.6252 (11.5447)\tVB Loss 2.2223 (2.5963)\tF1 0.625 (0.706)\n",
            "Validation: [0/325]\tBatch Time 0.293 (0.293)\tVB Loss 1.2062 (1.2062)\tF1 Score 0.915 (0.915)\t\n",
            "Validation: [100/325]\tBatch Time 0.105 (0.111)\tVB Loss 1.8395 (2.0684)\tF1 Score 0.806 (0.792)\t\n",
            "Validation: [200/325]\tBatch Time 0.088 (0.111)\tVB Loss 2.1017 (2.1548)\tF1 Score 0.790 (0.793)\t\n",
            "Validation: [300/325]\tBatch Time 0.092 (0.111)\tVB Loss 2.5298 (2.2311)\tF1 Score 0.612 (0.791)\t\n",
            "\n",
            " * LOSS - 2.220, F1 SCORE - 0.791\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000380\n",
            "\n",
            "Epoch: [59][0/1405]\tBatch Time 0.546 (0.546)\tData Load Time 0.222 (0.222)\tCE Loss 11.8081 (11.8081)\tVB Loss 2.1892 (2.1892)\tF1 0.623 (0.623)\n",
            "Epoch: [59][100/1405]\tBatch Time 0.226 (0.261)\tData Load Time 0.005 (0.008)\tCE Loss 11.4048 (11.4991)\tVB Loss 1.9723 (2.5031)\tF1 0.886 (0.714)\n",
            "Epoch: [59][200/1405]\tBatch Time 0.166 (0.245)\tData Load Time 0.005 (0.006)\tCE Loss 12.0734 (11.5468)\tVB Loss 1.4409 (2.5251)\tF1 0.662 (0.713)\n",
            "Epoch: [59][300/1405]\tBatch Time 0.242 (0.242)\tData Load Time 0.005 (0.006)\tCE Loss 12.0659 (11.5368)\tVB Loss 2.1105 (2.5278)\tF1 0.736 (0.713)\n",
            "Epoch: [59][400/1405]\tBatch Time 0.217 (0.238)\tData Load Time 0.005 (0.006)\tCE Loss 11.4084 (11.5297)\tVB Loss 0.9443 (2.5474)\tF1 0.708 (0.717)\n",
            "Epoch: [59][500/1405]\tBatch Time 0.284 (0.233)\tData Load Time 0.005 (0.006)\tCE Loss 11.6527 (11.5189)\tVB Loss 1.7464 (2.5546)\tF1 0.884 (0.716)\n",
            "Epoch: [59][600/1405]\tBatch Time 0.236 (0.232)\tData Load Time 0.005 (0.005)\tCE Loss 12.0521 (11.5324)\tVB Loss 3.1983 (2.5599)\tF1 0.729 (0.720)\n",
            "Epoch: [59][700/1405]\tBatch Time 0.153 (0.231)\tData Load Time 0.004 (0.005)\tCE Loss 11.7998 (11.5382)\tVB Loss 1.2440 (2.5728)\tF1 0.908 (0.717)\n",
            "Epoch: [59][800/1405]\tBatch Time 0.281 (0.230)\tData Load Time 0.005 (0.005)\tCE Loss 11.7513 (11.5439)\tVB Loss 3.7146 (2.5702)\tF1 0.605 (0.715)\n",
            "Epoch: [59][900/1405]\tBatch Time 0.241 (0.229)\tData Load Time 0.004 (0.005)\tCE Loss 12.1229 (11.5516)\tVB Loss 1.7679 (2.5871)\tF1 0.925 (0.714)\n",
            "Epoch: [59][1000/1405]\tBatch Time 0.237 (0.228)\tData Load Time 0.004 (0.005)\tCE Loss 10.4968 (11.5467)\tVB Loss 4.2857 (2.5850)\tF1 0.388 (0.714)\n",
            "Epoch: [59][1100/1405]\tBatch Time 0.219 (0.227)\tData Load Time 0.004 (0.005)\tCE Loss 12.4159 (11.5413)\tVB Loss 3.9773 (2.5699)\tF1 0.518 (0.714)\n",
            "Epoch: [59][1200/1405]\tBatch Time 0.254 (0.226)\tData Load Time 0.005 (0.005)\tCE Loss 10.7518 (11.5377)\tVB Loss 2.8462 (2.5572)\tF1 0.785 (0.714)\n",
            "Epoch: [59][1300/1405]\tBatch Time 0.302 (0.226)\tData Load Time 0.005 (0.005)\tCE Loss 11.9678 (11.5338)\tVB Loss 3.2943 (2.5545)\tF1 0.550 (0.715)\n",
            "Epoch: [59][1400/1405]\tBatch Time 0.254 (0.225)\tData Load Time 0.006 (0.005)\tCE Loss 12.1705 (11.5347)\tVB Loss 3.3203 (2.5625)\tF1 0.614 (0.716)\n",
            "Validation: [0/325]\tBatch Time 0.270 (0.270)\tVB Loss 2.1465 (2.1465)\tF1 Score 0.803 (0.803)\t\n",
            "Validation: [100/325]\tBatch Time 0.154 (0.118)\tVB Loss 2.2433 (2.2957)\tF1 Score 0.674 (0.792)\t\n",
            "Validation: [200/325]\tBatch Time 0.120 (0.122)\tVB Loss 2.0174 (2.2839)\tF1 Score 0.839 (0.787)\t\n",
            "Validation: [300/325]\tBatch Time 0.159 (0.122)\tVB Loss 2.9428 (2.2403)\tF1 Score 0.766 (0.788)\t\n",
            "\n",
            " * LOSS - 2.246, F1 SCORE - 0.787\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000375\n",
            "\n",
            "Epoch: [60][0/1405]\tBatch Time 0.419 (0.419)\tData Load Time 0.195 (0.195)\tCE Loss 11.4583 (11.4583)\tVB Loss 1.3156 (1.3156)\tF1 0.709 (0.709)\n",
            "Epoch: [60][100/1405]\tBatch Time 0.210 (0.247)\tData Load Time 0.006 (0.007)\tCE Loss 11.1777 (11.5687)\tVB Loss 1.7695 (2.5285)\tF1 0.785 (0.707)\n",
            "Epoch: [60][200/1405]\tBatch Time 0.227 (0.232)\tData Load Time 0.004 (0.006)\tCE Loss 11.6652 (11.5363)\tVB Loss 3.7486 (2.5285)\tF1 0.663 (0.712)\n",
            "Epoch: [60][300/1405]\tBatch Time 0.230 (0.228)\tData Load Time 0.004 (0.006)\tCE Loss 11.7157 (11.5126)\tVB Loss 2.7322 (2.5267)\tF1 0.812 (0.711)\n",
            "Epoch: [60][400/1405]\tBatch Time 0.243 (0.227)\tData Load Time 0.005 (0.005)\tCE Loss 11.9210 (11.5081)\tVB Loss 1.7849 (2.5447)\tF1 0.830 (0.713)\n",
            "Epoch: [60][500/1405]\tBatch Time 0.206 (0.225)\tData Load Time 0.004 (0.005)\tCE Loss 12.1736 (11.5121)\tVB Loss 4.8195 (2.5377)\tF1 0.641 (0.713)\n",
            "Epoch: [60][600/1405]\tBatch Time 0.214 (0.223)\tData Load Time 0.006 (0.005)\tCE Loss 9.6473 (11.4914)\tVB Loss 3.6517 (2.4882)\tF1 0.795 (0.719)\n",
            "Epoch: [60][700/1405]\tBatch Time 0.250 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 12.1838 (11.5002)\tVB Loss 3.4846 (2.5422)\tF1 0.853 (0.718)\n",
            "Epoch: [60][800/1405]\tBatch Time 0.246 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 11.4927 (11.5068)\tVB Loss 3.5384 (2.5233)\tF1 0.670 (0.718)\n",
            "Epoch: [60][900/1405]\tBatch Time 0.234 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 10.7723 (11.5199)\tVB Loss 1.9276 (2.5206)\tF1 0.822 (0.717)\n",
            "Epoch: [60][1000/1405]\tBatch Time 0.265 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 11.3525 (11.5264)\tVB Loss 2.1307 (2.5373)\tF1 0.704 (0.717)\n",
            "Epoch: [60][1100/1405]\tBatch Time 0.123 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 11.0840 (11.5219)\tVB Loss 0.8646 (2.5558)\tF1 0.895 (0.717)\n",
            "Epoch: [60][1200/1405]\tBatch Time 0.286 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 12.1258 (11.5219)\tVB Loss 4.1318 (2.5615)\tF1 0.612 (0.716)\n",
            "Epoch: [60][1300/1405]\tBatch Time 0.203 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 11.0410 (11.5223)\tVB Loss 2.1484 (2.5393)\tF1 0.846 (0.718)\n",
            "Epoch: [60][1400/1405]\tBatch Time 0.189 (0.221)\tData Load Time 0.005 (0.005)\tCE Loss 11.3990 (11.5190)\tVB Loss 3.1172 (2.5255)\tF1 0.760 (0.718)\n",
            "Validation: [0/325]\tBatch Time 0.325 (0.325)\tVB Loss 1.8850 (1.8850)\tF1 Score 0.878 (0.878)\t\n",
            "Validation: [100/325]\tBatch Time 0.118 (0.124)\tVB Loss 2.8578 (2.1514)\tF1 Score 0.895 (0.786)\t\n",
            "Validation: [200/325]\tBatch Time 0.121 (0.125)\tVB Loss 2.0546 (2.1999)\tF1 Score 0.649 (0.793)\t\n",
            "Validation: [300/325]\tBatch Time 0.107 (0.123)\tVB Loss 2.4716 (2.1901)\tF1 Score 0.848 (0.793)\t\n",
            "\n",
            " * LOSS - 2.187, F1 SCORE - 0.792\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000370\n",
            "\n",
            "Epoch: [61][0/1405]\tBatch Time 0.538 (0.538)\tData Load Time 0.195 (0.195)\tCE Loss 10.9146 (10.9146)\tVB Loss 3.5388 (3.5388)\tF1 0.644 (0.644)\n",
            "Epoch: [61][100/1405]\tBatch Time 0.338 (0.244)\tData Load Time 0.005 (0.007)\tCE Loss 10.9290 (11.4646)\tVB Loss 2.3477 (2.4555)\tF1 0.723 (0.744)\n",
            "Epoch: [61][200/1405]\tBatch Time 0.265 (0.234)\tData Load Time 0.004 (0.006)\tCE Loss 11.4589 (11.5206)\tVB Loss 3.1079 (2.4986)\tF1 0.652 (0.735)\n",
            "Epoch: [61][300/1405]\tBatch Time 0.253 (0.228)\tData Load Time 0.006 (0.006)\tCE Loss 12.2609 (11.5204)\tVB Loss 6.8582 (2.4832)\tF1 0.686 (0.728)\n",
            "Epoch: [61][400/1405]\tBatch Time 0.213 (0.225)\tData Load Time 0.004 (0.005)\tCE Loss 12.6101 (11.5151)\tVB Loss 1.7732 (2.4622)\tF1 0.748 (0.724)\n",
            "Epoch: [61][500/1405]\tBatch Time 0.254 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 11.4256 (11.5013)\tVB Loss 4.0298 (2.4405)\tF1 0.729 (0.726)\n",
            "Epoch: [61][600/1405]\tBatch Time 0.158 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 11.4804 (11.4888)\tVB Loss 1.2982 (2.4528)\tF1 0.671 (0.724)\n",
            "Epoch: [61][700/1405]\tBatch Time 0.125 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 11.0911 (11.4932)\tVB Loss 1.2075 (2.4464)\tF1 0.570 (0.722)\n",
            "Epoch: [61][800/1405]\tBatch Time 0.191 (0.224)\tData Load Time 0.004 (0.005)\tCE Loss 11.7539 (11.5113)\tVB Loss 1.7913 (2.4493)\tF1 0.754 (0.720)\n",
            "Epoch: [61][900/1405]\tBatch Time 0.223 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 10.9275 (11.5095)\tVB Loss 2.7525 (2.4633)\tF1 0.780 (0.718)\n",
            "Epoch: [61][1000/1405]\tBatch Time 0.208 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 11.8912 (11.5071)\tVB Loss 1.4094 (2.4742)\tF1 0.698 (0.719)\n",
            "Epoch: [61][1100/1405]\tBatch Time 0.221 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 11.5840 (11.5061)\tVB Loss 1.7956 (2.4769)\tF1 0.883 (0.721)\n",
            "Epoch: [61][1200/1405]\tBatch Time 0.220 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 11.7958 (11.5032)\tVB Loss 2.0303 (2.4806)\tF1 0.589 (0.721)\n",
            "Epoch: [61][1300/1405]\tBatch Time 0.231 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 11.5932 (11.5019)\tVB Loss 2.2876 (2.4939)\tF1 0.586 (0.719)\n",
            "Epoch: [61][1400/1405]\tBatch Time 0.273 (0.222)\tData Load Time 0.006 (0.005)\tCE Loss 12.0795 (11.5062)\tVB Loss 3.5538 (2.5022)\tF1 0.665 (0.718)\n",
            "Validation: [0/325]\tBatch Time 0.302 (0.302)\tVB Loss 0.6839 (0.6839)\tF1 Score 0.898 (0.898)\t\n",
            "Validation: [100/325]\tBatch Time 0.110 (0.122)\tVB Loss 3.0526 (2.2627)\tF1 Score 0.818 (0.772)\t\n",
            "Validation: [200/325]\tBatch Time 0.132 (0.122)\tVB Loss 0.7626 (2.1400)\tF1 Score 0.915 (0.783)\t\n",
            "Validation: [300/325]\tBatch Time 0.202 (0.121)\tVB Loss 1.8653 (2.1297)\tF1 Score 0.681 (0.791)\t\n",
            "\n",
            " * LOSS - 2.153, F1 SCORE - 0.794\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000366\n",
            "\n",
            "Epoch: [62][0/1405]\tBatch Time 0.483 (0.483)\tData Load Time 0.197 (0.197)\tCE Loss 11.2108 (11.2108)\tVB Loss 4.6232 (4.6232)\tF1 0.593 (0.593)\n",
            "Epoch: [62][100/1405]\tBatch Time 0.283 (0.253)\tData Load Time 0.004 (0.007)\tCE Loss 11.6030 (11.5062)\tVB Loss 4.6588 (2.6362)\tF1 0.728 (0.720)\n",
            "Epoch: [62][200/1405]\tBatch Time 0.236 (0.240)\tData Load Time 0.006 (0.006)\tCE Loss 10.8184 (11.4962)\tVB Loss 2.5808 (2.4974)\tF1 0.514 (0.720)\n",
            "Epoch: [62][300/1405]\tBatch Time 0.224 (0.237)\tData Load Time 0.006 (0.006)\tCE Loss 12.2361 (11.4837)\tVB Loss 2.6721 (2.4996)\tF1 0.694 (0.721)\n",
            "Epoch: [62][400/1405]\tBatch Time 0.283 (0.234)\tData Load Time 0.004 (0.006)\tCE Loss 11.7314 (11.5008)\tVB Loss 3.5770 (2.4734)\tF1 0.701 (0.721)\n",
            "Epoch: [62][500/1405]\tBatch Time 0.259 (0.232)\tData Load Time 0.005 (0.005)\tCE Loss 11.6592 (11.4903)\tVB Loss 3.2191 (2.4697)\tF1 0.677 (0.722)\n",
            "Epoch: [62][600/1405]\tBatch Time 0.287 (0.229)\tData Load Time 0.005 (0.005)\tCE Loss 11.3479 (11.4976)\tVB Loss 2.3053 (2.4866)\tF1 0.595 (0.722)\n",
            "Epoch: [62][700/1405]\tBatch Time 0.217 (0.228)\tData Load Time 0.004 (0.005)\tCE Loss 10.5504 (11.4961)\tVB Loss 2.2009 (2.4871)\tF1 0.681 (0.722)\n",
            "Epoch: [62][800/1405]\tBatch Time 0.191 (0.227)\tData Load Time 0.005 (0.005)\tCE Loss 11.5126 (11.4917)\tVB Loss 2.1368 (2.4756)\tF1 0.716 (0.722)\n",
            "Epoch: [62][900/1405]\tBatch Time 0.189 (0.227)\tData Load Time 0.004 (0.005)\tCE Loss 12.7333 (11.4886)\tVB Loss 2.3844 (2.4829)\tF1 0.723 (0.723)\n",
            "Epoch: [62][1000/1405]\tBatch Time 0.199 (0.227)\tData Load Time 0.004 (0.005)\tCE Loss 11.2338 (11.4846)\tVB Loss 1.9612 (2.4681)\tF1 0.874 (0.724)\n",
            "Epoch: [62][1100/1405]\tBatch Time 0.186 (0.227)\tData Load Time 0.004 (0.005)\tCE Loss 11.2201 (11.4877)\tVB Loss 1.7004 (2.4944)\tF1 0.899 (0.723)\n",
            "Epoch: [62][1200/1405]\tBatch Time 0.185 (0.227)\tData Load Time 0.004 (0.005)\tCE Loss 9.9444 (11.4901)\tVB Loss 1.9682 (2.4872)\tF1 0.718 (0.724)\n",
            "Epoch: [62][1300/1405]\tBatch Time 0.170 (0.227)\tData Load Time 0.005 (0.005)\tCE Loss 11.6041 (11.4890)\tVB Loss 2.3146 (2.4819)\tF1 0.819 (0.725)\n",
            "Epoch: [62][1400/1405]\tBatch Time 0.176 (0.227)\tData Load Time 0.005 (0.005)\tCE Loss 11.9012 (11.4884)\tVB Loss 1.1803 (2.4834)\tF1 0.712 (0.725)\n",
            "Validation: [0/325]\tBatch Time 0.310 (0.310)\tVB Loss 3.6142 (3.6142)\tF1 Score 0.744 (0.744)\t\n",
            "Validation: [100/325]\tBatch Time 0.130 (0.128)\tVB Loss 1.3307 (2.1575)\tF1 Score 0.749 (0.794)\t\n",
            "Validation: [200/325]\tBatch Time 0.110 (0.126)\tVB Loss 2.6039 (2.2242)\tF1 Score 0.863 (0.799)\t\n",
            "Validation: [300/325]\tBatch Time 0.122 (0.123)\tVB Loss 2.2031 (2.1270)\tF1 Score 0.865 (0.801)\t\n",
            "\n",
            " * LOSS - 2.156, F1 SCORE - 0.798\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000361\n",
            "\n",
            "Epoch: [63][0/1405]\tBatch Time 0.425 (0.425)\tData Load Time 0.197 (0.197)\tCE Loss 10.8452 (10.8452)\tVB Loss 1.7948 (1.7948)\tF1 0.833 (0.833)\n",
            "Epoch: [63][100/1405]\tBatch Time 0.235 (0.243)\tData Load Time 0.005 (0.007)\tCE Loss 10.9714 (11.4117)\tVB Loss 1.7605 (2.3718)\tF1 0.740 (0.719)\n",
            "Epoch: [63][200/1405]\tBatch Time 0.247 (0.237)\tData Load Time 0.005 (0.006)\tCE Loss 10.6853 (11.4243)\tVB Loss 1.8002 (2.3833)\tF1 0.660 (0.719)\n",
            "Epoch: [63][300/1405]\tBatch Time 0.234 (0.234)\tData Load Time 0.004 (0.006)\tCE Loss 11.8733 (11.4642)\tVB Loss 1.9776 (2.3693)\tF1 0.766 (0.722)\n",
            "Epoch: [63][400/1405]\tBatch Time 0.170 (0.230)\tData Load Time 0.005 (0.005)\tCE Loss 11.3080 (11.4605)\tVB Loss 2.4034 (2.3716)\tF1 0.759 (0.726)\n",
            "Epoch: [63][500/1405]\tBatch Time 0.261 (0.226)\tData Load Time 0.005 (0.005)\tCE Loss 11.3983 (11.4683)\tVB Loss 1.3190 (2.4071)\tF1 0.829 (0.725)\n",
            "Epoch: [63][600/1405]\tBatch Time 0.188 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 12.3214 (11.4898)\tVB Loss 4.3441 (2.4310)\tF1 0.562 (0.723)\n",
            "Epoch: [63][700/1405]\tBatch Time 0.229 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 11.2490 (11.4969)\tVB Loss 1.0152 (2.4513)\tF1 0.601 (0.721)\n",
            "Epoch: [63][800/1405]\tBatch Time 0.219 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 12.2205 (11.4906)\tVB Loss 3.0951 (2.4773)\tF1 0.690 (0.717)\n",
            "Epoch: [63][900/1405]\tBatch Time 0.179 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 10.5892 (11.4963)\tVB Loss 1.3440 (2.4669)\tF1 0.735 (0.720)\n",
            "Epoch: [63][1000/1405]\tBatch Time 0.193 (0.220)\tData Load Time 0.004 (0.005)\tCE Loss 10.7598 (11.4882)\tVB Loss 2.0799 (2.4524)\tF1 0.673 (0.723)\n",
            "Epoch: [63][1100/1405]\tBatch Time 0.211 (0.220)\tData Load Time 0.004 (0.005)\tCE Loss 11.8269 (11.4813)\tVB Loss 3.0758 (2.4464)\tF1 0.711 (0.723)\n",
            "Epoch: [63][1200/1405]\tBatch Time 0.286 (0.220)\tData Load Time 0.004 (0.005)\tCE Loss 11.3706 (11.4793)\tVB Loss 0.8316 (2.4629)\tF1 0.683 (0.722)\n",
            "Epoch: [63][1300/1405]\tBatch Time 0.233 (0.220)\tData Load Time 0.004 (0.005)\tCE Loss 11.7980 (11.4781)\tVB Loss 1.4979 (2.4797)\tF1 0.871 (0.723)\n",
            "Epoch: [63][1400/1405]\tBatch Time 0.181 (0.220)\tData Load Time 0.004 (0.005)\tCE Loss 11.0730 (11.4760)\tVB Loss 1.8814 (2.4708)\tF1 0.778 (0.722)\n",
            "Validation: [0/325]\tBatch Time 0.359 (0.359)\tVB Loss 7.7375 (7.7375)\tF1 Score 0.709 (0.709)\t\n",
            "Validation: [100/325]\tBatch Time 0.112 (0.126)\tVB Loss 3.6132 (1.9858)\tF1 Score 0.518 (0.804)\t\n",
            "Validation: [200/325]\tBatch Time 0.077 (0.125)\tVB Loss 0.6029 (2.0138)\tF1 Score 0.878 (0.797)\t\n",
            "Validation: [300/325]\tBatch Time 0.136 (0.124)\tVB Loss 2.6093 (2.0660)\tF1 Score 0.764 (0.795)\t\n",
            "\n",
            " * LOSS - 2.104, F1 SCORE - 0.797\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000357\n",
            "\n",
            "Epoch: [64][0/1405]\tBatch Time 0.532 (0.532)\tData Load Time 0.223 (0.223)\tCE Loss 11.6501 (11.6501)\tVB Loss 0.9485 (0.9485)\tF1 0.853 (0.853)\n",
            "Epoch: [64][100/1405]\tBatch Time 0.240 (0.259)\tData Load Time 0.005 (0.007)\tCE Loss 11.9822 (11.4794)\tVB Loss 1.6756 (2.3838)\tF1 0.845 (0.740)\n",
            "Epoch: [64][200/1405]\tBatch Time 0.199 (0.242)\tData Load Time 0.005 (0.006)\tCE Loss 11.3000 (11.4766)\tVB Loss 1.2952 (2.3721)\tF1 0.774 (0.737)\n",
            "Epoch: [64][300/1405]\tBatch Time 0.239 (0.235)\tData Load Time 0.005 (0.006)\tCE Loss 12.1815 (11.4747)\tVB Loss 1.3483 (2.3167)\tF1 0.723 (0.728)\n",
            "Epoch: [64][400/1405]\tBatch Time 0.211 (0.231)\tData Load Time 0.004 (0.005)\tCE Loss 10.2226 (11.4691)\tVB Loss 0.8980 (2.3718)\tF1 0.885 (0.727)\n",
            "Epoch: [64][500/1405]\tBatch Time 0.206 (0.229)\tData Load Time 0.006 (0.005)\tCE Loss 11.6416 (11.4816)\tVB Loss 1.3827 (2.3792)\tF1 0.922 (0.734)\n",
            "Epoch: [64][600/1405]\tBatch Time 0.225 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 10.4103 (11.4693)\tVB Loss 1.0222 (2.3651)\tF1 0.846 (0.732)\n",
            "Epoch: [64][700/1405]\tBatch Time 0.216 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 11.5028 (11.4711)\tVB Loss 3.3472 (2.3697)\tF1 0.519 (0.733)\n",
            "Epoch: [64][800/1405]\tBatch Time 0.184 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 11.0003 (11.4730)\tVB Loss 2.4488 (2.3869)\tF1 0.722 (0.733)\n",
            "Epoch: [64][900/1405]\tBatch Time 0.225 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 11.4467 (11.4767)\tVB Loss 3.3466 (2.3758)\tF1 0.566 (0.732)\n",
            "Epoch: [64][1000/1405]\tBatch Time 0.232 (0.221)\tData Load Time 0.005 (0.005)\tCE Loss 11.2736 (11.4658)\tVB Loss 2.2008 (2.3818)\tF1 0.697 (0.730)\n",
            "Epoch: [64][1100/1405]\tBatch Time 0.249 (0.221)\tData Load Time 0.005 (0.005)\tCE Loss 11.8217 (11.4639)\tVB Loss 0.9012 (2.3881)\tF1 0.808 (0.729)\n",
            "Epoch: [64][1200/1405]\tBatch Time 0.314 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 11.7711 (11.4665)\tVB Loss 3.0785 (2.3906)\tF1 0.677 (0.730)\n",
            "Epoch: [64][1300/1405]\tBatch Time 0.219 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 11.4358 (11.4680)\tVB Loss 3.4576 (2.4110)\tF1 0.686 (0.729)\n",
            "Epoch: [64][1400/1405]\tBatch Time 0.256 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 11.4027 (11.4646)\tVB Loss 3.6076 (2.4056)\tF1 0.699 (0.728)\n",
            "Validation: [0/325]\tBatch Time 0.306 (0.306)\tVB Loss 1.7740 (1.7740)\tF1 Score 0.892 (0.892)\t\n",
            "Validation: [100/325]\tBatch Time 0.095 (0.111)\tVB Loss 1.6273 (2.0821)\tF1 Score 0.840 (0.800)\t\n",
            "Validation: [200/325]\tBatch Time 0.105 (0.108)\tVB Loss 4.0817 (2.1153)\tF1 Score 0.764 (0.803)\t\n",
            "Validation: [300/325]\tBatch Time 0.154 (0.110)\tVB Loss 5.6052 (2.0819)\tF1 Score 0.575 (0.802)\t\n",
            "\n",
            " * LOSS - 2.097, F1 SCORE - 0.800\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000353\n",
            "\n",
            "Epoch: [65][0/1405]\tBatch Time 0.435 (0.435)\tData Load Time 0.214 (0.214)\tCE Loss 12.2931 (12.2931)\tVB Loss 2.5526 (2.5526)\tF1 0.526 (0.526)\n",
            "Epoch: [65][100/1405]\tBatch Time 0.279 (0.239)\tData Load Time 0.005 (0.007)\tCE Loss 11.5340 (11.4207)\tVB Loss 3.3001 (2.2896)\tF1 0.905 (0.752)\n",
            "Epoch: [65][200/1405]\tBatch Time 0.222 (0.235)\tData Load Time 0.004 (0.006)\tCE Loss 11.9278 (11.4628)\tVB Loss 1.6269 (2.3518)\tF1 0.828 (0.739)\n",
            "Epoch: [65][300/1405]\tBatch Time 0.181 (0.233)\tData Load Time 0.004 (0.006)\tCE Loss 11.4476 (11.4867)\tVB Loss 0.5680 (2.2822)\tF1 0.936 (0.744)\n",
            "Epoch: [65][400/1405]\tBatch Time 0.180 (0.232)\tData Load Time 0.005 (0.006)\tCE Loss 10.2682 (11.4846)\tVB Loss 1.1672 (2.3318)\tF1 0.889 (0.734)\n",
            "Epoch: [65][500/1405]\tBatch Time 0.194 (0.231)\tData Load Time 0.004 (0.006)\tCE Loss 10.6130 (11.4850)\tVB Loss 2.0206 (2.3266)\tF1 0.666 (0.736)\n",
            "Epoch: [65][600/1405]\tBatch Time 0.228 (0.230)\tData Load Time 0.004 (0.005)\tCE Loss 11.1554 (11.4761)\tVB Loss 3.9976 (2.3212)\tF1 0.747 (0.740)\n",
            "Epoch: [65][700/1405]\tBatch Time 0.165 (0.229)\tData Load Time 0.006 (0.005)\tCE Loss 11.5467 (11.4704)\tVB Loss 0.7339 (2.3184)\tF1 0.912 (0.741)\n",
            "Epoch: [65][800/1405]\tBatch Time 0.199 (0.227)\tData Load Time 0.004 (0.005)\tCE Loss 11.9105 (11.4663)\tVB Loss 1.0079 (2.3163)\tF1 0.904 (0.741)\n",
            "Epoch: [65][900/1405]\tBatch Time 0.171 (0.226)\tData Load Time 0.004 (0.005)\tCE Loss 11.1743 (11.4619)\tVB Loss 1.7950 (2.3411)\tF1 0.692 (0.736)\n",
            "Epoch: [65][1000/1405]\tBatch Time 0.256 (0.226)\tData Load Time 0.005 (0.005)\tCE Loss 11.4883 (11.4544)\tVB Loss 3.0053 (2.3653)\tF1 0.676 (0.734)\n",
            "Epoch: [65][1100/1405]\tBatch Time 0.229 (0.226)\tData Load Time 0.005 (0.005)\tCE Loss 10.9028 (11.4437)\tVB Loss 3.8693 (2.3748)\tF1 0.489 (0.736)\n",
            "Epoch: [65][1200/1405]\tBatch Time 0.258 (0.226)\tData Load Time 0.006 (0.005)\tCE Loss 11.2758 (11.4404)\tVB Loss 2.2547 (2.3923)\tF1 0.765 (0.735)\n",
            "Epoch: [65][1300/1405]\tBatch Time 0.219 (0.226)\tData Load Time 0.005 (0.005)\tCE Loss 11.0553 (11.4426)\tVB Loss 1.6749 (2.4024)\tF1 0.744 (0.732)\n",
            "Epoch: [65][1400/1405]\tBatch Time 0.218 (0.225)\tData Load Time 0.004 (0.005)\tCE Loss 11.2527 (11.4474)\tVB Loss 1.3211 (2.3927)\tF1 0.862 (0.732)\n",
            "Validation: [0/325]\tBatch Time 0.287 (0.287)\tVB Loss 4.4900 (4.4900)\tF1 Score 0.828 (0.828)\t\n",
            "Validation: [100/325]\tBatch Time 0.136 (0.124)\tVB Loss 0.9954 (2.0637)\tF1 Score 0.945 (0.787)\t\n",
            "Validation: [200/325]\tBatch Time 0.122 (0.124)\tVB Loss 3.1979 (2.1495)\tF1 Score 0.717 (0.793)\t\n",
            "Validation: [300/325]\tBatch Time 0.136 (0.124)\tVB Loss 1.1653 (2.1627)\tF1 Score 0.882 (0.796)\t\n",
            "\n",
            " * LOSS - 2.132, F1 SCORE - 0.797\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000349\n",
            "\n",
            "Epoch: [66][0/1405]\tBatch Time 0.445 (0.445)\tData Load Time 0.199 (0.199)\tCE Loss 10.6227 (10.6227)\tVB Loss 1.3299 (1.3299)\tF1 0.896 (0.896)\n",
            "Epoch: [66][100/1405]\tBatch Time 0.248 (0.255)\tData Load Time 0.006 (0.008)\tCE Loss 11.5506 (11.4939)\tVB Loss 2.0334 (2.3745)\tF1 0.786 (0.722)\n",
            "Epoch: [66][200/1405]\tBatch Time 0.238 (0.244)\tData Load Time 0.004 (0.006)\tCE Loss 10.6071 (11.4648)\tVB Loss 3.3973 (2.3936)\tF1 0.826 (0.730)\n",
            "Epoch: [66][300/1405]\tBatch Time 0.230 (0.237)\tData Load Time 0.004 (0.006)\tCE Loss 11.5345 (11.4536)\tVB Loss 1.7965 (2.4357)\tF1 0.816 (0.735)\n",
            "Epoch: [66][400/1405]\tBatch Time 0.174 (0.234)\tData Load Time 0.005 (0.006)\tCE Loss 10.6040 (11.4573)\tVB Loss 1.1964 (2.4885)\tF1 0.606 (0.729)\n",
            "Epoch: [66][500/1405]\tBatch Time 0.234 (0.233)\tData Load Time 0.004 (0.005)\tCE Loss 10.2348 (11.4372)\tVB Loss 0.9703 (2.5047)\tF1 0.838 (0.728)\n",
            "Epoch: [66][600/1405]\tBatch Time 0.142 (0.229)\tData Load Time 0.005 (0.005)\tCE Loss 10.1555 (11.4271)\tVB Loss 0.9697 (2.4574)\tF1 0.935 (0.729)\n",
            "Epoch: [66][700/1405]\tBatch Time 0.199 (0.228)\tData Load Time 0.004 (0.005)\tCE Loss 11.6096 (11.4277)\tVB Loss 0.7119 (2.4472)\tF1 0.908 (0.729)\n",
            "Epoch: [66][800/1405]\tBatch Time 0.171 (0.226)\tData Load Time 0.004 (0.005)\tCE Loss 11.9750 (11.4201)\tVB Loss 3.3418 (2.4167)\tF1 0.464 (0.728)\n",
            "Epoch: [66][900/1405]\tBatch Time 0.140 (0.225)\tData Load Time 0.005 (0.005)\tCE Loss 11.6875 (11.4200)\tVB Loss 1.7101 (2.4015)\tF1 0.496 (0.730)\n",
            "Epoch: [66][1000/1405]\tBatch Time 0.223 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 10.9113 (11.4285)\tVB Loss 2.2109 (2.3882)\tF1 0.856 (0.729)\n",
            "Epoch: [66][1100/1405]\tBatch Time 0.216 (0.224)\tData Load Time 0.004 (0.005)\tCE Loss 11.3583 (11.4283)\tVB Loss 3.6061 (2.3996)\tF1 0.553 (0.729)\n",
            "Epoch: [66][1200/1405]\tBatch Time 0.196 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 11.3998 (11.4341)\tVB Loss 3.0962 (2.3868)\tF1 0.557 (0.729)\n",
            "Epoch: [66][1300/1405]\tBatch Time 0.291 (0.225)\tData Load Time 0.005 (0.005)\tCE Loss 11.6180 (11.4383)\tVB Loss 2.0500 (2.3836)\tF1 0.700 (0.729)\n",
            "Epoch: [66][1400/1405]\tBatch Time 0.209 (0.225)\tData Load Time 0.005 (0.005)\tCE Loss 11.9413 (11.4316)\tVB Loss 1.3865 (2.3825)\tF1 0.701 (0.729)\n",
            "Validation: [0/325]\tBatch Time 0.320 (0.320)\tVB Loss 3.7210 (3.7210)\tF1 Score 0.776 (0.776)\t\n",
            "Validation: [100/325]\tBatch Time 0.116 (0.122)\tVB Loss 1.8680 (2.0466)\tF1 Score 0.665 (0.808)\t\n",
            "Validation: [200/325]\tBatch Time 0.118 (0.125)\tVB Loss 1.9746 (2.1016)\tF1 Score 0.878 (0.809)\t\n",
            "Validation: [300/325]\tBatch Time 0.080 (0.124)\tVB Loss 0.6555 (2.0613)\tF1 Score 0.822 (0.813)\t\n",
            "\n",
            " * LOSS - 2.057, F1 SCORE - 0.811\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000345\n",
            "\n",
            "Epoch: [67][0/1405]\tBatch Time 0.402 (0.402)\tData Load Time 0.209 (0.209)\tCE Loss 10.3573 (10.3573)\tVB Loss 1.0381 (1.0381)\tF1 0.589 (0.589)\n",
            "Epoch: [67][100/1405]\tBatch Time 0.220 (0.248)\tData Load Time 0.004 (0.007)\tCE Loss 12.4027 (11.4543)\tVB Loss 5.6594 (2.6187)\tF1 0.786 (0.733)\n",
            "Epoch: [67][200/1405]\tBatch Time 0.182 (0.232)\tData Load Time 0.005 (0.006)\tCE Loss 11.2922 (11.4138)\tVB Loss 0.6188 (2.4818)\tF1 0.869 (0.734)\n",
            "Epoch: [67][300/1405]\tBatch Time 0.184 (0.229)\tData Load Time 0.005 (0.006)\tCE Loss 12.3227 (11.4477)\tVB Loss 1.6050 (2.4430)\tF1 0.818 (0.731)\n",
            "Epoch: [67][400/1405]\tBatch Time 0.234 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 12.2201 (11.4201)\tVB Loss 1.3844 (2.4276)\tF1 0.863 (0.733)\n",
            "Epoch: [67][500/1405]\tBatch Time 0.205 (0.221)\tData Load Time 0.004 (0.005)\tCE Loss 11.2671 (11.4178)\tVB Loss 3.0541 (2.4130)\tF1 0.656 (0.736)\n",
            "Epoch: [67][600/1405]\tBatch Time 0.249 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 11.0368 (11.4186)\tVB Loss 2.4723 (2.3694)\tF1 0.753 (0.739)\n",
            "Epoch: [67][700/1405]\tBatch Time 0.215 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 11.7692 (11.4283)\tVB Loss 1.4967 (2.3699)\tF1 0.860 (0.737)\n",
            "Epoch: [67][800/1405]\tBatch Time 0.204 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 11.9790 (11.4446)\tVB Loss 2.1568 (2.3714)\tF1 0.834 (0.737)\n",
            "Epoch: [67][900/1405]\tBatch Time 0.242 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 10.9333 (11.4472)\tVB Loss 1.4995 (2.3832)\tF1 0.841 (0.735)\n",
            "Epoch: [67][1000/1405]\tBatch Time 0.229 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 11.9341 (11.4370)\tVB Loss 3.4418 (2.3730)\tF1 0.651 (0.736)\n",
            "Epoch: [67][1100/1405]\tBatch Time 0.247 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 11.5775 (11.4390)\tVB Loss 1.9566 (2.3849)\tF1 0.611 (0.735)\n",
            "Epoch: [67][1200/1405]\tBatch Time 0.241 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 11.7638 (11.4274)\tVB Loss 2.7504 (2.3745)\tF1 0.758 (0.735)\n",
            "Epoch: [67][1300/1405]\tBatch Time 0.177 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 10.6228 (11.4254)\tVB Loss 1.5785 (2.3654)\tF1 0.840 (0.735)\n",
            "Epoch: [67][1400/1405]\tBatch Time 0.171 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 11.5094 (11.4185)\tVB Loss 1.7004 (2.3534)\tF1 0.744 (0.737)\n",
            "Validation: [0/325]\tBatch Time 0.359 (0.359)\tVB Loss 6.7503 (6.7503)\tF1 Score 0.763 (0.763)\t\n",
            "Validation: [100/325]\tBatch Time 0.125 (0.123)\tVB Loss 1.9323 (2.1058)\tF1 Score 0.761 (0.798)\t\n",
            "Validation: [200/325]\tBatch Time 0.103 (0.125)\tVB Loss 2.1272 (2.0607)\tF1 Score 0.753 (0.807)\t\n",
            "Validation: [300/325]\tBatch Time 0.154 (0.124)\tVB Loss 1.7267 (2.0519)\tF1 Score 0.868 (0.806)\t\n",
            "\n",
            " * LOSS - 2.053, F1 SCORE - 0.807\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000341\n",
            "\n",
            "Epoch: [68][0/1405]\tBatch Time 0.511 (0.511)\tData Load Time 0.205 (0.205)\tCE Loss 12.5549 (12.5549)\tVB Loss 3.0964 (3.0964)\tF1 0.566 (0.566)\n",
            "Epoch: [68][100/1405]\tBatch Time 0.224 (0.249)\tData Load Time 0.006 (0.007)\tCE Loss 10.9744 (11.3956)\tVB Loss 2.9999 (2.2674)\tF1 0.714 (0.738)\n",
            "Epoch: [68][200/1405]\tBatch Time 0.199 (0.236)\tData Load Time 0.004 (0.006)\tCE Loss 10.7611 (11.3880)\tVB Loss 1.7620 (2.2714)\tF1 0.744 (0.724)\n",
            "Epoch: [68][300/1405]\tBatch Time 0.192 (0.229)\tData Load Time 0.006 (0.006)\tCE Loss 10.9481 (11.3843)\tVB Loss 3.5415 (2.2041)\tF1 0.700 (0.735)\n",
            "Epoch: [68][400/1405]\tBatch Time 0.224 (0.229)\tData Load Time 0.004 (0.006)\tCE Loss 10.7980 (11.3812)\tVB Loss 5.2160 (2.3001)\tF1 0.784 (0.735)\n",
            "Epoch: [68][500/1405]\tBatch Time 0.230 (0.226)\tData Load Time 0.005 (0.005)\tCE Loss 12.3190 (11.3887)\tVB Loss 1.2730 (2.2815)\tF1 0.828 (0.735)\n",
            "Epoch: [68][600/1405]\tBatch Time 0.299 (0.226)\tData Load Time 0.005 (0.005)\tCE Loss 10.8637 (11.3915)\tVB Loss 2.2528 (2.3021)\tF1 0.835 (0.734)\n",
            "Epoch: [68][700/1405]\tBatch Time 0.187 (0.226)\tData Load Time 0.005 (0.005)\tCE Loss 11.5262 (11.3920)\tVB Loss 2.4795 (2.3140)\tF1 0.564 (0.734)\n",
            "Epoch: [68][800/1405]\tBatch Time 0.162 (0.224)\tData Load Time 0.004 (0.005)\tCE Loss 11.7513 (11.3996)\tVB Loss 1.2964 (2.3114)\tF1 0.869 (0.736)\n",
            "Epoch: [68][900/1405]\tBatch Time 0.209 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 11.9316 (11.4028)\tVB Loss 2.8540 (2.3239)\tF1 0.604 (0.735)\n",
            "Epoch: [68][1000/1405]\tBatch Time 0.187 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 11.3807 (11.4069)\tVB Loss 2.0939 (2.3297)\tF1 0.821 (0.735)\n",
            "Epoch: [68][1100/1405]\tBatch Time 0.227 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 11.9024 (11.4067)\tVB Loss 0.9839 (2.3321)\tF1 0.888 (0.736)\n",
            "Epoch: [68][1200/1405]\tBatch Time 0.264 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 11.8807 (11.4020)\tVB Loss 3.6040 (2.3428)\tF1 0.540 (0.736)\n",
            "Epoch: [68][1300/1405]\tBatch Time 0.154 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 10.7501 (11.3966)\tVB Loss 0.9703 (2.3382)\tF1 0.715 (0.737)\n",
            "Epoch: [68][1400/1405]\tBatch Time 0.256 (0.222)\tData Load Time 0.004 (0.005)\tCE Loss 10.8917 (11.4004)\tVB Loss 3.3116 (2.3336)\tF1 0.730 (0.738)\n",
            "Validation: [0/325]\tBatch Time 0.325 (0.325)\tVB Loss 2.0302 (2.0302)\tF1 Score 0.943 (0.943)\t\n",
            "Validation: [100/325]\tBatch Time 0.083 (0.114)\tVB Loss 2.9908 (2.0358)\tF1 Score 0.485 (0.800)\t\n",
            "Validation: [200/325]\tBatch Time 0.128 (0.114)\tVB Loss 1.6134 (1.9930)\tF1 Score 0.904 (0.806)\t\n",
            "Validation: [300/325]\tBatch Time 0.103 (0.113)\tVB Loss 1.3500 (2.0842)\tF1 Score 0.880 (0.802)\t\n",
            "\n",
            " * LOSS - 2.038, F1 SCORE - 0.806\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000337\n",
            "\n",
            "Epoch: [69][0/1405]\tBatch Time 0.457 (0.457)\tData Load Time 0.205 (0.205)\tCE Loss 11.8030 (11.8030)\tVB Loss 3.6480 (3.6480)\tF1 0.529 (0.529)\n",
            "Epoch: [69][100/1405]\tBatch Time 0.199 (0.252)\tData Load Time 0.005 (0.008)\tCE Loss 11.1045 (11.3408)\tVB Loss 1.2064 (2.3636)\tF1 0.852 (0.728)\n",
            "Epoch: [69][200/1405]\tBatch Time 0.248 (0.236)\tData Load Time 0.005 (0.006)\tCE Loss 12.0939 (11.3690)\tVB Loss 1.9149 (2.2613)\tF1 0.629 (0.728)\n",
            "Epoch: [69][300/1405]\tBatch Time 0.254 (0.234)\tData Load Time 0.005 (0.006)\tCE Loss 11.8498 (11.3811)\tVB Loss 1.5060 (2.2389)\tF1 0.640 (0.731)\n",
            "Epoch: [69][400/1405]\tBatch Time 0.205 (0.234)\tData Load Time 0.005 (0.006)\tCE Loss 11.7447 (11.3770)\tVB Loss 2.6486 (2.2887)\tF1 0.791 (0.729)\n",
            "Epoch: [69][500/1405]\tBatch Time 0.153 (0.232)\tData Load Time 0.006 (0.006)\tCE Loss 10.9218 (11.3517)\tVB Loss 0.8129 (2.2716)\tF1 0.853 (0.737)\n",
            "Epoch: [69][600/1405]\tBatch Time 0.342 (0.230)\tData Load Time 0.004 (0.005)\tCE Loss 10.7957 (11.3570)\tVB Loss 4.9396 (2.2367)\tF1 0.731 (0.742)\n",
            "Epoch: [69][700/1405]\tBatch Time 0.189 (0.229)\tData Load Time 0.004 (0.005)\tCE Loss 11.0499 (11.3678)\tVB Loss 2.1464 (2.2421)\tF1 0.919 (0.740)\n",
            "Epoch: [69][800/1405]\tBatch Time 0.197 (0.228)\tData Load Time 0.006 (0.005)\tCE Loss 10.9311 (11.3662)\tVB Loss 2.5531 (2.2766)\tF1 0.827 (0.738)\n",
            "Epoch: [69][900/1405]\tBatch Time 0.283 (0.227)\tData Load Time 0.006 (0.005)\tCE Loss 11.1800 (11.3716)\tVB Loss 3.4063 (2.3089)\tF1 0.809 (0.738)\n",
            "Epoch: [69][1000/1405]\tBatch Time 0.288 (0.227)\tData Load Time 0.005 (0.005)\tCE Loss 11.5069 (11.3785)\tVB Loss 4.2832 (2.3144)\tF1 0.716 (0.740)\n",
            "Epoch: [69][1100/1405]\tBatch Time 0.202 (0.226)\tData Load Time 0.005 (0.005)\tCE Loss 11.5330 (11.3789)\tVB Loss 1.2015 (2.3272)\tF1 0.575 (0.740)\n",
            "Epoch: [69][1200/1405]\tBatch Time 0.121 (0.225)\tData Load Time 0.004 (0.005)\tCE Loss 9.7927 (11.3846)\tVB Loss 1.0534 (2.3271)\tF1 0.676 (0.739)\n",
            "Epoch: [69][1300/1405]\tBatch Time 0.201 (0.223)\tData Load Time 0.004 (0.005)\tCE Loss 10.9145 (11.3852)\tVB Loss 1.2853 (2.3124)\tF1 0.888 (0.740)\n",
            "Epoch: [69][1400/1405]\tBatch Time 0.198 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 11.2613 (11.3815)\tVB Loss 1.9307 (2.3089)\tF1 0.746 (0.739)\n",
            "Validation: [0/325]\tBatch Time 0.255 (0.255)\tVB Loss 0.7411 (0.7411)\tF1 Score 0.924 (0.924)\t\n",
            "Validation: [100/325]\tBatch Time 0.121 (0.114)\tVB Loss 3.5720 (1.9133)\tF1 Score 0.715 (0.822)\t\n",
            "Validation: [200/325]\tBatch Time 0.119 (0.118)\tVB Loss 2.5011 (2.0657)\tF1 Score 0.850 (0.800)\t\n",
            "Validation: [300/325]\tBatch Time 0.092 (0.118)\tVB Loss 2.1680 (2.0580)\tF1 Score 0.773 (0.803)\t\n",
            "\n",
            " * LOSS - 2.030, F1 SCORE - 0.805\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000333\n",
            "\n",
            "Epoch: [70][0/1405]\tBatch Time 0.541 (0.541)\tData Load Time 0.239 (0.239)\tCE Loss 11.6375 (11.6375)\tVB Loss 3.6610 (3.6610)\tF1 0.463 (0.463)\n",
            "Epoch: [70][100/1405]\tBatch Time 0.279 (0.239)\tData Load Time 0.005 (0.007)\tCE Loss 11.9452 (11.3717)\tVB Loss 2.7373 (2.2966)\tF1 0.547 (0.735)\n",
            "Epoch: [70][200/1405]\tBatch Time 0.218 (0.230)\tData Load Time 0.004 (0.006)\tCE Loss 11.0573 (11.3353)\tVB Loss 1.7760 (2.2694)\tF1 0.908 (0.750)\n",
            "Epoch: [70][300/1405]\tBatch Time 0.223 (0.224)\tData Load Time 0.005 (0.005)\tCE Loss 11.4127 (11.3281)\tVB Loss 2.3573 (2.3170)\tF1 0.641 (0.741)\n",
            "Epoch: [70][400/1405]\tBatch Time 0.213 (0.220)\tData Load Time 0.004 (0.005)\tCE Loss 12.0132 (11.3415)\tVB Loss 5.8792 (2.3044)\tF1 0.622 (0.740)\n",
            "Epoch: [70][500/1405]\tBatch Time 0.157 (0.218)\tData Load Time 0.005 (0.005)\tCE Loss 10.0322 (11.3496)\tVB Loss 1.2843 (2.3012)\tF1 0.868 (0.740)\n",
            "Epoch: [70][600/1405]\tBatch Time 0.223 (0.216)\tData Load Time 0.004 (0.005)\tCE Loss 11.4576 (11.3454)\tVB Loss 5.6085 (2.2996)\tF1 0.743 (0.739)\n",
            "Epoch: [70][700/1405]\tBatch Time 0.218 (0.214)\tData Load Time 0.004 (0.005)\tCE Loss 11.4000 (11.3457)\tVB Loss 1.3455 (2.2823)\tF1 0.889 (0.739)\n",
            "Epoch: [70][800/1405]\tBatch Time 0.220 (0.214)\tData Load Time 0.004 (0.005)\tCE Loss 11.6566 (11.3465)\tVB Loss 1.5866 (2.2699)\tF1 0.857 (0.742)\n",
            "Epoch: [70][900/1405]\tBatch Time 0.208 (0.214)\tData Load Time 0.004 (0.005)\tCE Loss 10.7202 (11.3509)\tVB Loss 1.1665 (2.2635)\tF1 0.856 (0.743)\n",
            "Epoch: [70][1000/1405]\tBatch Time 0.093 (0.214)\tData Load Time 0.004 (0.005)\tCE Loss 11.0516 (11.3546)\tVB Loss 0.5983 (2.2607)\tF1 0.832 (0.745)\n",
            "Epoch: [70][1100/1405]\tBatch Time 0.184 (0.214)\tData Load Time 0.004 (0.005)\tCE Loss 11.9045 (11.3595)\tVB Loss 2.8414 (2.2628)\tF1 0.734 (0.745)\n",
            "Epoch: [70][1200/1405]\tBatch Time 0.314 (0.215)\tData Load Time 0.004 (0.005)\tCE Loss 11.7070 (11.3668)\tVB Loss 4.3788 (2.2759)\tF1 0.722 (0.744)\n",
            "Epoch: [70][1300/1405]\tBatch Time 0.150 (0.214)\tData Load Time 0.005 (0.005)\tCE Loss 10.3452 (11.3657)\tVB Loss 1.1518 (2.2800)\tF1 0.836 (0.743)\n",
            "Epoch: [70][1400/1405]\tBatch Time 0.192 (0.215)\tData Load Time 0.004 (0.005)\tCE Loss 11.0938 (11.3661)\tVB Loss 1.4936 (2.2755)\tF1 0.868 (0.744)\n",
            "Validation: [0/325]\tBatch Time 0.305 (0.305)\tVB Loss 2.6395 (2.6395)\tF1 Score 0.864 (0.864)\t\n",
            "Validation: [100/325]\tBatch Time 0.109 (0.110)\tVB Loss 2.0501 (2.0358)\tF1 Score 0.643 (0.791)\t\n",
            "Validation: [200/325]\tBatch Time 0.122 (0.117)\tVB Loss 3.2865 (1.9726)\tF1 Score 0.770 (0.804)\t\n",
            "Validation: [300/325]\tBatch Time 0.123 (0.119)\tVB Loss 1.0473 (1.9725)\tF1 Score 0.932 (0.802)\t\n",
            "\n",
            " * LOSS - 1.989, F1 SCORE - 0.800\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000330\n",
            "\n",
            "Epoch: [71][0/1405]\tBatch Time 0.489 (0.489)\tData Load Time 0.206 (0.206)\tCE Loss 11.4093 (11.4093)\tVB Loss 1.4471 (1.4471)\tF1 0.788 (0.788)\n",
            "Epoch: [71][100/1405]\tBatch Time 0.231 (0.235)\tData Load Time 0.004 (0.007)\tCE Loss 12.0290 (11.3090)\tVB Loss 2.4738 (2.0989)\tF1 0.815 (0.756)\n",
            "Epoch: [71][200/1405]\tBatch Time 0.234 (0.224)\tData Load Time 0.004 (0.006)\tCE Loss 11.4997 (11.3394)\tVB Loss 1.9720 (2.2702)\tF1 0.770 (0.738)\n",
            "Epoch: [71][300/1405]\tBatch Time 0.257 (0.223)\tData Load Time 0.005 (0.005)\tCE Loss 10.5848 (11.3374)\tVB Loss 1.2047 (2.2673)\tF1 0.894 (0.741)\n",
            "Epoch: [71][400/1405]\tBatch Time 0.239 (0.222)\tData Load Time 0.005 (0.005)\tCE Loss 11.4258 (11.3391)\tVB Loss 2.0200 (2.2507)\tF1 0.759 (0.745)\n",
            "Epoch: [71][500/1405]\tBatch Time 0.234 (0.221)\tData Load Time 0.005 (0.005)\tCE Loss 11.4090 (11.3332)\tVB Loss 3.1079 (2.2521)\tF1 0.655 (0.744)\n",
            "Epoch: [71][600/1405]\tBatch Time 0.201 (0.219)\tData Load Time 0.004 (0.005)\tCE Loss 10.8455 (11.3436)\tVB Loss 1.1078 (2.2507)\tF1 0.684 (0.745)\n",
            "Epoch: [71][700/1405]\tBatch Time 0.254 (0.219)\tData Load Time 0.005 (0.005)\tCE Loss 10.9299 (11.3486)\tVB Loss 2.5643 (2.2775)\tF1 0.858 (0.745)\n",
            "Epoch: [71][800/1405]\tBatch Time 0.222 (0.218)\tData Load Time 0.005 (0.005)\tCE Loss 11.9932 (11.3479)\tVB Loss 1.7251 (2.2877)\tF1 0.760 (0.745)\n",
            "Epoch: [71][900/1405]\tBatch Time 0.181 (0.217)\tData Load Time 0.004 (0.005)\tCE Loss 10.9773 (11.3457)\tVB Loss 0.6410 (2.2771)\tF1 0.739 (0.744)\n",
            "Epoch: [71][1000/1405]\tBatch Time 0.286 (0.216)\tData Load Time 0.005 (0.005)\tCE Loss 11.6414 (11.3498)\tVB Loss 3.2722 (2.2725)\tF1 0.772 (0.744)\n",
            "Epoch: [71][1100/1405]\tBatch Time 0.307 (0.215)\tData Load Time 0.006 (0.005)\tCE Loss 11.8767 (11.3491)\tVB Loss 3.2292 (2.2761)\tF1 0.684 (0.743)\n",
            "Epoch: [71][1200/1405]\tBatch Time 0.239 (0.215)\tData Load Time 0.005 (0.005)\tCE Loss 10.9050 (11.3542)\tVB Loss 3.0395 (2.2808)\tF1 0.648 (0.743)\n",
            "Epoch: [71][1300/1405]\tBatch Time 0.247 (0.214)\tData Load Time 0.004 (0.005)\tCE Loss 11.2098 (11.3539)\tVB Loss 1.8319 (2.2792)\tF1 0.701 (0.742)\n",
            "Epoch: [71][1400/1405]\tBatch Time 0.228 (0.214)\tData Load Time 0.004 (0.005)\tCE Loss 11.3057 (11.3496)\tVB Loss 2.4256 (2.2678)\tF1 0.794 (0.742)\n",
            "Validation: [0/325]\tBatch Time 0.256 (0.256)\tVB Loss 1.0233 (1.0233)\tF1 Score 0.819 (0.819)\t\n",
            "Validation: [100/325]\tBatch Time 0.122 (0.118)\tVB Loss 5.5431 (2.1213)\tF1 Score 0.707 (0.814)\t\n",
            "Validation: [200/325]\tBatch Time 0.143 (0.119)\tVB Loss 2.9223 (2.0783)\tF1 Score 0.748 (0.809)\t\n",
            "Validation: [300/325]\tBatch Time 0.111 (0.119)\tVB Loss 3.3723 (2.0159)\tF1 Score 0.666 (0.809)\t\n",
            "\n",
            " * LOSS - 1.995, F1 SCORE - 0.810\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000326\n",
            "\n",
            "Epoch: [72][0/1405]\tBatch Time 0.528 (0.528)\tData Load Time 0.218 (0.218)\tCE Loss 10.9489 (10.9489)\tVB Loss 2.5788 (2.5788)\tF1 0.824 (0.824)\n",
            "Epoch: [72][100/1405]\tBatch Time 0.273 (0.234)\tData Load Time 0.006 (0.007)\tCE Loss 11.3591 (11.3720)\tVB Loss 2.1372 (2.2970)\tF1 0.696 (0.737)\n",
            "Epoch: [72][200/1405]\tBatch Time 0.203 (0.222)\tData Load Time 0.004 (0.006)\tCE Loss 11.1536 (11.3378)\tVB Loss 2.1687 (2.2754)\tF1 0.914 (0.734)\n",
            "Epoch: [72][300/1405]\tBatch Time 0.278 (0.220)\tData Load Time 0.005 (0.006)\tCE Loss 11.1489 (11.3567)\tVB Loss 1.7424 (2.2361)\tF1 0.699 (0.743)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OipsVUg24hin",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rates"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}