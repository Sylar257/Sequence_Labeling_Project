{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sequence_labeling_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sylar257/Sequence_Labeling_Project/blob/master/Sequence_labeling_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEyZ1KroE9iI",
        "colab_type": "text"
      },
      "source": [
        "We will be implementing the [Empower Sequence Labeling with Task-Aware Neural Language Model](https://arxiv.org/pdf/1709.04109.pdf) paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tmFs0zFHDQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from utils import *\n",
        "import torch.nn.functional as F\n",
        "decive = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QENba6KHo59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Highway(nn.Module):\n",
        "    \"\"\"\n",
        "    Highway network\n",
        "    \"\"\"\n",
        "    def __init__(self, size, num_layers=1, dropout=0.5):\n",
        "        \"\"\"\n",
        "        size: size of Linear layer (should match input size)\n",
        "        num_layers: number of transform and gate layers\n",
        "        dropout: dropout rate\n",
        "        \"\"\"\n",
        "        super(Highway, self).__init__()\n",
        "        self.size = size\n",
        "        self.num_layers = num_layers\n",
        "        self.transform = nn.ModuleList() # A list of transform layers\n",
        "        self.gate = nn.ModuleList()      # A list of gate layers\n",
        "        self.dropout = torch.nn.Dropout(p=dropout)\n",
        "\n",
        "        for i in range(num_layers):\n",
        "            transform = nn.Linear(size, size)\n",
        "            gate = nn.Linear(size, size)\n",
        "            self.transform.append(transform)\n",
        "            self.gate.append(gate)\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward-prop.\n",
        "        Returns a tensor with the same dimensions as input tensor\n",
        "        \"\"\"\n",
        "\n",
        "        transformed = F.relu(self.transform[0](x))  # transform with the first transform layer\n",
        "        g = F.sigmoid(self.gate[0](x))              # calculate how much of the transformed input to keep\n",
        "\n",
        "        out = self.dropout(g*transformed + (1-g)*x)               # combine input and transformed input with ratio of g\n",
        "\n",
        "        # If there are additional layers\n",
        "        for i in range(self.num_layers):\n",
        "            transformed = F.relu(self.transform[i](out))\n",
        "            g = F.sigmoid(self.gate[i](out))\n",
        "            out = self.dropout(g*transformed+(1-g)*out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zaftUlVLOad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CRF(nn.Module):\n",
        "    \"\"\"\n",
        "    Confitional Random Field\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dim, tagset_size):\n",
        "        \"\"\"\n",
        "        hidden_dim: the size of word/BLSTM's output (which is the input size for CRF)\n",
        "        tagset_size: number of tags(depending on our dataset)\n",
        "        \"\"\"\n",
        "\n",
        "        super(CRF, self).__init__()\n",
        "        self.tagset_size = tagset_size\n",
        "        self.emission = nn.Linear(hidden_dim, self.tagset_size)\n",
        "        self.transition = nn.Parameter(torch.Tensor(self.tagset_size, self.tagset_size))\n",
        "        self.transition.data.zero_() # initializa the transition matrix to be all zeros\n",
        "\n",
        "    def forward(self, feats):\n",
        "        \"\"\"\n",
        "        feats:   output of word/BLSTM, a tensor of dimensions-(batch_size, timesteps, hidden_dim)\n",
        "        returns: CRF scores, a tensor of dimensions-(batch_size, timesteps, tagset_size, tagset_size)\n",
        "        \"\"\"\n",
        "        self.batch_size = feats.size(0)\n",
        "        self.timesteps  = feats.size(1)\n",
        "\n",
        "        emission_scores = self.emission(feats)  # (batch_size, timesteps, tagset_size)\n",
        "        # here we broadcast emission_scores in order to compute the total score later with transition score\n",
        "        emission_scores = emission_scores.unsqueeze(2).expand(self.batch_size, self.timesteps, self.tagset_size, self.tagset_size)  # (batch_size, timesteps, tagset_size, tagset_size)\n",
        "\n",
        "        crf_scores = emission_scores + self.transition.unsqueeze(0).unsqueeze(0)  # (batch_size, timesteps, tagset_size, tagset_size)\n",
        "        return crf_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAK0OBqVT61P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ViterbiLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Viterbi Loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tag_map):\n",
        "        \"\"\"\n",
        "        :param tag_map: tag map\n",
        "        \"\"\"\n",
        "        super(ViterbiLoss, self).__init__()\n",
        "        self.tagset_size = len(tag_map)\n",
        "        self.start_tag = tag_map['<start>']\n",
        "        self.end_tag = tag_map['<end>']\n",
        "\n",
        "    def forward(self, scores, targets, lengths):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param scores: CRF scores\n",
        "        :param targets: true tags indices in unrolled CRF scores\n",
        "        :param lengths: word sequence lengths\n",
        "        :return: viterbi loss\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size = scores.size(0)\n",
        "        word_pad_len = scores.size(1)\n",
        "\n",
        "        # Gold score\n",
        "\n",
        "        targets = targets.unsqueeze(2)\n",
        "        scores_at_targets = torch.gather(scores.view(batch_size, word_pad_len, -1), 2, targets).squeeze(\n",
        "            2)  # (batch_size, word_pad_len)\n",
        "\n",
        "        # Everything is already sorted by lengths\n",
        "        scores_at_targets = pack_padded_sequence(scores_at_targets, lengths, batch_first=True)\n",
        "        gold_score = scores_at_targets.data.sum()\n",
        "\n",
        "        # All paths' scores\n",
        "\n",
        "        # Create a tensor to hold accumulated sequence scores at each current tag\n",
        "        scores_upto_t = torch.zeros(batch_size, self.tagset_size).to(device)\n",
        "\n",
        "        for t in range(max(lengths)):\n",
        "            batch_size_t = sum([l > t for l in lengths])  # effective batch size (sans pads) at this timestep\n",
        "            if t == 0:\n",
        "                scores_upto_t[:batch_size_t] = scores[:batch_size_t, t, self.start_tag, :]  # (batch_size, tagset_size)\n",
        "            else:\n",
        "                # We add scores at current timestep to scores accumulated up to previous timestep, and log-sum-exp\n",
        "                # Remember, the cur_tag of the previous timestep is the prev_tag of this timestep\n",
        "                # So, broadcast prev. timestep's cur_tag scores along cur. timestep's cur_tag dimension\n",
        "                scores_upto_t[:batch_size_t] = log_sum_exp(\n",
        "                    scores[:batch_size_t, t, :, :] + scores_upto_t[:batch_size_t].unsqueeze(2),\n",
        "                    dim=1)  # (batch_size, tagset_size)\n",
        "\n",
        "        # We only need the final accumulated scores at the <end> tag\n",
        "        all_paths_scores = scores_upto_t[:, self.end_tag].sum()\n",
        "\n",
        "        viterbi_loss = all_paths_scores - gold_score\n",
        "        viterbi_loss = viterbi_loss / batch_size\n",
        "\n",
        "        return viterbi_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4w6o7q1jQxtd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LM_LSTM_CRF(nn.Module):\n",
        "    \"\"\"\n",
        "    The encompassing LM-LSTM-CRF\n",
        "    \"\"\"\n",
        "    def __init__(self, tagset_size, charset_size, char_emb_dim, char_rnn_dim, char_rnn_layers, vocab_size,\n",
        "                 lm_vocab_size, word_emb_dim, word_rnn_dim, word_rnn_layers, dropout, highway_layers=1):\n",
        "        \"\"\"\n",
        "        tagset_size:   number of tags\n",
        "        charset_size:  size of character vocabulary\n",
        "        char_emb_dim:  size of character embeddings\n",
        "        char_rnn_dim:  size of charactor RNNS/LSTMs\n",
        "        char_rnn_layers: number of layers in character RNN/LSTMs\n",
        "        vocab_size:    input vocabulary size\n",
        "        lm_vocab_size: vocabulary size of language models (in-corpus words subject to word frequency threshold)\n",
        "        word_emb_dim:  size of word embeddings\n",
        "        word_rnn_dim:  size of word RNN/BLSTM\n",
        "        word_rnn_layers: number of layers in word RNNs/LSTMs\n",
        "        dropout:       dropout\n",
        "        highway_layers: number of transform and gate layers\n",
        "        \"\"\"\n",
        "        \n",
        "        super(LM_LSTM_CRF, self).__init__()\n",
        "\n",
        "        self.tagset_size  = tagset_size # this is the size of the outout vocab of the tagging model\n",
        "\n",
        "        self.charset_size = charset_size\n",
        "        self.char_emb_dim = char_emb_dim\n",
        "        self.char_rnn_dim = char_rnn_dim\n",
        "        self.char_rnn_layers = char_rnn_layers\n",
        "\n",
        "        self.wordset_size  = vocab_size     # this is the size of the input vocab (embedding layer) of the tagging model\n",
        "        self.lm_vocab_size = lm_vocab_size  # this is the size of the output vocab of the language model\n",
        "        self.word_emb_dim  = word_emb_dim\n",
        "        self.word_rnn_dim  = word_rnn_dim\n",
        "        self.word_rnn_layers = word_rnn_layers\n",
        "        \n",
        "        self.highway_layers = highway_layers\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # charactor embedding layer\n",
        "        self.char_embeds = nn.Embedding(self.charset_size, self.char_emb_dim) \n",
        "\n",
        "        # forward char LSTM\n",
        "        self.forw_char_lstm = nn.LSTM(input_size=self.char_emb_dim, hidden_size=self.char_rnn_dim, \n",
        "                                      num_layers=self.char_rnn_layers, bidirectional=False, dropout = dropout)\n",
        "        # backward char LSTM\n",
        "        self.back_char_lstm = nn.LSTM(input_size=self.char_emb_dim, hidden_size=self.char_rnn_dim,\n",
        "                                      num_layers=self.char_rnn_layers, bidirectional=False, dropout = dropout)\n",
        "        \n",
        "        # word embedding layer\n",
        "        self.word_embeds = nn.Embedding(num_embeddings=self.wordset_size,embedding_dim=self.word_emb_dim)\n",
        "        # Define word-level bidirection LSTM\n",
        "        # Take note on the hidden_size\n",
        "        self.word_blstm   = nn.LSTM(self.word_emb_dim + self.char_rnn_dim * 2, # input_size\n",
        "                                    self.word_rnn_dim // 2,                    # hidden_size\n",
        "                                    # This is because Bi-directional LSTM will concat forward and backward output\n",
        "                                    # therefore we specify word_rnn_dim//2 but will get output size of word_rnn_dim\n",
        "                                    num_layers=self.word_rnn_layers,\n",
        "                                    bidirectional=True,\n",
        "                                    dropout=dropout\n",
        "                                    )\n",
        "        \n",
        "        # Conditinoal Random Field layer\n",
        "        self.crf = CRF(hidden_dim=self.word_rnn_dim,tagset_size=self.tagset_size)\n",
        "\n",
        "        # 3 places that we implemented highway connections\n",
        "        self.forw_lm_hw = Highway(size=self.char_rnn_dim,\n",
        "                                  num_layers=self.highway_layers,\n",
        "                                  dropout=dropout)\n",
        "        self.back_lm_hw = Highway(size=self.char_rnn_dim,\n",
        "                                  num_layers=self.highway_layers,\n",
        "                                  dropout=dropout)\n",
        "        self.subword_hw = Highway(2 * self.char_rnn_dim, \n",
        "                                  num_layers=self.highway_layers,\n",
        "                                  dropout=dropout)\n",
        "        \n",
        "        # Linear layers for language models, They are used for \"muti-task training\" for language models (predicting next word)\n",
        "        self.forw_lm_out = nn.Linear(self.char_rnn_dim, self.lm_vocab_size)\n",
        "        self.back_lm_out = nn.Linear(self.char_rnn_dim, self.lm_vocab_size)\n",
        "\n",
        "    def init_word_embedding(self, embedding):\n",
        "        \"\"\"\n",
        "        Initialize embeddings with pre-trained embeddings.\n",
        "\n",
        "        embedding: pre-trained embeddings to be loaded\n",
        "        \"\"\"\n",
        "        self.word_embeds.weights = nn.Parameter(embeddings)\n",
        "\n",
        "    def fine_tune_word_embeddings(self, fine_tune=False):\n",
        "        \"\"\"\n",
        "        Fine-tune embedding layer? (if using pre-trained embedding layer, consider no fine-tuning)\n",
        "\n",
        "        fine_tune: bool decides if fine_tune\n",
        "        \"\"\"\n",
        "        for p in self.word_embeds.parameters():\n",
        "            p.requires_grad = fine_tune\n",
        "    \n",
        "    def forward(self, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, wmaps, tmaps, wmap_lengths, cmap_lengths):\n",
        "        \"\"\"\n",
        "        cmaps_f: padded encoded forward  character sequences. (batch_size, char_pad_len)\n",
        "        cmaps_b: padded encoded backward character sequences. (batch_size, char_pad_len)\n",
        "        cmarkers_f: padded forward character markers.          (batch_size, word_pad_len)\n",
        "        cmarkers_b: padded backward character markers.         (batch_size, word_pad_len)\n",
        "        wmaps: padded encoded word sequences.                 (batch_size, word_pad_len)\n",
        "        tmaps: padded tag sequences.                          (batch_size, word_pad_len)\n",
        "        wmap_lengths: word sequence lengths.                  (batch_size)\n",
        "        cmap_lengths: character sequence lengths              (batch_size)\n",
        "        \"\"\"\n",
        "\n",
        "        self.batch_size   = cmaps_f.size(0)\n",
        "        self.word_pad_len = wmaps.size(1)\n",
        "\n",
        "        # Sort by decreasing true char. sequence length for grouping up for padding later\n",
        "        cmap_lengths, char_sort_ind = cmap_lengths.sort(dim=0, descending=True)\n",
        "        cmaps_f = cmaps_f[char_sort_ind]\n",
        "        cmaps_b = cmaps_b[char_sort_ind]\n",
        "        cmarkers_f = cmarkers_f[char_sort_ind]\n",
        "        cmarkers_b = cmarkers_b[char_sort_ind]\n",
        "        wmaps = wmaps[char_sort_ind]\n",
        "        tmaps = tmaps[char_sort_ind]\n",
        "        wmap_lengths = wmap_lengths[char_sort_ind]\n",
        "\n",
        "        # Embedding look-up for characters, turning each character to its embedding of char_emb_dim size\n",
        "        cf = self.char_embeds(cmaps_f)  # (batch_size, char_pad_len, char_emb_dim)\n",
        "        cb = self.char_embeds(cmaps_b)  # (batch_size, char_pad_len, char_emb_dim)\n",
        "\n",
        "        # Dropout\n",
        "        cf = self.dropout(cf)  # (batch_size, char_pad_len, char_emb_dim)\n",
        "        cb = self.dropout(cb)\n",
        "\n",
        "        # Pack padded sequence\n",
        "        cf = pack_padded_sequence(cf, lengths=cmap_lengths.tolist(), batch_first=True) # packed sequence of char_emb_dim, with real sequence lengths\n",
        "        cb = pack_padded_sequence(cb, lengths=cmap_lengths.tolist(), batch_first=True)\n",
        "\n",
        "        # LSTM for forward and backword language model & feature extraction for Bi-directional LSTM\n",
        "        cf, _ = self.forw_char_lstm(cf)  # packed sequence of char_rnn_dim, with real sequence lengths\n",
        "        cb, _ = self.back_char_lstm(cb)   \n",
        "\n",
        "        # Unpack packed sequence\n",
        "        cf, _ = pad_packed_sequence(cf, batch_first=True) # (batch, max_char_len_in_batch, char_rnn_dim)\n",
        "        cb, _ = pad_packed_sequence(cb, batch_first=True) \n",
        "\n",
        "        # Sanity check\n",
        "        assert cf.size(1) == max(cmap_lengths.tolist()) == list(cmap_lengths)[0]\n",
        "\n",
        "        # Select RNN outpus only at marker points (spaces in the character sequence)\n",
        "        cmarkers_f = cmarkers_f.unsqueeze(2).expand(self.batch_size, self.word_pad_len, self.char_rnn_dim)\n",
        "        cmarkers_b = cmarkers_b.unsqueeze(2).expand(self.batch_size, self.word_pad_len, self.char_rnn_dim)\n",
        "        # torch.gather return output same dim as index, with value taken from cf. In this case we can see that dim=1 of output might be different from input(cf)\n",
        "        cf_selected = torch.gather(cf, 1, index=cmarkers_f) # (batch_size, word_pad_len, char_rnn_dim)\n",
        "        cb_selected = torch.gather(cb, 1, index=cmarkers_b)\n",
        "\n",
        "        # Only for co-training language model(to boost performance), not useful for tagging after model is trained\n",
        "        if self.training:   # this toggle is true when we set model.train(), false if we set model.eval()\n",
        "            lm_f = self.forw_lm_hw(self.dropout(cf_selected))  # (batch_size, word_pad_len, char_rnn_dim)\n",
        "            lm_b = self.back_lm_hw(self.dropout(cb_selected))\n",
        "            lm_f_scores = self.forw_lm_out(self.dropout(lm_f))  # (batch_size, word_pad_len, lm_vocab_size)\n",
        "            lm_b_scores = self.back_lm_out(self.dropout(lm_b))\n",
        "\n",
        "        # Sort by decreasing true word sequence length\n",
        "        wmap_lengths, word_sort_ind = wmap_lengths.sort(dim=0, descending=True)\n",
        "        wmaps = wmaps[word_sort_ind]\n",
        "        tmaps = tmaps[word_sort_ind]\n",
        "        cf_selected = cf_selected[word_sort_ind]  # for language model\n",
        "        cb_selected = cb_selected[word_sort_ind]\n",
        "        if self.training:\n",
        "            lm_f_scores = lm_f_scores[word_sort_ind]\n",
        "            lm_b_scores = lm_b_scores[word_sort_ind]\n",
        "\n",
        "        # Embedding look-up for words\n",
        "        w = self.word_embeds(wmaps)  # (batch_size, word_pad_len, word_emb_dim)\n",
        "        w = self.dropout(w)\n",
        "\n",
        "        # Sub-word information at each word\n",
        "        subword = self.subword_hw(self.dropout(torch.cat((cf_selected, cb_selected), dim=2)))  # (batch_size, word_pad_len, 2 * char_rnn_dim)\n",
        "        subword = self.dropout(subword)\n",
        "\n",
        "        # Concatenate word embeddings and sub-word features\n",
        "        w = torch.cat((w, subword), dim=2)  # (batch_size, word_pad_len, 2*char_rnn_dim+word_emb_dim)\n",
        "\n",
        "        # Pack padded sequence\n",
        "        w = pack_padded_sequence(w, list(wmap_lengths), batch_first=True)   # packed sequence of word_emb_dim+2*char_rnn_dim\n",
        "\n",
        "        # Bi-directional LSTM\n",
        "        w, _ = self.word_blstm(w)   # packed sequence of word_rnn_dim, with real sequence lengths\n",
        "        \n",
        "        # Unpack packed sequence\n",
        "        w, _ = pad_packed_sequence(w, batch_first=True)  # (batch_size, max_word_len_in_batch, word_rnn_dim)\n",
        "        w = self.dropout(w)\n",
        "\n",
        "        crf_scores = self.crf(w)     # (batch_size, max_word_len_in_batch, tagset_size, tagset_size)\n",
        "\n",
        "        if self.training:\n",
        "            return crf_scores, lm_f_scores, lm_b_scores, wmaps, tmaps, wmap_lengths, word_sort_ind, char_sort_ind\n",
        "        else:\n",
        "            return crf_scores, wmaps, tmaps, wmap_lengths, word_sort_ind, char_sort_ind  # sort inds to reorder, if req."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XRFQhNj_Bfr",
        "colab_type": "code",
        "outputId": "b9370141-7dad-4b4d-a13f-83b57854b0fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My\\ Drive"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n",
            "/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ixkzwHv_ye6",
        "colab_type": "code",
        "outputId": "b5cf79be-928a-43ab-8cff-dfb21ae9f876",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 2018\n",
            " 2019\n",
            "'Attention in deep learning.md'\n",
            " BEST_checkpoint_lm_lstm_crf.pth.tar\n",
            " checkpoint_lm_lstm_crf.pth.tar\n",
            "'Colab Notebooks'\n",
            " data\n",
            " Enoava\n",
            " FastAI-NLP.ipynb\n",
            "'LSTM in PyTorch.md'\n",
            "'LSTM in PyTorch.pdf'\n",
            "'Response to comments_MECHMT_2018_1107_-V4_17_Dec.docx'\n",
            " Snaps\n",
            " Stack-presentation-Dermotologist.png\n",
            "'Starwars Project Video V2.mp4'\n",
            " YOLO.md\n",
            " YOLO.pdf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIwrkKoX_zxO",
        "colab_type": "code",
        "outputId": "d4595d3c-98c6-458e-fd48-a623fde65eef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /gdrive/My\\ Drive/data/embeddings"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive/data/embeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obm_y4aE__pm",
        "colab_type": "code",
        "outputId": "8e7b1ce3-8813-4fed-91ee-de4f20f761c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "glove.6B.100d.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATeBjYOFA5pg",
        "colab_type": "code",
        "outputId": "0dfa8142-bad8-4a13-80d4-1ccebc97cc44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive/data/embeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUmg97Kl9Z-v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "# Rewrite the __getitem__ and add __len__\n",
        "class WCDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for the LM-LSTM-CRF model. To be used by a PyTorch DataLoader to feed batches to the model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, wmaps, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, tmaps, wmap_lengths, cmap_lengths):\n",
        "        \"\"\"\n",
        "        :param wmaps: padded encoded word sequences\n",
        "        :param cmaps_f: padded encoded forward character sequences\n",
        "        :param cmaps_b: padded encoded backward character sequences\n",
        "        :param cmarkers_f: padded forward character markers\n",
        "        :param cmarkers_b: padded backward character markers\n",
        "        :param tmaps: padded encoded tag sequences (indices in unrolled CRF scores)\n",
        "        :param wmap_lengths: word sequence lengths\n",
        "        :param cmap_lengths: character sequence lengths\n",
        "        \"\"\"\n",
        "        self.wmaps = wmaps\n",
        "        self.cmaps_f = cmaps_f\n",
        "        self.cmaps_b = cmaps_b\n",
        "        self.cmarkers_f = cmarkers_f\n",
        "        self.cmarkers_b = cmarkers_b\n",
        "        self.tmaps = tmaps\n",
        "        self.wmap_lengths = wmap_lengths\n",
        "        self.cmap_lengths = cmap_lengths\n",
        "\n",
        "        self.data_size = self.wmaps.size(0)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.wmaps[i], self.cmaps_f[i], self.cmaps_b[i], self.cmarkers_f[i], self.cmarkers_b[i], self.tmaps[i], \\\n",
        "               self.wmap_lengths[i], self.cmap_lengths[i]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHc2_4Nj-iTF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ViterbiDecoder():\n",
        "    \"\"\"\n",
        "    Viterbi Decoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tag_map):\n",
        "        \"\"\"\n",
        "        :param tag_map: tag map\n",
        "        \"\"\"\n",
        "        self.tagset_size = len(tag_map)\n",
        "        self.start_tag = tag_map['<start>']\n",
        "        self.end_tag = tag_map['<end>']\n",
        "\n",
        "    def decode(self, scores, lengths):\n",
        "        \"\"\"\n",
        "        :param scores: CRF scores\n",
        "        :param lengths: word sequence lengths\n",
        "        :return: decoded sequences\n",
        "        \"\"\"\n",
        "        batch_size = scores.size(0)\n",
        "        word_pad_len = scores.size(1)\n",
        "\n",
        "        # Create a tensor to hold accumulated sequence scores at each current tag\n",
        "        scores_upto_t = torch.zeros(batch_size, self.tagset_size)\n",
        "\n",
        "        # Create a tensor to hold back-pointers\n",
        "        # i.e., indices of the previous_tag that corresponds to maximum accumulated score at current tag\n",
        "        # Let pads be the <end> tag index, since that was the last tag in the decoded sequence\n",
        "        backpointers = torch.ones((batch_size, max(lengths), self.tagset_size), dtype=torch.long) * self.end_tag\n",
        "\n",
        "        for t in range(max(lengths)):\n",
        "            batch_size_t = sum([l > t for l in lengths])  # effective batch size (sans pads) at this timestep\n",
        "            if t == 0:\n",
        "                scores_upto_t[:batch_size_t] = scores[:batch_size_t, t, self.start_tag, :]  # (batch_size, tagset_size)\n",
        "                backpointers[:batch_size_t, t, :] = torch.ones((batch_size_t, self.tagset_size),\n",
        "                                                               dtype=torch.long) * self.start_tag\n",
        "            else:\n",
        "                # We add scores at current timestep to scores accumulated up to previous timestep, and\n",
        "                # choose the previous timestep that corresponds to the max. accumulated score for each current timestep\n",
        "                scores_upto_t[:batch_size_t], backpointers[:batch_size_t, t, :] = torch.max(\n",
        "                    scores[:batch_size_t, t, :, :] + scores_upto_t[:batch_size_t].unsqueeze(2),\n",
        "                    dim=1)  # (batch_size, tagset_size)\n",
        "\n",
        "        # Decode/trace best path backwards\n",
        "        decoded = torch.zeros((batch_size, backpointers.size(1)), dtype=torch.long)\n",
        "        pointer = torch.ones((batch_size, 1),\n",
        "                             dtype=torch.long) * self.end_tag  # the pointers at the ends are all <end> tags\n",
        "\n",
        "        for t in list(reversed(range(backpointers.size(1)))):\n",
        "            decoded[:, t] = torch.gather(backpointers[:, t, :], 1, pointer).squeeze(1)\n",
        "            pointer = decoded[:, t].unsqueeze(1)  # (batch_size, 1)\n",
        "\n",
        "        # Sanity check\n",
        "        assert torch.equal(decoded[:, 0], torch.ones((batch_size), dtype=torch.long) * self.start_tag)\n",
        "\n",
        "        # Remove the <starts> at the beginning, and append with <ends> (to compare to targets, if any)\n",
        "        decoded = torch.cat([decoded[:, 1:], torch.ones((batch_size, 1), dtype=torch.long) * self.start_tag],\n",
        "                            dim=1)\n",
        "\n",
        "        return decoded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGatVIt56gXe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import sys\n",
        "from utils import *\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI_mY6W265rh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "task = 'ner'        # tagging task, choose between [ner, pos]\n",
        "train_file = '/gdrive/My Drive/data/CoNLL-2003/eng.train'\n",
        "val_file   = '/gdrive/My Drive/data/CoNLL-2003/eng.testa'\n",
        "test_file  = '/gdrive/My Drive/data/CoNLL-2003/eng.testb'\n",
        "emb_file   = '/gdrive/My Drive/data/embeddings/glove.6B.100d.txt'\n",
        "min_word_freq = 5 # threshold for word frequency to be recognized not as xxunk\n",
        "min_char_freq = 1 # same thing for char frequency\n",
        "caseless   = True # lowercase everything?\n",
        "expand_vocab = True # expand model's input vocabulary to the pre-trained embedding vocabulary?\n",
        "\n",
        "# Model parameters\n",
        "char_emb_dim = 30 # character embedding size\n",
        "with open(emb_file, 'r') as f:\n",
        "    word_emb_dim = len(f.readline().split(' ')) - 1  # word embdding size, \"-1\" is because in the txt file the first place is the word itself, followed by the actual embeddings\n",
        "word_rnn_dim = 300  # word BLSTM hidden size\n",
        "char_rnn_dim = 300  # character RNN size\n",
        "char_rnn_layers = 1 # number of layers in character RNN\n",
        "word_rnn_layers = 1 # number of layers in word BLSTM\n",
        "highway_layers  = 1 # number of layers in highway network\n",
        "dropout = 0.55       # universal dropout rate\n",
        "fine_tune_word_embeddings = False\n",
        "\n",
        "# Training parameters\n",
        "start_epoch = 0   # start at this epoch\n",
        "batch_size  = 10  # batch size\n",
        "lr = 0.015  \n",
        "lr_decay = 0.05\n",
        "momentum = 0.9\n",
        "workers  = 4\n",
        "epochs   = 200    # number of epochs without triggering early stoping\n",
        "grad_clip = 5.\n",
        "print_freq = 300  # print every ___ batches\n",
        "best_f1  = 0.\n",
        "checkpoint = 'BEST_checkpoint_lm_lstm_crf.pth.tar' # Model checkpoint to load. None if training from scratch\n",
        "\n",
        "tag_ind = 1 if task == 'pos' else 3 # choose column in CoNLL 2003 dataset\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnLn_7JoDGUf",
        "colab_type": "code",
        "outputId": "894203ce-c94c-48bc-b52b-5f3625d255b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 911
        }
      },
      "source": [
        "global best_f1, epochs_since_improvement, checkpoint, start_epoch, word_map, char_map, tag_map\n",
        "\n",
        "# Read training and validation data\n",
        "train_words, train_tags = read_words_tags(train_file, tag_ind, caseless)\n",
        "val_words, val_tags = read_words_tags(val_file, tag_ind, caseless)\n",
        "\n",
        "if checkpoint is not None:\n",
        "    checkpoint = torch.load(checkpoint)\n",
        "    model = checkpoint['model']\n",
        "    optimizer = checkpoint['optimizer']\n",
        "    word_map  = checkpoint['word_map']\n",
        "    lm_vocab_size = checkpoint['lm_vocab_size']\n",
        "    tag_map   = checkpoint['tag_map']\n",
        "    char_map  = checkpoint['char_map']\n",
        "    start_epoch = checkpoint['epoch'] +1\n",
        "    best_f1   = checkpoint['f1']\n",
        "else:\n",
        "    # create word, char, tag maps\n",
        "    # maps are essentially dictionaries that map a token to an integer\n",
        "    word_map, char_map, tag_map = create_maps(train_words+val_words,train_tags+val_tags, min_word_freq, min_char_freq)\n",
        "\n",
        "    # load pre-trained embeddings, if expand_vocab==True, word_map expand to embedding_word_map\n",
        "    # lm_vocab_size is the word_map size before expand to \"out-of-corpus vocab\"\n",
        "    embeddings, word_map, lm_vocab_size = load_embeddings(emb_file, word_map, expand_vocab)\n",
        "\n",
        "    model = LM_LSTM_CRF(tagset_size=len(tag_map),\n",
        "                        charset_size=len(char_map),\n",
        "                        char_emb_dim=char_emb_dim,\n",
        "                        char_rnn_dim=char_rnn_dim,\n",
        "                        char_rnn_layers=char_rnn_layers,\n",
        "                        vocab_size=len(word_map),       # This is the length after expand\n",
        "                        lm_vocab_size=lm_vocab_size,    # len(word_map) before expand, not influenced by the embedding vocab\n",
        "                        word_emb_dim=word_emb_dim,\n",
        "                        word_rnn_dim=word_rnn_dim,\n",
        "                        word_rnn_layers=word_rnn_layers,\n",
        "                        dropout=dropout,\n",
        "                        highway_layers=highway_layers).to(device)\n",
        "    model.init_word_embedding(embeddings.to(device)) # initializa embedding layers with pre-trained embeddings.(Essentially we just make it nn.Parameter)\n",
        "    model.fine_tune_word_embeddings(fine_tune_word_embeddings)    # decide if these nn.Parameters has requires_grad = True (trainable)\n",
        "    optimizer = optim.SGD(params=filter(lambda p: p.requires_grad, model.parameters()), lr=lr, momentum=momentum)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_check_seekable\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'seek'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-05d742b97bd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'encoding'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    586\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown saved id type: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msaved_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m     \u001b[0m_check_seekable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m     \u001b[0mf_should_read_directly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_should_read_directly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_check_seekable\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedOperation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0mraise_err_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"seek\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mraise_err_msg\u001b[0;34m(patterns, e)\u001b[0m\n\u001b[1;32m    221\u001b[0m                                 \u001b[0;34m\" Please pre-load the data into a buffer like io.BytesIO and\"\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                                 \" try to load from it instead.\")\n\u001b[0;32m--> 223\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re6HfRf1tGAy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from functools import partial\n",
        "\n",
        "def annealer(f):\n",
        "    def _inner(start, end): return partial(f, start, end)\n",
        "    return _inner\n",
        "\n",
        "@annealer\n",
        "def sched_lin(start, end, pos): return start + pos*(end-start)\n",
        "@annealer\n",
        "def sched_cos(start, end, pos): return start + (1 + math.cos(math.pi*(1-pos))) * (end-start) / 2\n",
        "@annealer\n",
        "def sched_no(start, end, pos): return start\n",
        "@annealer\n",
        "def sched_exp(start, end, pos): return start*(end/start)**pos\n",
        "\n",
        "# This monkey-path is here to enable plotting tensors\n",
        "torch.Tensor.ndim = property(lambda x: len(x.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd0BzLrdtmkC",
        "colab_type": "code",
        "outputId": "cf19a24b-afd0-48f9-e9c5-41048245b224",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "from torch import tensor\n",
        "def combine_scheds(pcts, scheds):\n",
        "    assert sum(pcts) == 1.\n",
        "    pcts = tensor([0] + list(pcts))\n",
        "    assert torch.all(pcts >= 0)\n",
        "    pcts = torch.cumsum(pcts, 0)\n",
        "    def _inner(pos):\n",
        "        idx = (pos >= pcts).nonzero().max()\n",
        "        actual_pos = (pos-pcts[idx]) / (pcts[idx+1]-pcts[idx])\n",
        "        return scheds[idx](actual_pos)\n",
        "    return _inner\n",
        "sched = combine_scheds([0.3, 0.7], [sched_cos(0.3, 1.1), sched_cos(1.1, 0.1)])\n",
        "a = torch.arange(0, 100)\n",
        "p = torch.linspace(0.01,1,100)\n",
        "plt.plot(a, [sched(o) for o in p])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f0186f881d0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJ8UVncplAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7eiHeWJ7zSf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining how one training step will be computed.\n",
        "# train() is used in each epoch later\n",
        "sched = combine_scheds([0.3, 0.7], [sched_cos(1e-5, 1e-3), sched_cos(1e-3, 1e-6)])\n",
        "learning_rates = list()\n",
        "\n",
        "def train(train_loader, model, lm_criterion, crf_criterion, optimizer, epoch, vb_decoder, total_iters):\n",
        "    \"\"\"\n",
        "    train_loader: DataLoader for training data\n",
        "    model: LM_LSTM_CRF\n",
        "    lm_criterion:  nn.CrossEntropyLoss()\n",
        "    crf_criterion: ViterbiLoss()\n",
        "    optimizer: SGD/adam/adbound whatever your choice is\n",
        "    epoch: epoch number\n",
        "    vb_decoder: viterbi decoder(to decode and find F1 score)\n",
        "    \"\"\"\n",
        "    model.train()  # training mode, so dropout\n",
        "\n",
        "    batch_time = AverageMeter()  # forward prop. + back prop. time per batch\n",
        "    data_time = AverageMeter()  # data loading time per batch\n",
        "    ce_losses = AverageMeter()  # cross entropy loss\n",
        "    vb_losses = AverageMeter()  # viterbi loss\n",
        "    f1s = AverageMeter()  # f1 score\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    # Batches\n",
        "    for i, (wmaps, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, tmaps, wmap_lengths, cmap_lengths) in enumerate(train_loader):\n",
        "        data_time.update(time.time()-start)\n",
        "\n",
        "        max_word_len = max(wmap_lengths.tolist())\n",
        "        max_char_len = max(cmap_lengths.tolist())\n",
        "\n",
        "        # Reduce batch's padded length to maximum in-batch sequence. \n",
        "        # This saves some compute on nn.Linear layers (RNNs are not affected, since they don't compute over the pads)\n",
        "        wmaps = wmaps[:, :max_word_len].to(device)\n",
        "        cmaps_f = cmaps_f[:, :max_char_len].to(device)\n",
        "        cmaps_b = cmaps_b[:, :max_char_len].to(device)\n",
        "        cmarkers_f = cmarkers_f[:, :max_word_len].to(device)\n",
        "        cmarkers_b = cmarkers_b[:, :max_word_len].to(device)\n",
        "        tmaps = tmaps[:, :max_word_len].to(device)\n",
        "        wmap_lengths = wmap_lengths.to(device)\n",
        "        cmap_lengths = cmap_lengths.to(device)\n",
        "\n",
        "        # Forward prop.\n",
        "        crf_scores, lm_f_scores, lm_b_scores, wmaps_sorted, tmaps_sorted, wmap_lengths_sorted, _, __ = model(cmaps_f,\n",
        "                                                                                                             cmaps_b,\n",
        "                                                                                                             cmarkers_f,\n",
        "                                                                                                             cmarkers_b,\n",
        "                                                                                                             wmaps,\n",
        "                                                                                                             tmaps,\n",
        "                                                                                                             wmap_lengths,\n",
        "                                                                                                             cmap_lengths)\n",
        "        \n",
        "        # LM loss\n",
        "\n",
        "        # We don't predict the next word at the pads or <end> tokens\n",
        "        # Hence, only predict [word1, word2, word3, word4] among [word1, word2, word3, word4,<pad>,<pad>,<pad>,<pad>,<pad>,<end>]\n",
        "        # So prediction lengths are word sequence lengths -1\n",
        "        lm_lengths = wmap_lengths_sorted - 1 # (batch_size) the effective length of each row\n",
        "        lm_lengths = lm_lengths.tolist()\n",
        "\n",
        "        # Remove scores at timesteps we won't predict at\n",
        "        # pack_padded_sequence is a good trick to do this (145 PyTorch tricks(my other repo)---Trick #11)\n",
        "        lm_f_scores = pack_padded_sequence(lm_f_scores, lm_lengths, batch_first=True)\n",
        "        lm_b_scores = pack_padded_sequence(lm_b_scores, lm_lengths, batch_first=True)\n",
        "\n",
        "        # For the forward sequence, targets are from the second word onwards, up to <end>\n",
        "        # (timestep -> target) ...dunston -> checks, ...checks -> in, ...in -> <end>\n",
        "        lm_f_targets = wmaps_sorted[:, 1:]\n",
        "        lm_f_targets = pack_padded_sequence(lm_f_targets, lm_lengths, batch_first=True)\n",
        "\n",
        "        # For the backward sequence, targets are <end> followed by all words except the last word\n",
        "        # ...notsnud -> <end>, ...skcehc -> dunston, ...ni -> checks\n",
        "        lm_b_targets = torch.cat([torch.LongTensor([word_map['<end>']] * wmaps_sorted.size(0)).unsqueeze(1).to(device), \n",
        "                                  wmaps_sorted], dim=1)\n",
        "        lm_b_targets = pack_padded_sequence(lm_b_targets, lm_lengths, batch_first=True)\n",
        "        \n",
        "        # Calculate loss\n",
        "        ce_loss = lm_criterion(lm_f_scores.data, lm_f_targets.data) + lm_criterion(lm_b_scores.data, lm_b_targets.data)\n",
        "        vb_loss = crf_criterion(crf_scores, tmaps_sorted, wmap_lengths_sorted)\n",
        "        loss = ce_loss + vb_loss\n",
        "\n",
        "        # Learning rate annealing\n",
        "        current_iter = epoch*n_batches + i\n",
        "        new_lr = sched(current_iter/total_iters)\n",
        "        optimizer.param_groups[0]['lr'] = new_lr\n",
        "        learning_rates.append(new_lr)\n",
        "        # print(f'the learning rate in Epoch {epoch}, step {i} is {new_lr}')\n",
        "\n",
        "        # Back-prop\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        if grad_clip is not None:\n",
        "            clip_gradient(optimizer, grad_clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        # Viterbi decode to find accuracy/F1\n",
        "        decoded = vb_decoder.decode(crf_scores.to(\"cpu\"), wmap_lengths_sorted.to(\"cpu\"))\n",
        "\n",
        "        # Remove timesteps we won't predict at, and also <end> tags, because to predict them would be cheating\n",
        "        decoded      = pack_padded_sequence(decoded, lm_lengths, batch_first=True)\n",
        "        tmaps_sorted = tmaps_sorted % vb_decoder.tagset_size  # actual target indices (see create_input_tensors())\n",
        "        tmaps_sorted = pack_padded_sequence(tmaps_sorted, lm_lengths, batch_first=True)\n",
        "\n",
        "        # Compute F1\n",
        "        f1 = f1_score(tmaps_sorted.data.to(\"cpu\").numpy(), decoded.data.numpy(), average='macro')\n",
        "\n",
        "        # Keep track of metrics\n",
        "        ce_losses.update(ce_loss.item(), sum(lm_lengths))\n",
        "        vb_losses.update(vb_loss.item(), crf_scores.size(0))\n",
        "        batch_time.update(time.time() - start)\n",
        "        f1s.update(f1, sum(lm_lengths))\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "        # Print training status\n",
        "        if i % print_freq == 0:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Data Load Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "                  'CE Loss {ce_loss.val:.4f} ({ce_loss.avg:.4f})\\t'\n",
        "                  'VB Loss {vb_loss.val:.4f} ({vb_loss.avg:.4f})\\t'\n",
        "                  'F1 {f1.val:.3f} ({f1.avg:.3f})'.format(epoch, i, len(train_loader),\n",
        "                                                          batch_time=batch_time,\n",
        "                                                          data_time=data_time, ce_loss=ce_losses,\n",
        "                                                          vb_loss=vb_losses, f1=f1s))\n",
        "def validate(val_loader, model, crf_criterion, vb_decoder):\n",
        "    \"\"\"\n",
        "    val_loader:    Dataloader for validation data\n",
        "    model:         Model\n",
        "    crf_criterion: Viterbi loss layer\n",
        "    return:        validation F1 score\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    batch_time = AverageMeter()\n",
        "    vb_losses  = AverageMeter()\n",
        "    f1s        = AverageMeter()\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    # validation loops\n",
        "    for i, (wmaps, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, tmaps, wmap_lengths, cmap_lengths) in enumerate(val_loader):\n",
        "        max_word_len = max(wmap_lengths.tolist())\n",
        "        max_char_len = max(cmap_lengths.tolist())\n",
        "\n",
        "        # Reduce batch's padded length to maximum in-batch sequence\n",
        "        # This saves some compute on nn.Linear layers (RNNs are unaffected, since they don't compute over the pads)\n",
        "        wmaps = wmaps[:, :max_word_len].to(device)\n",
        "        cmaps_f = cmaps_f[:, :max_char_len].to(device)\n",
        "        cmaps_b = cmaps_b[:, :max_char_len].to(device)\n",
        "        cmarkers_f = cmarkers_f[:, :max_word_len].to(device)\n",
        "        cmarkers_b = cmarkers_b[:, :max_word_len].to(device)\n",
        "        tmaps = tmaps[:, :max_word_len].to(device)\n",
        "        wmap_lengths = wmap_lengths.to(device)\n",
        "        cmap_lengths = cmap_lengths.to(device)\n",
        "\n",
        "        # Forward prop.\n",
        "        crf_scores, wmaps_sorted, tmaps_sorted, wmap_lengths_sorted, _, __ = model(cmaps_f,\n",
        "                                                                                    cmaps_b,\n",
        "                                                                                    cmarkers_f,\n",
        "                                                                                    cmarkers_b,\n",
        "                                                                                    wmaps,\n",
        "                                                                                    tmaps,\n",
        "                                                                                    wmap_lengths,\n",
        "                                                                                    cmap_lengths)\n",
        "\n",
        "        # Viterbi / CRF layer loss\n",
        "        vb_loss = crf_criterion(crf_scores, tmaps_sorted, wmap_lengths_sorted)\n",
        "\n",
        "        # Viterbi decode to find accuracy / f1\n",
        "        decoded = vb_decoder.decode(crf_scores.to(\"cpu\"), wmap_lengths_sorted.to(\"cpu\"))\n",
        "\n",
        "        # Remove timesteps we won't predict at, and also <end> tags, because to predict them would be cheating\n",
        "        decoded      = pack_padded_sequence(decoded, (wmap_lengths_sorted - 1).tolist(), batch_first=True)\n",
        "        tmaps_sorted = tmaps_sorted % vb_decoder.tagset_size  # actual target indices (see create_input_tensors())\n",
        "        tmaps_sorted = pack_padded_sequence(tmaps_sorted, (wmap_lengths_sorted - 1).tolist(), batch_first=True)\n",
        "\n",
        "        # f1\n",
        "        f1 = f1_score(tmaps_sorted.data.to(\"cpu\").numpy(), decoded.data.numpy(), average='macro')\n",
        "\n",
        "        # Keep track of metrics\n",
        "        vb_losses.update(vb_loss.item(), crf_scores.size(0))\n",
        "        f1s.update(f1, sum((wmap_lengths_sorted - 1).tolist()))\n",
        "        batch_time.update(time.time() - start)\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "        if i % print_freq == 0:\n",
        "            print('Validation: [{0}/{1}]\\t'\n",
        "                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'VB Loss {vb_loss.val:.4f} ({vb_loss.avg:.4f})\\t'\n",
        "                  'F1 Score {f1.val:.3f} ({f1.avg:.3f})\\t'.format(i, len(val_loader), batch_time=batch_time,\n",
        "                                                                  vb_loss=vb_losses, f1=f1s))\n",
        "\n",
        "    print(\n",
        "        '\\n * LOSS - {vb_loss.avg:.3f}, F1 SCORE - {f1.avg:.3f}\\n'.format(vb_loss=vb_losses,\n",
        "                                                                          f1=f1s))\n",
        "\n",
        "    return f1s.avg    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCUtUGyqDAuu",
        "colab_type": "code",
        "outputId": "edc0af3a-7fd8-43ff-a862-d3c9ebc98540",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# Loss funcitons\n",
        "lm_criterion  = nn.CrossEntropyLoss().to(device)\n",
        "crf_criterion = ViterbiLoss(tag_map).to(device)\n",
        "\n",
        "# Since the language model's vocab is restricted on in-corpus indices, encode training/val with only these!\n",
        "# word_map might have been expanded, and in-corpus words eliminated due to low frequency might still be added because\n",
        "# they exist in the pre-trained embeddings.(these embeddings are added after the <unk> token)\n",
        "temp_word_map = {k: v for k, v in word_map.items() if v <= word_map['<unk>']}\n",
        "\n",
        "# train_input = (padded_wmaps, padded_cmaps_f, padded_cmaps_b, padded_cmarkers_f, padded_cmarkers_b, \n",
        "#                padded_tmaps, wmap_lengths,   cmap_lengths)\n",
        "train_inputs = create_input_tensors(train_words, train_tags, temp_word_map, char_map,\n",
        "                                        tag_map)\n",
        "val_inputs = create_input_tensors(val_words, val_tags, temp_word_map, char_map, tag_map)\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = torch.utils.data.DataLoader(WCDataset(*train_inputs), batch_size=batch_size, shuffle=True,\n",
        "                                            num_workers=workers, pin_memory=False)\n",
        "val_loader = torch.utils.data.DataLoader(WCDataset(*val_inputs), batch_size=batch_size, shuffle=True,\n",
        "                                             num_workers=workers, pin_memory=False)\n",
        "\n",
        "# Viterbi decoder (to find accuracy during validation)\n",
        "vb_decoder = ViterbiDecoder(tag_map)\n",
        "\n",
        "# Calculate total number of iterations for learning rate annealing\n",
        "n_batches   = len(train_loader)\n",
        "total_iters = epochs*n_batches\n",
        "epochs_since_improvement = 0\n",
        "\n",
        "# Epochs\n",
        "for epoch in range(start_epoch, epochs):\n",
        "    # One epoch's training\n",
        "    train(train_loader  = train_loader,\n",
        "          model         = model,\n",
        "          lm_criterion  = lm_criterion,\n",
        "          crf_criterion = crf_criterion,\n",
        "          optimizer     = optimizer,\n",
        "          epoch         = epoch,\n",
        "          vb_decoder    = vb_decoder,\n",
        "          total_iters   = total_iters\n",
        "          )\n",
        "    # One epoch's validation\n",
        "    val_f1 = validate(val_loader = val_loader,\n",
        "                      model      = model,\n",
        "                      crf_criterion = crf_criterion,\n",
        "                      vb_decoder    = vb_decoder)\n",
        "    is_best = val_f1 > best_f1\n",
        "    if not is_best:\n",
        "        epochs_since_improvement += 1\n",
        "        print(\"\\nEpochs since improvement: %d\\n\" % (epochs_since_improvement,))\n",
        "    else:\n",
        "        epochs_since_improvement = 0\n",
        "    \n",
        "    # Save checkpoint\n",
        "    save_checkpoint(epoch, model, optimizer, val_f1, word_map, char_map, tag_map, lm_vocab_size, is_best)\n",
        "\n",
        "    # Decay learning rate every epoch\n",
        "    # adjust_learning_rate(optimizer, lr / (1 + (epoch + 1) * lr_decay))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: [88][0/1405]\tBatch Time 0.345 (0.345)\tData Load Time 0.188 (0.188)\tCE Loss 11.4445 (11.4445)\tVB Loss 0.5451 (0.5451)\tF1 0.789 (0.789)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: [88][300/1405]\tBatch Time 0.129 (0.123)\tData Load Time 0.004 (0.005)\tCE Loss 11.4837 (11.1575)\tVB Loss 0.2710 (1.9228)\tF1 0.942 (0.779)\n",
            "Epoch: [88][600/1405]\tBatch Time 0.149 (0.123)\tData Load Time 0.004 (0.005)\tCE Loss 10.9598 (11.1175)\tVB Loss 4.3769 (1.9267)\tF1 0.683 (0.775)\n",
            "Epoch: [88][900/1405]\tBatch Time 0.094 (0.123)\tData Load Time 0.004 (0.004)\tCE Loss 10.3788 (11.1088)\tVB Loss 0.6879 (1.9498)\tF1 0.959 (0.772)\n",
            "Epoch: [88][1200/1405]\tBatch Time 0.105 (0.123)\tData Load Time 0.004 (0.004)\tCE Loss 10.5042 (11.1092)\tVB Loss 0.7330 (1.9629)\tF1 0.932 (0.773)\n",
            "Validation: [0/325]\tBatch Time 0.264 (0.264)\tVB Loss 1.0537 (1.0537)\tF1 Score 0.930 (0.930)\t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OipsVUg24hin",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rates"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}